{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Recognition Model Training from Scratch\n",
    "\n",
    "In this notebook, we will develop a simple speech recognition model using PyTorch and the `torchaudio` library. We will look at defining, training, and evaluating a neural network on the SPEECHCOMMANDS dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "from torch.utils.data import DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "The SPEECHCOMMANDS dataset is a collection of spoken words designed for command recognition. Below, we define a subclass to handle data loading and preprocessing for different subsets of this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUICK = True\n",
    "\n",
    "class SubsetSC(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        super().__init__(\"./\", download=True)\n",
    "\n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            with open(filepath) as fileobj:\n",
    "                return [os.path.join(self._path, line.strip()) for line in fileobj]\n",
    "\n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
    "            excludes = set(excludes)\n",
    "            self._walker = [w for w in self._walker if w not in excludes]\n",
    "        elif subset == \"debug\":\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
    "            excludes = set(excludes)\n",
    "            self._walker = [self._walker[w] for w in range(len(self._walker)) if self._walker[w] not in excludes and w%10 == 0]\n",
    "        elif subset == \"dev\":\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
    "            excludes = set(excludes)\n",
    "            self._walker = [self._walker[w] for w in range(len(self._walker)) if self._walker[w] not in excludes and w%1000 == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "We will define a simple neural network with fully connected layers to classify audio into one of 35 categories based on the command spoken.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.n_labels = 35\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(16000, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, self.n_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "We will train our model on the training subset, refining the data to only include samples that match our input dimensionality and training it for a number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1, Loss:3.5317\n",
      "Epoch:2, Loss:3.4479\n",
      "Epoch:3, Loss:3.3486\n",
      "Epoch:4, Loss:3.2293\n",
      "Epoch:5, Loss:3.0913\n",
      "Epoch:6, Loss:2.9352\n",
      "Epoch:7, Loss:2.7646\n",
      "Epoch:8, Loss:2.5826\n",
      "Epoch:9, Loss:2.3928\n",
      "Epoch:10, Loss:2.1990\n",
      "Epoch:11, Loss:2.0055\n",
      "Epoch:12, Loss:1.8166\n",
      "Epoch:13, Loss:1.6361\n",
      "Epoch:14, Loss:1.4666\n",
      "Epoch:15, Loss:1.3109\n",
      "Epoch:16, Loss:1.1706\n",
      "Epoch:17, Loss:1.0468\n",
      "Epoch:18, Loss:0.9389\n",
      "Epoch:19, Loss:0.8455\n",
      "Epoch:20, Loss:0.7648\n"
     ]
    }
   ],
   "source": [
    "def refine(data):\n",
    "    \"\"\" Refine data to ensure each sample is of correct dimensionality. \"\"\"\n",
    "    return [data[i] for i in range(len(data)) if data[i][0].shape[1] == 16000 ]\n",
    "\n",
    "def train_model():\n",
    "    NUM_EPOCHS = 20\n",
    "    BATCH_SIZE = 100\n",
    "\n",
    "    working_set = SubsetSC(\"training\")\n",
    "    if QUICK:\n",
    "        working_set = SubsetSC(\"dev\")\n",
    "    working_set = refine(working_set)\n",
    "    dataloader = DataLoader(working_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    model = SimpleNet()\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "    for epoch  in range(1, NUM_EPOCHS + 1):\n",
    "        for features, size, train_labels, serial, train_labels_indices in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            features = features.reshape(features.shape[0], features.shape[2])\n",
    "            y_pred = model(features)\n",
    "            loss = F.nll_loss(y_pred, train_labels_indices)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch:{epoch}, Loss:{loss.item():.4f}')\n",
    "    return model\n",
    "\n",
    "model = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model\n",
    "\n",
    "Finally, evaluate the performance of our trained model using the validation subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 correct among 97 in test set\n"
     ]
    }
   ],
   "source": [
    "def test_model(model):\n",
    "    working_set = SubsetSC(\"testing\")\n",
    "    if QUICK:\n",
    "        working_set = SubsetSC(\"dev\")\n",
    "    working_set = refine(working_set)\n",
    "    dataloader = DataLoader(working_set, batch_size=100, shuffle=True)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for features, size, train_labels, serial, train_labels_indices in dataloader:\n",
    "            features = features.reshape(features.shape[0], features.shape[2])\n",
    "            y_pred = model(features)\n",
    "            pred = y_pred.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(train_labels_indices.data.view_as(pred)).sum()\n",
    "    print(f\"{correct} correct among {len(working_set)} in test set\")\n",
    "    \n",
    "test_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
