{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python Libraries for NLP\n",
    "\n",
    "This notebook provides an introduction to some of the most popular Python libraries for NLP: NLTK, spaCy, and PyTorch. We will learn how to setup these libraries, understand their fundamental concepts, and apply them in simple NLP tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Setting Up Your Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "pip install nltk spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Introduction to NLTK\n",
    "\n",
    "The [Natural Language Toolkit (NLTK)](https://www.nltk.org/) is one of the leading platforms for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a range of functionalities each suited for different aspects of NLP. Below we will explore some of these functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/waseem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download the required models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Tokenization\n",
    "Tokenization is the process of breaking down text into smaller components, such as words or sentences. This is a fundamental step in most NLP tasks as it helps in preparing text for further modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization: ['Hello', 'world', '.', 'NLTK', 'is', 'cool', '!']\n",
      "Sentence Tokenization: ['Hello world.', 'NLTK is cool!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/waseem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"Hello world. NLTK is cool!\"\n",
    "print(\"Word Tokenization:\", word_tokenize(text))\n",
    "print(\"Sentence Tokenization:\", sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we perform tokenization on a text using `word_tokenize` and `sent_tokenize`, the output is a list.\n",
    "\n",
    "- **Word Tokenization:** This splits the input text into individual elements where each element is a word. Punctuation marks are also treated as separate tokens.\n",
    "  \n",
    "  Output: `['Hello', 'world', '.', 'NLTK', 'is', 'cool', '!']`\n",
    "  \n",
    "  This shows each word and punctuation mark from the input text \"Hello world. NLTK is cool!\" as a separate token.\n",
    "\n",
    "- **Sentence Tokenization:** This splits the text into individual sentences. Each sentence is a string in the resulting list.\n",
    "  \n",
    "  Output: `['Hello world.', 'NLTK is cool!']`\n",
    "  \n",
    "  The input text is split into sentences wherever punctuation marks that typically end sentences (like `.`, `!`, `?`) are found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Stemming and Lemmatization\n",
    "Stemming and lemmatization are techniques used to reduce a word to its base or root form. Stemming generally chops off endings based on common prefixes and suffixes, while lemmatization takes into consideration the morphological analysis of the word, returning the lemma or base form according to the language's lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/waseem/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: run\n",
      "Lemmatization: run\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word = \"running\"\n",
    "print(\"Stemming:\", stemmer.stem(word))\n",
    "print(\"Lemmatization:\", lemmatizer.lemmatize(word, pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both stemming and lemmatization aim to reduce a word to its base form, but their methods and results differ.\n",
    "\n",
    "- **Stemming Output**: `run`\n",
    "  \n",
    "  The word \"running\" is reduced to \"run\". Stemming often results in a form that is not a valid word but captures the core meaning.\n",
    "  \n",
    "- **Lemmatization Output**: `run`\n",
    "  \n",
    "  Unlike stemming, lemmatization returns a root form that is a valid word (\"run\"), which is the lemma of \"running\" when the word is treated as a verb (`pos='v'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Part-of-Speech Tagging\n",
    "Part-of-speech (POS) tagging is the process of marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context. POS tagging is used for syntax parsing, text analysis, and in the preprocessing steps for more complex NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging: [('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/waseem/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "words = word_tokenize(\"And now for something completely different\")\n",
    "print(\"POS Tagging:\", nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of `pos_tag` is a list of tuples where each tuple consists of a word from the input text and a tag indicating its part of speech.\n",
    "\n",
    "Output: `[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]`\n",
    "\n",
    "Here's what the tags mean:\n",
    "- **CC**: Coordinating conjunction\n",
    "- **RB**: Adverb\n",
    "- **IN**: Preposition\n",
    "- **NN**: Noun, singular\n",
    "- **JJ**: Adjective\n",
    "\n",
    "Each tag helps in understanding the grammatical role of a word in the sentence, which is crucial for syntactic parsing and other language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Named Entity Recognition (NER)\n",
    "\n",
    "Named Entity Recognition (NER) is used to identify important named entities in the text—people, places, organizations—and label them with the appropriate entity types. NER is crucial for many applications such as building content recommenders, automating customer support, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/waseem/nltk_data...\n",
      "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to /Users/waseem/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity Recognition: (S\n",
      "  (PERSON Albert/NNP)\n",
      "  (PERSON Einstein/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Ulm/NNP)\n",
      "  ,/,\n",
      "  (GPE Germany/NNP)\n",
      "  in/IN\n",
      "  1879/CD\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "from nltk import pos_tag\n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "sentence = \"Albert Einstein was born in Ulm, Germany in 1879.\"\n",
    "tags = pos_tag(word_tokenize(sentence))\n",
    "entities = ne_chunk(tags)\n",
    "print(\"Named Entity Recognition:\", entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of `ne_chunk` when combined with `pos_tag` is a nested tree structure where each branch of the tree represents a chunk of the sentence.\n",
    "- **PERSON**: A named entity representing a person's name.\n",
    "- **GPE**: Geo-Political Entity, which includes countries, cities, states.\n",
    "- **NNP**: Proper noun, singular\n",
    "- **VBD**: Verb, past tense\n",
    "- **VBN**: Verb, past participle\n",
    "- **IN**: Preposition\n",
    "- **CD**: Cardinal number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entities like \"Albert Einstein\", \"Ulm\", and \"Germany\" are recognized and categorized, showing their importance and type within the context of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Parsing\n",
    "\n",
    "Parsing is the process of analyzing a string of symbols, either in natural language or computer languages, according to the rules of a formal grammar. In the context of NLP, parsing involves determining the parse tree (graphical representation of the syntactic structure) of a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing: (S\n",
      "  (NP The/DT quick/JJ brown/NN)\n",
      "  (NP fox/NN)\n",
      "  jumps/VBZ\n",
      "  over/IN\n",
      "  (NP the/DT lazy/JJ dog/NN))\n"
     ]
    }
   ],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "tags = pos_tag(word_tokenize(sentence))\n",
    "parsed_sentence = cp.parse(tags)\n",
    "print(\"Parsing:\", parsed_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of parsing with a defined grammar is a tree structure where each node represents a syntactic chunk based on the specified grammar.\n",
    "\n",
    "- **NP**: Noun Phrase\n",
    "- **DT**: Determiner\n",
    "- **JJ**: Adjective\n",
    "- **NN**: Noun, singular\n",
    "\n",
    "The parse tree identifies noun phrases in the sentence \"The quick brown fox jumps over the lazy dog\", helping in understanding the structure and components of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Sentiment Analysis Using VADER\n",
    "\n",
    "[VADER](https://github.com/cjhutto/vaderSentiment) (Valence Aware Dictionary and sEntiment Reasoner) is a popular sentiment analysis tool because of its specificity to social media contexts and its ability to understand modern colloquialisms, slang, and emojis. It provides positive, negative, and neutral sentiment scores, as well as a compound score that summarizes the overall sentiment.\n",
    "VADER analyzes text to classify sentiment into positive, negative, and neutral categories based on a lexicon of words and rules that interpret grammatical constructions of sentiment expressions. The compound score it generates is a normalized, weighted composite score that can be used to gauge the overall sentiment of a text. Scores range from -1 (most extreme negative) to +1 (most extreme positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this new phone, it's awesome!\n",
      "Scores: {'neg': 0.0, 'neu': 0.318, 'pos': 0.682, 'compound': 0.8622}\n",
      "Text: This is the worst movie I have ever seen.\n",
      "Scores: {'neg': 0.369, 'neu': 0.631, 'pos': 0.0, 'compound': -0.6249}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/waseem/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download VADER lexicon if not already downloaded\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Create a Sentiment Intensity Analyzer object\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Example texts\n",
    "texts = [\n",
    "    \"I love this new phone, it's awesome!\",\n",
    "    \"This is the worst movie I have ever seen.\"\n",
    "]\n",
    "\n",
    "# Apply VADER to each text\n",
    "for text in texts:\n",
    "    scores = sia.polarity_scores(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"Scores:\", scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the compound scores:\n",
    "- positive sentiment: compound score >= 0.05\n",
    "- neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n",
    "- negative sentiment: compound score <= -0.05\n",
    "\n",
    "Given the example texts:\n",
    "1. \"I love this new phone, it's awesome!\" - This text received a high positive score and low negative and neutral scores. The compound score should reflect strong positive sentiment.\n",
    "2. \"This is the worst movie I have ever seen.\" - This text showed a high negative score, low positive and neutral scores, and a compound score indicating strong negative sentiment.\n",
    "\n",
    "This method is especially useful in cases where quick, effective sentiment analysis is needed, and can be particularly effective in analyzing customer feedback, reviews, or social media data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[spaCy](https://spacy.io/) is a modern, robust library for Natural Language Processing (NLP) in Python.  It can be used to build information extraction or natural language understanding systems, or to pre-process text for deep learning. It is designed for practical, real-world applications and built for performance, with the ability to scale and integrate into large systems. spaCy provides pre-trained models for various languages and capabilities. Below we will explore some of its capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Word Vectors and Similarity\n",
    "\n",
    "Word vectors are numerical representations of words that capture their meanings, semantic relationships, and the contexts in which they appear. spaCy provides support for using these vectors to compute similarities between words, texts, and other linguistic units.\n",
    "Word vectors, or embeddings, are learned from the context in which words appear. They are fundamental in various NLP tasks because they help algorithms understand synonymy and semantic similarity between different pieces of text. spaCy can use pre-trained word embeddings from models like GloVe, or you can train your own. (Topic explored further in 1.2.2 Basic Text Processing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.7827692113563378\n"
     ]
    }
   ],
   "source": [
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "if not nlp.vocab.vectors.size:\n",
    "    raise ValueError(\"The model does not contain any word vectors.\")\n",
    "\n",
    "word1 = nlp(\"king\")\n",
    "word2 = nlp(\"prince\")\n",
    "\n",
    "# Compute similarity between two words\n",
    "similarity = word1.similarity(word2)\n",
    "print(\"Similarity:\", similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity score is on a scale from 0 to 1. The similarity between \"king\" and \"prince\", reflecting their semantic relation as royalty counterparts. This illustrates how word embeddings capture not just superficial lexical similarities but deeper semantic relationships, useful in tasks like semantic search, text clustering, and even creative language generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Sentence Segmentation\n",
    "\n",
    "Similar to Sentence Tokenization seen earlier, sentence segmentation involves dividing a text into its constituent sentences, which is a crucial first step in many NLP applications. spaCy's sentence segmentation algorithm uses a combination of tokenization rules and a statistical model to accurately predict sentence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "Here is another sentence.\n",
      "And yet another one.\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example text with multiple sentences\n",
    "text = \"Hello world! Here is another sentence. And yet another one.\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print each sentence in the document\n",
    "for sentence in doc.sents:\n",
    "    print(sentence.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Custom Rule-Based Entity Recognition\n",
    "\n",
    "Similar to NER but for situations where you might want to identify entities using custom, rule-based methods. This can be particularly useful for recognizing specific terms or patterns that are unique to a particular domain, such as product codes in retail or specific jargon in legal or medical texts.\n",
    "spaCy allows for the addition of rule-based matching capabilities via the `Matcher` and `PhraseMatcher` classes, which can be used to search the text for a series of tokens or phrases based on patterns you define. This feature is highly customizable and can be used in conjunction with the statistical models to enhance the entity recognition capabilities of your NLP pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: 10 dollars\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define the pattern\n",
    "pattern = [{'IS_DIGIT': True}, {'LOWER': {'IN': ['dollars', 'dollar', 'usd']}}]\n",
    "matcher.add(\"MONEY\", [pattern])\n",
    "\n",
    "text = \"I need a subscription for 10 dollars.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Apply the matcher to the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the custom rule-based entity recognition, we defined a pattern to identify monetary values, for the input text: \"I need a subscription for 10 dollars.\"\n",
    "\n",
    "Given this input and the pattern we defined (`{'IS_DIGIT': True}, {'LOWER': {'IN': ['dollars', 'dollar', 'usd']}}`), the matcher would identify \"10 dollars\" as a match because:\n",
    "- \"10\" matches the first part of the pattern (`{'IS_DIGIT': True}`), indicating it’s a digit.\n",
    "- \"dollars\" matches the second part of the pattern (`{'LOWER': {'IN': ['dollars', 'dollar', 'usd']}}`), confirming it’s one of the specified keywords.\n",
    "\n",
    "The output would then correctly highlight \"10 dollars\" as an entity of type \"MONEY\", showcasing how rule-based patterns can effectively identify specific entities in text that fit predefined criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Text Classification\n",
    "\n",
    "Text classification is another powerful feature spaCy can be adapted for. While spaCy doesn't provide direct, built-in methods for text classification like some other libraries, it can be used to prepare data and features for a classifier and to deploy a classification model.\n",
    "Text classification involves assigning categories or labels to text based on its content. This is commonly used in sentiment analysis, topic categorization, and spam detection. In spaCy, you can use the `Doc` and `Span` objects to extract features and train a classifier using any machine learning library, such as scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for 'The product was good': Positive\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample texts and labels\n",
    "texts = [\"I love this phone\", \"This movie is terrible\", \"What a wonderful day\"]\n",
    "labels = [1, 0, 1]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Use spaCy to process texts and create a feature extractor\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Create a classification pipeline\n",
    "classifier = make_pipeline(vectorizer, MultinomialNB())\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(texts, labels)\n",
    "\n",
    "# Test the classifier\n",
    "test_text = \"The product was good\"\n",
    "prediction = classifier.predict([test_text])\n",
    "print(\"Prediction for '{}': {}\".format(test_text, \"Positive\" if prediction[0] == 1 else \"Negative\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a basic sentiment analysis setup with texts labeled as either positive (1) or negative (0). The classifier was trained on a few sample texts, and we then tested it with the text: \"The product was good\".\n",
    "\n",
    "This prediction indicates that the classifier successfully associated the word \"good\" with positive sentiment, based on its training data. It demonstrates how text classification can be applied to analyze sentiments in customer feedback, reviews, or any textual data where understanding sentiment or categorizing text is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both NLTK and spaCy offer comprehensive suites of tools and functionalities for natural language processing, each with its own strengths. While NLTK provides a broad array of algorithms and is highly suitable for academic and educational purposes, spaCy excels in performance and offers industrial-strength capabilities with a focus on efficiency and scalability. Choosing between the two depends largely on the specific requirements of the project, including the complexity of tasks involved and the need for speed and efficiency in processing large volumes of text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
