{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Text Processing Techniques\n",
    "\n",
    "In this notebook, we will explore some fundamental text processing techniques used in natural language processing (NLP). These techniques are essential for preparing text data for more complex tasks like question answering, sentiment analysis, and other machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1. Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down a string or text into smaller components called tokens. Tokens are typically words or phrases, but they can also include punctuation and other symbols. The main purpose of tokenization is to simplify the text data by reducing it to manageable units for further processing.\n",
    "\n",
    "There are several methods of tokenization:\n",
    "- **Word tokenization**: Splits the text by words.\n",
    "- **Sentence tokenization**: Splits the text into sentences.\n",
    "- **Subword tokenization**: Splits words into smaller units (like syllables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokens: ['Hello', 'world', '!', 'This', 'is', 'an', 'example', 'of', 'word', 'tokenization', '.']\n",
      "Sentence Tokens: ['Hello world!', 'This is an example of word tokenization.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/waseem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello world! This is an example of word tokenization.\"\n",
    "\n",
    "# Word tokenization\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "print(\"Word Tokens:\", word_tokens)\n",
    "\n",
    "# Sentence tokenization\n",
    "sentence_tokens = nltk.sent_tokenize(text)\n",
    "print(\"Sentence Tokens:\", sentence_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stemming\n",
    "\n",
    "Stemming is a process of reducing words to their word stem or root form. The base or root form of a word may not be a valid word itself. Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['run', 'runner', 'run', 'ran', 'run', 'easili', 'fairli']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Example words\n",
    "words = ['run', 'runner', 'running', 'ran', 'runs', 'easily', 'fairly']\n",
    "\n",
    "# Applying stemming\n",
    "stemmed_words = [porter.stem(word) for word in words]\n",
    "print(\"Stemmed Words:\", stemmed_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lemmatization\n",
    "\n",
    "Lemmatization is similar to stemming but it brings context to the words. It links words with similar meanings to one word. Text analysis is often improved by this method as it reduces words to their dictionary form (lemma).\n",
    "\n",
    "Unlike stemming, lemmatization considers the morphological analysis of the words. To do this, it is necessary to have detailed dictionaries which the algorithm can look through to link the word to its lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/waseem/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['run', 'runner', 'running', 'ran', 'run', 'easily', 'fairly']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example words\n",
    "words = ['run', 'runner', 'running', 'ran', 'runs', 'easily', 'fairly']\n",
    "\n",
    "# Applying lemmatization\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stop Word Removal\n",
    "\n",
    "Stop words are common words that are usually filtered out before processing natural language data (text). Words like \"and\", \"the\", \"a\", and similar are considered stop words because they appear frequently and are unlikely to contribute to the meaning of text for many tasks. The removal of stop words can help in reducing the dataset size and improving the performance of the processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: example filter stop words sentence .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/waseem/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"This is an example of how to filter out stop words in a sentence.\"\n",
    "\n",
    "# Removing stop words\n",
    "filtered_sentence = ' '.join([word for word in nltk.word_tokenize(sentence) if word.lower() not in stop_words])\n",
    "print(\"Filtered Sentence:\", filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word Embeddings\n",
    "\n",
    "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. They are a set of feature learning techniques in NLP where words or phrases from the vocabulary are mapped to vectors of real numbers. Conceptually it involves the mathematical embedding from a space with many dimensions per word to a continuous vector space with much lower dimension.\n",
    "\n",
    "Common models to generate word embeddings include:\n",
    "- **Word2Vec**: Uses neural networks to learn word associations from a large corpus of text.\n",
    "- **GloVe (Global Vectors for Word Representation)**: Uses matrix factorization based on global word-word co-occurrence to provide vector representations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/embedding.png\" alt=\"drawing\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install gensim wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vector for 'sentence': [-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "             ['this', 'is', 'the', 'second', 'sentence'],\n",
    "             ['yet', 'another', 'sentence'],\n",
    "             ['one', 'more', 'sentence'],\n",
    "             ['and', 'the', 'final', 'sentence']]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Access vector for one word\n",
    "word_vector = model.wv['sentence']\n",
    "print(\"Word Vector for 'sentence':\", word_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Model Training with Text8 Corpus\n",
    "\n",
    "Next, we'll train a Word2Vec model using the `text8` dataset. The `text8` corpus is a compact version of Wikipedia text that has been cleaned and formatted specifically for use in natural language processing tasks. After training, we'll demonstrate how to use this model to find words that are most similar to a given input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "corpus = api.load('text8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dataset, let's load it and train a Word2Vec model using the Gensim library. Training a Word2Vec model involves specifying parameters like the size of the word vectors (`vector_size`), the size of the context window (`window`), and the minimum count of words (`min_count`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus,vector_size=100, window=5, min_count=150, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'king': [('graduate', 0.7455926537513733), ('teacher', 0.7073326706886292), ('students', 0.6708106398582458), ('undergraduate', 0.6443020701408386), ('faculty', 0.6319653987884521), ('professor', 0.622151255607605), ('school', 0.6202729940414429), ('harvard', 0.6047377586364746), ('bachelor', 0.6045958995819092), ('institution', 0.5951842665672302)]\n"
     ]
    }
   ],
   "source": [
    "# Find words similar to 'student'\n",
    "similar_words = model.wv.most_similar('student', topn=10)\n",
    "print(\"Words similar to 'student':\", similar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Text Normalization\n",
    "\n",
    "Text normalization is the process of transforming text into a more uniform format. This can include converting all characters to lowercase, removing punctuation, eliminating special characters, and correcting common misspellings. These steps can help in standardizing the text data and improving the performance of NLP models by reducing the number of unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Text: this is an example it should be normalized right nlp\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"This is an Example! It SHOULD be normalized, right? #NLP\"\n",
    "\n",
    "# Normalize text\n",
    "normalized_text = text.lower().replace('!', '').replace(',', '').replace('?', '').replace('#', '')\n",
    "\n",
    "print(\"Normalized Text:\", normalized_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
