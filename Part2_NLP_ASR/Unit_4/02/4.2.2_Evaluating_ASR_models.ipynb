{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of ASR Systems\n",
    "\n",
    "This notebook demonstrates the evaluation of Automatic Speech Recognition (ASR) systems using the Word Error Rate (WER) metric and Character Error Rate (CER)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Word Error Rate\n",
    "\n",
    "Word Error Rate (WER) is a crucial metric used to evaluate the performance of an Automatic Speech Recognition (ASR) system. It measures how accurately the ASR system transcribes spoken language into text by comparing the machine-generated transcription to a human-generated reference transcription.\n",
    "\n",
    "The formula for WER is:\n",
    "\n",
    "$$ WER = \\frac{S + D + I}{N} $$\n",
    "\n",
    "Where:\n",
    "- \\( S \\) represents the number of substitutions, which occur when a word from the reference is replaced by a different word in the hypothesis.\n",
    "- \\( D \\) represents the number of deletions, where a word from the reference is missing in the hypothesis.\n",
    "- \\( I \\) represents the number of insertions, where a word not present in the reference appears in the hypothesis.\n",
    "- \\( N \\) is the total number of words in the reference transcription.\n",
    "\n",
    "The WER gives us a percentage that reflects the proportion of errors (substitutions, deletions, and insertions) in the hypothesis compared to the total number of words in the reference. A WER of 0% means perfect transcription, while a higher WER indicates more discrepancies between the hypothesis and the reference.\n",
    "\n",
    "### Intuition behind WER\n",
    "\n",
    "WER is designed to provide a simple, yet effective way to quantify the performance of an ASR system. The metric helps us understand how well a system understands and processes natural language in spoken form. By identifying how many changes one would need to make to transform the hypothesis into the reference, WER offers insights into both the accuracy and reliability of the ASR system.\n",
    "\n",
    "Additionally, analyzing the types of errors (substitutions, deletions, and insertions) can help developers fine-tune the ASR algorithms, focusing on reducing specific kinds of errors that are more prevalent or more impactful on the user experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Next, we take a look at calculating WER in Python. First, let's install `jiwer` for calculating the Word Error Rate.\n",
    "\n",
    "```bash\n",
    "pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import jiwer  # This is a simple library to calculate WER\n",
    "\n",
    "# Ensure that PyTorch is installed\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Data\n",
    "\n",
    "We will use a set of simulated transcriptions and their references to illustrate how WER is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "references = [\n",
    "    \"hello world\",\n",
    "    \"this is a test\",\n",
    "    \"the quick brown fox jumps over the lazy dog\"\n",
    "]\n",
    "\n",
    "hypotheses = [\n",
    "    \"helo world\",\n",
    "    \"this is test\",\n",
    "    \"the quick brown fox jump over the lazy\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 1: hello world\n",
      "Hypothesis 1: helo world\n",
      "WER: 50.00%\n",
      "\n",
      "Reference 2: this is a test\n",
      "Hypothesis 2: this is test\n",
      "WER: 25.00%\n",
      "\n",
      "Reference 3: the quick brown fox jumps over the lazy dog\n",
      "Hypothesis 3: the quick brown fox jump over the lazy\n",
      "WER: 22.22%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate WER\n",
    "def calculate_wer(references, hypotheses):\n",
    "    wer_scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        wer_score = jiwer.wer(ref, hyp)\n",
    "        wer_scores.append(wer_score)\n",
    "    return wer_scores\n",
    "\n",
    "# Calculate WER for each pair\n",
    "wer_scores = calculate_wer(references, hypotheses)\n",
    "\n",
    "# Display the results\n",
    "for i, score in enumerate(wer_scores):\n",
    "    print(f\"Reference {i+1}: {references[i]}\")\n",
    "    print(f\"Hypothesis {i+1}: {hypotheses[i]}\")\n",
    "    print(f\"WER: {score:.2%}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of WER Results\n",
    "\n",
    "#### Example 1:\n",
    "- **Reference**: \"hello world\"\n",
    "- **Hypothesis**: \"helo world\"\n",
    "- **WER**: 50.00%\n",
    "\n",
    "**Breakdown**:\n",
    "- **Substitutions**: 0 (No words were wrongly replaced; \"hello\" was shortened but not replaced)\n",
    "- **Deletions**: 0 (All words in the reference appear in the hypothesis)\n",
    "- **Insertions**: 0 (No extra words were added)\n",
    "- **Modifications**: 1 (\"hello\" was misspelled as \"helo\")\n",
    "- **Total Words in Reference (N)**: 2\n",
    "\n",
    "In this case, there is one modification (misspelling), which is treated as a substitution in WER calculation. Since there is 1 error and there are 2 reference words:\n",
    "\n",
    "$$ WER = \\frac{1}{2} = 50\\% $$\n",
    "\n",
    "This high WER reflects a significant error given the short length of the reference.\n",
    "\n",
    "#### Example 2:\n",
    "- **Reference**: \"this is a test\"\n",
    "- **Hypothesis**: \"this is test\"\n",
    "- **WER**: 25.00%\n",
    "\n",
    "**Breakdown**:\n",
    "- **Substitutions**: 0\n",
    "- **Deletions**: 1 (\"a\" is missing in the hypothesis)\n",
    "- **Insertions**: 0\n",
    "- **Total Words in Reference (N)**: 4\n",
    "\n",
    "The hypothesis is missing one word:\n",
    "\n",
    "$$ WER = \\frac{1}{4} = 25\\% \\$$\n",
    "\n",
    "The WER indicates that to make the hypothesis match the reference, 25% of the reference words need to be considered.\n",
    "\n",
    "#### Example 3:\n",
    "- **Reference**: \"the quick brown fox jumps over the lazy dog\"\n",
    "- **Hypothesis**: \"the quick brown fox jump over the lazy\"\n",
    "- **WER**: 22.22%\n",
    "\n",
    "**Breakdown**:\n",
    "- **Substitutions**: 0\n",
    "- **Deletions**: 1 (\"jumps\" to \"jump\" - treating as a grammatical error involving pluralization)\n",
    "- **Insertions**: 0\n",
    "- **Total Words in Reference (N)**: 9\n",
    "\n",
    "Again, there's one deletion:\n",
    "\n",
    "$$ WER = \\frac{1}{9} \\approx 22.22\\% $$\n",
    "\n",
    "This WER indicates that about 22.22% of the reference words are in error due to the hypothesis's lack of alignment with the reference, which in this case is the missing plural form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Error Rate (CER)\n",
    "\n",
    "CER is similar to WER but measures the performance at the character level. This metric is particularly useful for languages where the character is a more significant unit than the word, such as in Mandarin or Japanese. CER is calculated by:\n",
    "\n",
    "$$ CER = \\frac{S_c + D_c + I_c}{N_c} $$\n",
    "\n",
    "Where:\n",
    "- **`S_c`** is the number of character substitutions,\n",
    "- **`D_c`** is the number of character deletions,\n",
    "- **`I_c`** is the number of character insertions,\n",
    "- **`N_c`** is the number of characters in the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference 1: hello world\n",
      "Hypothesis 1: helo world\n",
      "CER: 9.09%\n",
      "\n",
      "Reference 2: this is a test\n",
      "Hypothesis 2: this is test\n",
      "CER: 14.29%\n",
      "\n",
      "Reference 3: the quick brown fox jumps over the lazy dog\n",
      "Hypothesis 3: the quick brown fox jump over the lazy\n",
      "CER: 11.63%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import jiwer\n",
    "\n",
    "def calculate_cer(references, hypotheses):\n",
    "    cer_scores = []\n",
    "    for ref, hyp in zip(references, hypotheses):\n",
    "        cer_score = jiwer.cer(ref, hyp)\n",
    "        cer_scores.append(cer_score)\n",
    "    return cer_scores\n",
    "\n",
    "# Example data\n",
    "references = [\n",
    "    \"hello world\",\n",
    "    \"this is a test\",\n",
    "    \"the quick brown fox jumps over the lazy dog\"\n",
    "]\n",
    "\n",
    "hypotheses = [\n",
    "    \"helo world\",\n",
    "    \"this is test\",\n",
    "    \"the quick brown fox jump over the lazy\"\n",
    "]\n",
    "\n",
    "# Calculate CER for each pair\n",
    "cer_scores = calculate_cer(references, hypotheses)\n",
    "\n",
    "# Display the results\n",
    "for i, score in enumerate(cer_scores):\n",
    "    print(f\"Reference {i+1}: {references[i]}\")\n",
    "    print(f\"Hypothesis {i+1}: {hypotheses[i]}\")\n",
    "    print(f\"CER: {score:.2%}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences Between WER and CER\n",
    "\n",
    "- **Context Sensitivity**: WER is considered less forgiving than CER because it requires a correct match at the word level, which often contains more contextual information. A single error in a long word is counted the same as an error in a short word, potentially making WER a harsher metric.\n",
    "- **Language Suitability**: CER is often more suitable for languages with a character-based writing system, such as Mandarin and Japanese. In these languages, words can consist of one or more characters, and the distinction between words is less clear than in space-delimited languages like English. Thus, CER can provide a more accurate reflection of the transcription accuracy for these languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Summary\n",
    "Both WER and CER are essential for diagnosing and improving ASR systems. Understanding where and how errors occur allows developers to make targeted improvements, enhancing both the accuracy and reliability of ASR systems. WER is generally more sensitive to word-level errors and can be unforgiving, especially in short texts. In contrast, CER offers a more granular insight into character-level inaccuracies, making it particularly relevant for languages where character precision is crucial. By calculating and analyzing these metrics, we can better understand the strengths and limitations of ASR techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
