{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a pre-trained model for ASR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wave2Vec 2.0 is a powerful model developed by Facebook for converting speech to text. Here, we'll demonstrate how fine-tuning this model can improve its accuracy, even with minimal additional training. We'll use the example of an audio clip of the word \"Visual\", which is initially misrecognized by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Fine-Tuning\n",
    "Let's see how the pre-trained model performs without any fine-tuning in the below steps:\n",
    "- Load the pre-trained Wave2Vec 2.0 model.\n",
    "- Read the audio file and converts it into the format the model expects.\n",
    "- Use the model to predict text from audio without any fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription: FISIAL\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "model_name = \"facebook/wav2vec2-base-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "\n",
    "audio_file = 'data/visual.wav'\n",
    "audio_input, sample_rate = librosa.load(audio_file, sr=16000)\n",
    "\n",
    "input_values = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "print(\"Transcription:\", transcription)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Fine-Tuning\n",
    "Now, let's fine-tune the model on the same audio data to see if we can improve the accuracy of the transcription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and Loading Data\n",
    "We start by setting up the environment and loading our training data (the misrecognized audio file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CTCLoss\n",
    "\n",
    "# Load processor and model\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model.train()\n",
    "\n",
    "# Load your custom audio file and its corresponding transcript\n",
    "audio_file_path = 'data/visual.wav'\n",
    "transcript = \"visual\"\n",
    "\n",
    "# Load audio\n",
    "speech_array, sampling_rate = torchaudio.load(audio_file_path)\n",
    "\n",
    "# Ensure the sampling rate is correct\n",
    "if sampling_rate != 16000:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
    "    speech_array = resampler(speech_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Audio and Label Data\n",
    "Convert the raw audio into the model's input format and encode the correct transcription into label format for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the audio\n",
    "input_values = processor(speech_array.squeeze(), sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "# Process transcript using batch encoding\n",
    "with processor.as_target_processor():\n",
    "    labels = processor([transcript], return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Configuration\n",
    "Set up the device for computations, the optimizer for adjusting model weights, and the loss function to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "input_values = input_values.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# Set up the optimizer and the loss function\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss_func = CTCLoss(blank=processor.tokenizer.pad_token_id, zero_infinity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training & Evaluation\n",
    "Conduct 5 epochs of training. This involves a forward pass (computing the logits), calculating the loss, and updating the model using backpropagation. After training, we use the model to predict the transcription of the audio file again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: -31.09031867980957\n",
      "FISUAL\n",
      "Epoch 2, Loss: -30.60826301574707\n",
      "FISUAL\n",
      "Epoch 3, Loss: -29.554391860961914\n",
      "VISUAL\n",
      "Epoch 4, Loss: -25.355257034301758\n",
      "VUSUAL\n",
      "Epoch 5, Loss: -15.027382850646973\n",
      "VISUAL\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5  # Adjust based on experiment observations\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "    # Compute the input lengths for the CTC loss\n",
    "    input_lengths = torch.full(size=(logits.size(0),), fill_value=logits.size(1), dtype=torch.long)\n",
    "    label_lengths = torch.full(size=(labels.size(0),), fill_value=labels.size(1), dtype=torch.long)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = loss_func(logits.transpose(0, 1), labels, input_lengths, label_lengths)\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Evaluation in each epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    predicted_text = processor.batch_decode(predicted_ids)[0]\n",
    "    print(predicted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script provides a very basic example of fine-tuning a Wav2Vec2 model on a single data point. We can see that with fine-tuning the model over 5 epochs, it has improved and learnt the correct transct of the audio files. However, fine-tuning on such a specific and small piece of data is for illustrative purposes. \n",
    "In a real-world scenario, one would need to manage larger datasets and more sophisticated training routines involving, more epochs, hyper-parameter tuning, validation and possibly early stopping based on performance metrics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
