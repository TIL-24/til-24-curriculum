{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Question-Answering Models\n",
    "\n",
    "Evaluating question-answering (QA) models is a critical step in the development of AI systems that can understand and process human language. Question-answering systems are widely used in a range of applications, from customer service automation to interactive educational systems. The reliability of these systems depends heavily on their ability to understand questions correctly and provide accurate and relevant answers. Thus, evaluating these models with robust metrics ensures that they perform well across diverse scenarios and datasets. These models are typically evaluated using a set of metrics that determine how well the model's answers match the expected answers. In this notebok, we explore key metrics used in evaluating QA models such as Exact Match, F1 Score.\n",
    "\n",
    "### Exact Match (EM)\n",
    "The Exact Match metric is the strictest form of evaluation for QA models. It measures whether the predicted answer matches the ground truth answer exactly, on a case-insensitive basis. This metric is binary, providing a score of 1 for a perfect match and 0 otherwise.\n",
    "\n",
    "### F1 Score\n",
    "The F1 Score is used to evaluate the model's accuracy on a token level, balancing precision and recall. It considers both the tokens that are present in the predicted answer and the tokens that should have been included, offering a more nuanced assessment than the Exact Match metric.\n",
    "\n",
    "Next we will, explore the evaluation of question-answering models using PyTorch and the transformers library. We'll use a small example dataset for demonstration purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "Before evaluating the models, you need to prepare your dataset. Here's how you can create and preprocess a sample dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n",
    "Before evaluating the models, we need to prepare your dataset. This involves loading the data, preprocessing it, and preparing it for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "data = {\n",
    "    \"context\": [\n",
    "        \"Paris is the capital and most populous city of France.\",\n",
    "        \"The Pacific Ocean is the largest and deepest of Earth's oceanic divisions.\",\n",
    "        \"Python is a high-level, interpreted, and general-purpose programming language.\",\n",
    "        \"Tesla, Inc. is an American electric vehicle and clean energy company based in Palo Alto, California.\",\n",
    "        \"The Nobel Prize is a set of annual international awards bestowed in several categories by Swedish and Norwegian institutions.\"\n",
    "    ],\n",
    "    \"question\": [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Which ocean is the largest?\",\n",
    "        \"What type of language is Python?\",\n",
    "        \"Where is Tesla based?\",\n",
    "        \"What is the Nobel Prize?\"\n",
    "    ],\n",
    "    \"answer\": [\n",
    "        \"Paris\",\n",
    "        \"The Pacific Ocean\",\n",
    "        \"a high-level, interpreted, and general-purpose programming language\",\n",
    "        \"Palo Alto, California\",\n",
    "        \"a set of annual international awards\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Tokenizing the data\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def preprocess(question, context):\n",
    "    return tokenizer(question, context, return_tensors='pt', truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "df['encoded'] = df.apply(lambda row: preprocess(row['question'], row['context']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation\n",
    "We define the evaluation metrics and functions here, typically involving exact match (EM) and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/waseem/Desktop/Files/Tribe/venv_ml/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/waseem/Desktop/Files/Tribe/venv_ml/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/waseem/Desktop/Files/Tribe/venv_ml/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>True Answer</th>\n",
       "      <th>Predicted Answer</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>BLEU Score</th>\n",
       "      <th>ROUGE Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "      <td>paris</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.821832e-231</td>\n",
       "      <td>{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which ocean is the largest?</td>\n",
       "      <td>The Pacific Ocean</td>\n",
       "      <td>pacific ocean</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>9.047425e-155</td>\n",
       "      <td>{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What type of language is Python?</td>\n",
       "      <td>a high-level, interpreted, and general-purpose...</td>\n",
       "      <td>high - level , interpreted , and general - pur...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.189646e-231</td>\n",
       "      <td>{'rouge-1': {'r': 0.2857142857142857, 'p': 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where is Tesla based?</td>\n",
       "      <td>Palo Alto, California</td>\n",
       "      <td>palo alto , california</td>\n",
       "      <td>0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.531972e-231</td>\n",
       "      <td>{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the Nobel Prize?</td>\n",
       "      <td>a set of annual international awards</td>\n",
       "      <td>a set of annual international awards bestowed ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>3.237723e-01</td>\n",
       "      <td>{'rouge-1': {'r': 1.0, 'p': 0.4, 'f': 0.571428...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Question  \\\n",
       "0    What is the capital of France?   \n",
       "1       Which ocean is the largest?   \n",
       "2  What type of language is Python?   \n",
       "3             Where is Tesla based?   \n",
       "4          What is the Nobel Prize?   \n",
       "\n",
       "                                         True Answer  \\\n",
       "0                                              Paris   \n",
       "1                                  The Pacific Ocean   \n",
       "2  a high-level, interpreted, and general-purpose...   \n",
       "3                              Palo Alto, California   \n",
       "4               a set of annual international awards   \n",
       "\n",
       "                                    Predicted Answer  Exact Match  F1 Score  \\\n",
       "0                                              paris            1  1.000000   \n",
       "1                                      pacific ocean            0  0.800000   \n",
       "2  high - level , interpreted , and general - pur...            0  0.222222   \n",
       "3                             palo alto , california            0  0.571429   \n",
       "4  a set of annual international awards bestowed ...            0  0.571429   \n",
       "\n",
       "      BLEU Score                                        ROUGE Score  \n",
       "0  1.821832e-231  {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'r...  \n",
       "1  9.047425e-155  {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'r...  \n",
       "2  1.189646e-231  {'rouge-1': {'r': 0.2857142857142857, 'p': 0.2...  \n",
       "3  1.531972e-231  {'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'r...  \n",
       "4   3.237723e-01  {'rouge-1': {'r': 1.0, 'p': 0.4, 'f': 0.571428...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def exact_match(prediction, truth):\n",
    "    return int(prediction.strip().lower() == truth.strip().lower())\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = prediction.lower().split()\n",
    "    truth_tokens = truth.lower().split()\n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    if not common_tokens:\n",
    "        return 0\n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "# Function to calculate BLEU score\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference = [reference.lower().split()]\n",
    "    candidate = candidate.lower().split()\n",
    "    score = sentence_bleu(reference, candidate)\n",
    "    return score\n",
    "\n",
    "# Function to calculate ROUGE scores\n",
    "def calculate_rouge(reference, candidate):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(candidate, reference)[0]\n",
    "    return scores\n",
    "\n",
    "# Function to calculate METEOR score\n",
    "def calculate_meteor(reference, candidate):\n",
    "    score = meteor_score([reference], candidate)\n",
    "    return score\n",
    "\n",
    "def evaluate(model, dataset):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for _, row in dataset.iterrows():\n",
    "            inputs = row['encoded']\n",
    "            output = model(**inputs)\n",
    "            answer_start = torch.argmax(output.start_logits)\n",
    "            answer_end = torch.argmax(output.end_logits) + 1\n",
    "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze()[answer_start:answer_end]))\n",
    "            results.append((row['question'], row['answer'], answer))\n",
    "    return results\n",
    "\n",
    "evaluation_results = evaluate(model, df)\n",
    "evaluation_df = pd.DataFrame(evaluation_results, columns=['Question', 'True Answer', 'Predicted Answer'])\n",
    "evaluation_df['Exact Match'] = [exact_match(pred, true) for pred, true in zip(evaluation_df['Predicted Answer'], evaluation_df['True Answer'])]\n",
    "evaluation_df['F1 Score'] = [compute_f1(pred, true) for pred, true in zip(evaluation_df['Predicted Answer'], evaluation_df['True Answer'])]\n",
    "evaluation_df['BLEU Score'] = [calculate_bleu(true, pred) for true, pred in zip(evaluation_df['True Answer'], evaluation_df['Predicted Answer'])]\n",
    "evaluation_df['ROUGE Score'] = [calculate_rouge(true, pred) for true, pred in zip(evaluation_df['True Answer'], evaluation_df['Predicted Answer'])]\n",
    "# evaluation_df['METEOR Score'] = [calculate_meteor(true, pred) for true, pred in zip(evaluation_df['True Answer'], evaluation_df['Predicted Answer'])]\n",
    "\n",
    "evaluation_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/waseem/Desktop/Files/Tribe/venv_ml/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/waseem/Desktop/Files/Tribe/venv_ml/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/waseem/Desktop/Files/Tribe/venv_ml/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>True Answer</th>\n",
       "      <th>Predicted Answer</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>BLEU Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "      <td>paris</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.821832e-231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which ocean is the largest?</td>\n",
       "      <td>The Pacific Ocean</td>\n",
       "      <td>pacific ocean</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>9.047425e-155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What type of language is Python?</td>\n",
       "      <td>a high-level, interpreted, and general-purpose...</td>\n",
       "      <td>high - level , interpreted , and general - pur...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1.189646e-231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where is Tesla based?</td>\n",
       "      <td>Palo Alto, California</td>\n",
       "      <td>palo alto , california</td>\n",
       "      <td>0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.531972e-231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the Nobel Prize?</td>\n",
       "      <td>a set of annual international awards</td>\n",
       "      <td>a set of annual international awards bestowed ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>3.237723e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Question  \\\n",
       "0    What is the capital of France?   \n",
       "1       Which ocean is the largest?   \n",
       "2  What type of language is Python?   \n",
       "3             Where is Tesla based?   \n",
       "4          What is the Nobel Prize?   \n",
       "\n",
       "                                         True Answer  \\\n",
       "0                                              Paris   \n",
       "1                                  The Pacific Ocean   \n",
       "2  a high-level, interpreted, and general-purpose...   \n",
       "3                              Palo Alto, California   \n",
       "4               a set of annual international awards   \n",
       "\n",
       "                                    Predicted Answer  Exact Match  F1 Score  \\\n",
       "0                                              paris            1  1.000000   \n",
       "1                                      pacific ocean            0  0.800000   \n",
       "2  high - level , interpreted , and general - pur...            0  0.222222   \n",
       "3                             palo alto , california            0  0.571429   \n",
       "4  a set of annual international awards bestowed ...            0  0.571429   \n",
       "\n",
       "      BLEU Score  \n",
       "0  1.821832e-231  \n",
       "1  9.047425e-155  \n",
       "2  1.189646e-231  \n",
       "3  1.531972e-231  \n",
       "4   3.237723e-01  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Preprocessing function to tokenize the data\n",
    "def preprocess(question, context):\n",
    "    return tokenizer.encode_plus(question, context, return_tensors='pt', max_length=512, truncation=True, padding='max_length', add_special_tokens=True)\n",
    "\n",
    "df['encoded'] = df.apply(lambda row: preprocess(row['question'], row['context']), axis=1)\n",
    "\n",
    "# Evaluate the model on the provided data\n",
    "evaluation_results = evaluate(model, df)\n",
    "evaluation_df = pd.DataFrame(evaluation_results, columns=['Question', 'True Answer', 'Predicted Answer'])\n",
    "evaluation_df['Exact Match'] = [exact_match(pred, true) for pred, true in zip(evaluation_df['Predicted Answer'], evaluation_df['True Answer'])]\n",
    "evaluation_df['F1 Score'] = [compute_f1(pred, true) for pred, true in zip(evaluation_df['Predicted Answer'], evaluation_df['True Answer'])]\n",
    "\n",
    "evaluation_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
