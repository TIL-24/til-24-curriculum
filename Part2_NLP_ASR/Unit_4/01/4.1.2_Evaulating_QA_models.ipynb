{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Question-Answering Models\n",
    "\n",
    "Evaluating question-answering (QA) models is a critical step in the development of AI systems that can understand and process human language. Question-answering systems are widely used in a range of applications, from customer service automation to interactive educational systems. The reliability of these systems depends heavily on their ability to understand questions correctly and provide accurate and relevant answers. Thus, evaluating these models with robust metrics ensures that they perform well across diverse scenarios and datasets. These models are typically evaluated using a set of metrics that determine how well the model's answers match the expected answers. In this notebok, we explore key metrics used in evaluating QA models such as Exact Match, F1 Score.\n",
    "\n",
    "### Exact Match (EM)\n",
    "The Exact Match metric is the strictest form of evaluation for QA models. It measures whether the predicted answer matches the ground truth answer exactly, on a case-insensitive basis. This metric is binary, providing a score of 1 for a perfect match and 0 otherwise.\n",
    "\n",
    "### F1 Score\n",
    "The F1 Score is used to evaluate the model's accuracy on a token level, balancing precision and recall. It considers both the tokens that are present in the predicted answer and the tokens that should have been included, offering a more nuanced assessment than the Exact Match metric.\n",
    "\n",
    "<img src=\"./imgs/metrics_example.webp\" alt=\"drawing\" width=\"650\"/>\n",
    "\n",
    "In addition to the Exact Match (EM) and F1 score metrics, some other commonly used metrics for NLP tasks are:\n",
    "\n",
    "#### BLEU Score\n",
    "The BLEU (Bilingual Evaluation Understudy) score measures the similarity of n-grams (contiguous sequences of n items, such as words) between the predicted and reference answers. A higher BLEU Score indicates a higher level of n-gram overlap.\n",
    "\n",
    "#### ROUGE Score:\n",
    "The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score evaluates the quality of a summary by comparing n-gram overlap, word sequences, and word pairs between the predicted and reference answers. A higher ROUGE Score suggests that the predicted answers are more similar to the reference answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n",
    "Before evaluating the models, we need to prepare a sample dataset. This involves loading the data, preprocessing it, and preparing it for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data\n",
    "data = {\n",
    "    \"context\": [\n",
    "        \"Paris is the capital and most populous city of France.\",\n",
    "        \"The Pacific Ocean is the largest and deepest of Earth's oceanic divisions.\",\n",
    "        \"Python is a high-level, interpreted, and general-purpose programming language.\",\n",
    "        \"Tesla, Inc. is an American electric vehicle and clean energy company based in Palo Alto, California.\",\n",
    "        \"The Nobel Prize is a set of annual international awards bestowed in several categories by Swedish and Norwegian institutions.\"\n",
    "    ],\n",
    "    \"question\": [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Which ocean is the largest?\",\n",
    "        \"What type of language is Python?\",\n",
    "        \"Where is Tesla based?\",\n",
    "        \"What is the Nobel Prize?\"\n",
    "    ],\n",
    "    \"answer\": [\n",
    "        \"Paris\",\n",
    "        \"The Pacific Ocean\",\n",
    "        \"a high-level, interpreted, and general-purpose programming language\",\n",
    "        \"Palo Alto, California\",\n",
    "        \"a set of annual international awards\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "We define the evaluation metrics and functions here, typically involving exact match (EM) and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(prediction, truth):\n",
    "    return int(prediction.strip().lower() == truth.strip().lower())\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = prediction.lower().split()\n",
    "    truth_tokens = truth.lower().split()\n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    if not common_tokens:\n",
    "        return 0\n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "def evaluate(model, dataset):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for _, row in dataset.iterrows():\n",
    "            inputs = row['encoded']\n",
    "            output = model(**inputs)\n",
    "            answer_start = torch.argmax(output.start_logits)\n",
    "            answer_end = torch.argmax(output.end_logits) + 1\n",
    "            answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'].squeeze()[answer_start:answer_end]))\n",
    "            results.append((row['question'], row['answer'], answer))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison\n",
    "Next, we compare the performance of two models on the scores - the base BERT model and a fine-tuned BERT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and evaluate a pre-trained base BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>True Answer</th>\n",
       "      <th>Predicted Answer</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which ocean is the largest?</td>\n",
       "      <td>The Pacific Ocean</td>\n",
       "      <td>the pacific ocean is the largest and deepest of</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What type of language is Python?</td>\n",
       "      <td>a high-level, interpreted, and general-purpose...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where is Tesla based?</td>\n",
       "      <td>Palo Alto, California</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the Nobel Prize?</td>\n",
       "      <td>a set of annual international awards</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Question  \\\n",
       "0    What is the capital of France?   \n",
       "1       Which ocean is the largest?   \n",
       "2  What type of language is Python?   \n",
       "3             Where is Tesla based?   \n",
       "4          What is the Nobel Prize?   \n",
       "\n",
       "                                         True Answer  \\\n",
       "0                                              Paris   \n",
       "1                                  The Pacific Ocean   \n",
       "2  a high-level, interpreted, and general-purpose...   \n",
       "3                              Palo Alto, California   \n",
       "4               a set of annual international awards   \n",
       "\n",
       "                                  Predicted Answer  Exact Match  F1 Score  \n",
       "0                                                             0       0.0  \n",
       "1  the pacific ocean is the largest and deepest of            0       0.5  \n",
       "2                                                             0       0.0  \n",
       "3                                                             0       0.0  \n",
       "4                                                             0       0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Preprocessing function to tokenize the data\n",
    "def preprocess(question, context):\n",
    "    return tokenizer(question, context, return_tensors='pt', truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "df['encoded'] = df.apply(lambda row: preprocess(row['question'], row['context']), axis=1)\n",
    "evaluation_results = evaluate(model, df)\n",
    "evaluation_df = pd.DataFrame(evaluation_results, columns=['Question', 'True Answer', 'Predicted Answer'])\n",
    "evaluation_df['Exact Match'] = [exact_match(pred, true) for pred, true in zip(evaluation_df['Predicted Answer'], evaluation_df['True Answer'])]\n",
    "evaluation_df['F1 Score'] = [compute_f1(pred, true) for pred, true in zip(evaluation_df['Predicted Answer'], evaluation_df['True Answer'])]\n",
    "evaluation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>True Answer</th>\n",
       "      <th>Predicted Answer</th>\n",
       "      <th>Exact Match</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "      <td>paris</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which ocean is the largest?</td>\n",
       "      <td>The Pacific Ocean</td>\n",
       "      <td>pacific ocean</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What type of language is Python?</td>\n",
       "      <td>a high-level, interpreted, and general-purpose...</td>\n",
       "      <td>high - level , interpreted , and general - pur...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where is Tesla based?</td>\n",
       "      <td>Palo Alto, California</td>\n",
       "      <td>palo alto , california</td>\n",
       "      <td>0</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the Nobel Prize?</td>\n",
       "      <td>a set of annual international awards</td>\n",
       "      <td>a set of annual international awards bestowed ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.571429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Question  \\\n",
       "0    What is the capital of France?   \n",
       "1       Which ocean is the largest?   \n",
       "2  What type of language is Python?   \n",
       "3             Where is Tesla based?   \n",
       "4          What is the Nobel Prize?   \n",
       "\n",
       "                                         True Answer  \\\n",
       "0                                              Paris   \n",
       "1                                  The Pacific Ocean   \n",
       "2  a high-level, interpreted, and general-purpose...   \n",
       "3                              Palo Alto, California   \n",
       "4               a set of annual international awards   \n",
       "\n",
       "                                    Predicted Answer  Exact Match  F1 Score  \n",
       "0                                              paris            1  1.000000  \n",
       "1                                      pacific ocean            0  0.800000  \n",
       "2  high - level , interpreted , and general - pur...            0  0.222222  \n",
       "3                             palo alto , california            0  0.571429  \n",
       "4  a set of annual international awards bestowed ...            0  0.571429  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Preprocessing function to tokenize the data\n",
    "def preprocess(question, context):\n",
    "    return tokenizer.encode_plus(question, context, return_tensors='pt', max_length=512, truncation=True, padding='max_length', add_special_tokens=True)\n",
    "\n",
    "df['encoded'] = df.apply(lambda row: preprocess(row['question'], row['context']), axis=1)\n",
    "\n",
    "# Evaluate the model on the provided data\n",
    "evaluation_results = evaluate(model, df)\n",
    "evaluation_df = pd.DataFrame(evaluation_results, columns=['Question', 'True Answer', 'Predicted Answer'])\n",
    "evaluation_df['Exact Match'] = [exact_match(pred, true) for pred, true in zip(evaluation_df['Predicted Answer'], evaluation_df['True Answer'])]\n",
    "evaluation_df['F1 Score'] = [compute_f1(pred, true) for pred, true in zip(evaluation_df['Predicted Answer'], evaluation_df['True Answer'])]\n",
    "evaluation_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuned BERT model performs better than the base BERT model in both Exact Match and F1 Score. This improvement is expected because fine-tuning allows the model to adapt to the specific task of question-answering, leading to more accurate predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
