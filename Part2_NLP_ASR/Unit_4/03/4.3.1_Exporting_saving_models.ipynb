{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting and saving machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting and saving machine learning models is a crucial step in the model development process, allowing you to preserve the state of a model after training and deploy it in different environments. Here's a detailed guide on how to export and save machine learning models, focusing on various aspects and formats commonly used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Saving vs. Exporting\n",
    "\n",
    "**Saving**\n",
    "- Preserves the model's architecture, trained weights, and often associated configuration information (like hyperparameters or a vocabulary) so you don't need to train each time.\n",
    "- Primarily intended for future use within the same framework or closely related environments where you started training.\n",
    "\n",
    "**Exporting**:\n",
    "- Converts the model into a representation suitable for deployment in production environments or for use across different frameworks.\n",
    "- Often involves optimizations or format changes for better inference speed and compatibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Formats for Saving Models\n",
    "**Framework Specific Formats**\n",
    "- PyTorch: (`.pth` or `.pt`): Saves either the entire model or just the state dictionary, which includes weights and biases but not the architecture.\n",
    "\n",
    "- TensorFlow/Keras: (`.h5` or `SavedModel`): TensorFlow offers multiple ways to save models; as a single HDF5 file containing the architecture, weights, and training configuration, or as a SavedModel directory, which is a more comprehensive save format.\n",
    "\n",
    "**Framework-Agnostic Formats**\n",
    "- ONNX (Open Neural Network Exchange): A cross-platform format supported by many deep learning frameworks, which allows for model exchange between different tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Models in PyTorch\n",
    "To save a model in PyTorch, you typically use either `torch.save` for the whole model or just the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Entire Model\n",
    "torch.save(model, 'qa_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Only the State Dictionary\n",
    "torch.save(model.state_dict(), 'qa_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting Models to ONNX\n",
    "Exporting a model to ONNX requires the model to be in evaluation mode and a sample input to trace the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Create dummy input as required for the model to run\n",
    "inputs = tokenizer(\"What is AI?\", \"AI is Artificial Intelligence\", return_tensors=\"pt\")\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model, \n",
    "                  args=(inputs['input_ids'], inputs['attention_mask']), \n",
    "                  f=\"qa_model.onnx\",\n",
    "                  input_names=['input_ids', 'attention_mask'],\n",
    "                  output_names=['start_logits', 'end_logits'],\n",
    "                  dynamic_axes={'input_ids' : {0 : 'batch_size'},    # Variable batch size\n",
    "                                'attention_mask' : {0 : 'batch_size'},\n",
    "                                'start_logits' : {0 : 'batch_size'},\n",
    "                                'end_logits' : {0 : 'batch_size'}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Using Saved Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Loading Models in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the entire model\n",
    "model = torch.load('qa_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running an ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "session = ort.InferenceSession('qa_model.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations for Production\n",
    "- Version Control: Always version your models to manage updates smoothly.\n",
    "- Testing: Before deployment, rigorously test the model's performance and stability in an environment similar to production.\n",
    "- Optimization: Consider model optimization techniques for better performance, especially in resource-constrained environments (e.g., model quantization, pruning)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
