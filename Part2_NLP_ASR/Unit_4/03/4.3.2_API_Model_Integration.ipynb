{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Model Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will take a look at integrating the fine-tuned BERT model for question answering into a real-world application using FastAPI is a great way to deploy and utilize it in a production environment. The setup involves creating an API that receives a question and context as input and returns the predicted answer. We will create a Docker container for the FastAPI app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Saving the Model and Tokenizer\n",
    "After training the model in 4.1.1, you can save it along with its tokenizer to a directory. This is commonly done using the `save_pretrained()` method provided by the Hugging Face Transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering, AdamW\n",
    "import torch\n",
    "\n",
    "# Assume the rest of your model training setup is here\n",
    "\n",
    "model_path = \"qa_model.pth\"\n",
    "\n",
    "# Training loop here\n",
    "# After training:\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating the Saved Model into FastAPI\n",
    "Now that your model and tokenizer are saved, you can load them from the saved directory in your FastAPI application. This will allow your API to use the fine-tuned model to answer questions. The below example code is stored in `app.py` in the `qa_app` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "import os\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Fetch the model directory from the environment variable\n",
    "model_directory = os.getenv('MODEL_PATH', '/app/models')\n",
    "model_filename = 'qa_model.pth'  # Specify your model filename here\n",
    "\n",
    "# Full path to the model file\n",
    "model_path = os.path.join(model_directory, model_filename)\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "class QAInput(BaseModel):\n",
    "    question: str\n",
    "    context: str\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(data: QAInput):\n",
    "    # Encode the inputs\n",
    "    inputs = tokenizer.encode_plus(data.question, data.context, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        answer_start_scores, answer_end_scores = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    # Convert tokens to answer string\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][answer_start:answer_end]))\n",
    "\n",
    "    return {\"question\": data.question, \"context\": data.context, \"answer\": answer}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dockerfile\n",
    "Create a `Dockerfile` in the same directory as your FastAPI app (`app.py`). This file will define the Docker image that includes your app and all its dependencies.\n",
    "\n",
    "```Docker\n",
    "# Use an official Python runtime as a parent image\n",
    "FROM python:3.8-slim\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /usr/src/app\n",
    "\n",
    "# Copy the local directory contents into the container\n",
    "COPY . .\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Make port 8000 available to the world outside this container\n",
    "EXPOSE 8000\n",
    "\n",
    "# Define environment variable\n",
    "ENV MODEL_PATH=/usr/src/app/model\n",
    "\n",
    "# Run app.py when the container launches\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Requirements File\n",
    "Create a `requirements.txt` file that lists the packages that your app depends on. Make sure to include fastapi, uvicorn, torch, transformers, and any other required libraries.\n",
    "\n",
    "```txt\n",
    "fastapi\n",
    "uvicorn\n",
    "torch\n",
    "transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Docker Image\n",
    "From your project directory (where your `Dockerfile` and `app.py` are located), run the following command to build the Docker image\n",
    "```bash\n",
    "docker build -t qa_app ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/docker_build.png\" alt=\"drawing\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Docker Container\n",
    "```bash\n",
    "docker run -p 8000:8000 qa_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker runs the container and map port 8000 of the container to port 8000 on your host, allowing us to access the FastAPI application using the browser, `requests` library or Postman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/docker_run.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `qa_app` using `requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: {'question': 'When did the Titanic sink?', 'context': 'The RMS Titanic sank in the early morning hours of April 15, 1912, after colliding with an iceberg during its maiden voyage from Southampton to New York City.', 'answer': 'april 15 , 1912'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The endpoint URL\n",
    "url = 'http://localhost:8000/predict'\n",
    "\n",
    "# Example question and context\n",
    "data = {\n",
    "    \"question\": \"When did the Titanic sink?\",\n",
    "    \"context\": \"The RMS Titanic sank in the early morning hours of April 15, 1912, after colliding with an iceberg during its maiden voyage from Southampton to New York City.\"\n",
    "}\n",
    "\n",
    "# Sending a POST request\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Print the response from the server\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result shows that the model is successfully able to respond to the question. While this notebook provides a basic foundation for setting up a question-answering model using BERT and deploying it via FastAPI, it requires more extensive training with diverse and complex datasets, ensure the model can generalize well across different types of queries, and continuously monitor and update the model to adapt to new data and user feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
