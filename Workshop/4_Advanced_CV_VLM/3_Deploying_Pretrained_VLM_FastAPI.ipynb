{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying pretrained VLM to FastAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating the pretrained VLM Model (Owl-Vit) into FastAPI\n",
    "\n",
    "To aid in deployment, we would define the following functions and class\n",
    "\n",
    "**FastAPI Application**\n",
    "- test is a simple GET endpoint that returns a greeting.\n",
    "- predict is a POST endpoint that takes a VLMInput object as input, performs object detection, and returns the bounding box coordinates of the detected objects.\n",
    "\n",
    "1. **.Test Endpoint**\n",
    "    - This endpoint is a simple test endpoint that returns a hello message with the item ID.\n",
    "    - Request Method: GET\n",
    "    - Request Body: None\n",
    "    - Response: {\"Hello\": \"World_{item_id}\"}\n",
    "    \n",
    "2. **.od_predict - Object Detection Endpoint**\n",
    "    - This endpoint takes an image URL or path and runs object detection on it using the finetuned model.\n",
    "    - Request Method: POST\n",
    "    - Request Body: VLMInput object with path_or_url and optional labels and threshold fields\n",
    "    - Response: Object detection predictions in the format {\"label\": [(x1, y1, x2, y2), ...]}\n",
    "    \n",
    "3. **.clip_predict - CLIP Endpoint**\n",
    "    - This endpoint takes an image URL or path and runs CLIP (Contrastive Language-Image Pre-training) on it with the given labels.\n",
    "    - Request Method: POST\n",
    "    - Request Body: VLMInput object with path_or_url and labels fields\n",
    "    - Response: CLIP predictions in the format {\"label\": probability}\n",
    "    \n",
    "4. **owl_predict - OWL (Zero-Shot Object Detection) Endpoint**\n",
    "    - This endpoint takes an image URL or path and runs OWL on it with the given labels and threshold.\n",
    "    - Request Method: POST\n",
    "    - Request Body: VLMInput object with path_or_url, labels, and threshold fields\n",
    "    - Response: OWL predictions in the format {\"label\": [(x1, y1, x2, y2), ...]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we required the fine-tuned model for object detection from earlier units, please `run all` on the `1_Object_Detection_Fine_Tuning` notebook if you haven't run the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import base64\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import pipeline\n",
    "import urllib.request \n",
    "from torchvision.transforms import v2 as T\n",
    "import torch\n",
    "from urllib.request import urlretrieve\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from os import remove\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def load_image(path_or_url):\n",
    "    \"\"\"Loads an image from a given URL or path. If the input is a URL, \n",
    "    it downloads the image and saves it as a temporary file. If the input is a path,\n",
    "    it loads the image from the path. The image is then converted to RGB format and returned.\"\"\"\n",
    "    if path_or_url.startswith('http'):  # assume URL if starts with http\n",
    "        urlretrieve(path_or_url, \"tmp.png\")\n",
    "        img = Image.open(\"tmp.png\").convert(\"RGB\")\n",
    "        remove(\"tmp.png\")  # cleanup temporary file\n",
    "    else:\n",
    "        img = Image.open(path_or_url).convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "def ft_object_detection_predict(image):\n",
    "    \"\"\"Runs object detection on a given image using the fined tuned model that is safed locally. \n",
    "    The image is preprocessed, and the model is run on the device (either CPU or GPU). \n",
    "    The detections are then returned.\"\"\"\n",
    "    # helper function\n",
    "    def get_transform(train):\n",
    "        transforms = []\n",
    "        if train:\n",
    "            transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "        transforms.append(T.ToTensor())\n",
    "        return T.Compose(transforms)\n",
    "    \n",
    "    model = torch.load('fine_tuned_OD_pedestrain_model.pth')\n",
    "    eval_transform = get_transform(train=False)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = eval_transform(image)\n",
    "        x = x[:3, ...].to(device)\n",
    "        predictions = model([x, ])\n",
    "        pred = predictions[0]\n",
    "    return pred\n",
    "\n",
    "def clip_predict(image, labels):\n",
    "    \"\"\"Runs CLIP (Contrastive Language-Image Pre-training) on a given image with the given labels. \n",
    "    The image and labels are preprocessed, and the model is run on the device (either CPU or GPU).\n",
    "    The predictions are then returned.\"\"\"\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    model.to(device)\n",
    "    inputs = processor(text=labels.split(\",\"), images=image, return_tensors=\"pt\", padding=True)\n",
    "    inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "    probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n",
    "    return  {x:y.item() for x,y in zip(labels.split(\",\"), probs[0])}\n",
    "\n",
    "def owl_predict(img, labels, threshold):\n",
    "    \"\"\"Runs OWL (Zero-Shot Object Detection) on a given image with the given labels and threshold. \n",
    "    The image and labels are preprocessed, and the model is run on the device (either CPU or GPU). \n",
    "    The predictions are then filtered based on the threshold and returned.\"\"\"\n",
    "    checkpoint = \"google/owlv2-base-patch16-ensemble\"\n",
    "    detector = pipeline(model=checkpoint, task=\"zero-shot-object-detection\", device=0)\n",
    "    predictions = detector(img,candidate_labels=labels.split(\",\"))    \n",
    "    predict_dict = {}\n",
    "    for prediction in predictions:\n",
    "        if prediction[\"score\"]>threshold:\n",
    "            label = prediction[\"label\"]\n",
    "            predict_dict[label] = [(prediction[\"box\"]['xmin'], prediction[\"box\"]['xmax'], prediction[\"box\"]['ymin'], prediction[\"box\"]['ymax'])]\n",
    "    return predict_dict\n",
    "\n",
    "class VLMInput(BaseModel):\n",
    "    path_or_url: str\n",
    "    labels: str = \"None\"\n",
    "    threshold: float = 0.01\n",
    "    \n",
    "@app.get(\"/{item_id}\")\n",
    "def test():\n",
    "    return {\"Hello\": f\"World_{item_id}\"}\n",
    "\n",
    "@app.post(\"/od_predict\")\n",
    "async def predict(data: VLMInput):\n",
    "    img = load_image(data.path_or_url)\n",
    "    predict_dict = ft_object_detection_predict(img)\n",
    "    normal_dict = {\n",
    "        'boxes': predict_dict['boxes'].tolist(),\n",
    "        'labels': predict_dict['labels'].tolist(),\n",
    "        'scores': predict_dict['scores'].tolist()\n",
    "    }\n",
    "    return normal_dict\n",
    "\n",
    "@app.post(\"/clip_predict\")\n",
    "async def predict(data: VLMInput):\n",
    "    img = load_image(data.path_or_url)   \n",
    "    predict_dict = clip_predict(img, data.labels)  \n",
    "    return predict_dict\n",
    "\n",
    "@app.post(\"/owl_predict\")\n",
    "async def predict(data: VLMInput):\n",
    "    img = load_image(data.path_or_url)\n",
    "    predict_dict = owl_predict(img, data.labels, data.threshold)\n",
    "    return predict_dict\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dockerfile\n",
    "Create a `Dockerfile` in the same directory as your FastAPI app (`app.py`). This file will define the Docker image that includes your app and all its dependencies.\n",
    "\n",
    "```docker\n",
    "FROM us-docker.pkg.dev/deeplearning-platform-release/gcr.io/pytorch-gpu.2-2.py310\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /usr/src/app\n",
    "\n",
    "COPY . /usr/src/app\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Make port 8000 available to the world outside this container\n",
    "EXPOSE 8000\n",
    "\n",
    "# Define environment variable\n",
    "ENV MODEL_PATH=/usr/src/app/models\n",
    "\n",
    "# Run app.py when the container launches\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Requirements File\n",
    "Create a `requirements.txt` file that lists the packages that your app depends on. Make sure to include fastapi, uvicorn, torch, transformers, and any other required libraries. Torch isn't included in this `requirements.txt` because it's included in the starting Docker image (i.e. the image indicated in the first `FROM` line in the `Dockerfile`).\n",
    "\n",
    "```txt\n",
    "fastapi\n",
    "uvicorn[standard]\n",
    "pydantic\n",
    "timm\n",
    "transformers==4.37.0\n",
    "accelerate\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Docker Image\n",
    "From your project directory (where your `Dockerfile` and `app.py` are located), run the following command to build the Docker image\n",
    "```bash\n",
    "docker build -t vlm_app .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Docker Container\n",
    "```bash\n",
    "docker run -p 8000:8000 --gpus all vlm_app\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker runs the container and map port 8000 of the container to port 8000 on your host, allowing us to access the FastAPI application using the browser, `requests` library or Postman. We also give the container access to all the GPUs on our system such that it can run the models on GPU using CUDA, rather than on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `vlm_app` using `requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: {'boxes': [[265.0237121582031, 4.923355579376221, 388.3898010253906, 371.2566833496094], [112.45767211914062, 23.6782169342041, 201.7181854248047, 316.217041015625], [23.977638244628906, 20.964824676513672, 119.54765319824219, 319.9417419433594], [0.0, 75.43830871582031, 35.9254150390625, 310.942138671875], [43.246219635009766, 16.398618698120117, 184.44650268554688, 325.70440673828125]], 'labels': [1, 1, 1, 1, 1], 'scores': [0.9903967380523682, 0.9883424639701843, 0.9856445789337158, 0.46158814430236816, 0.33229777216911316]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The endpoint URL\n",
    "url = \"http://localhost:8000/od_predict\"\n",
    "\n",
    "# Example url and context\n",
    "data = {\n",
    "    \"path_or_url\": \"imgs/PennPed00048.png\",\n",
    "}\n",
    "\n",
    "# Sending a POST request\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Print the response from the server\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: {'horserider': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The endpoint URL\n",
    "url = \"http://localhost:8000/clip_predict\"\n",
    "\n",
    "# Example url and context\n",
    "data = {\n",
    "    \"path_or_url\": \"imgs/horserider.jpg\",\n",
    "    \"labels\": \"horserider\",\n",
    "}\n",
    "\n",
    "# Sending a POST request\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Print the response from the server\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: {'helmet': [[57, 176, 336, 493]], ' nasa badge': [[158, 250, 123, 168]]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The endpoint URL\n",
    "url = \"http://localhost:8000/owl_predict\"\n",
    "\n",
    "# Example url and context\n",
    "data = {\n",
    "    \"path_or_url\": \"https://th.bing.com/th/id/OIP.WhJW62tRiVMktCDMwRb52gHaJQ?rs=1&pid=ImgDetMain\",\n",
    "    \"labels\": \"helmet, nasa badge\",\n",
    "}\n",
    "\n",
    "# Sending a POST request\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Print the response from the server\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result shows that the model is successfully able to respond to the request. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise (20 mins)\n",
    "\n",
    "**1. Object Detection: Refining Object Detection Results: Finding Bounding Box Centers**\n",
    "\n",
    "Adjust the functions value to provide the center of the object's bounding box, you will need to modifying the app.py, rebuild the docker file and validate it with request.\n",
    "\n",
    "**2. OWL-Vit: Refining Object Detection Results: Applying Confidence Thresholds (Bonus)** \n",
    "\n",
    "In object detection tasks, it's essential to refine the results to ensure accuracy and relevance. \n",
    "Let's explore how to modify the request to return only confident predictions"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python310cv",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "python310cv (Local)",
   "language": "python",
   "name": "python310cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
