{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced NLP Models for Question-Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you'll explore some of the more advanced Transformer models for Question-Answering - BERT, T5, ALBERT, ELECTRA, and Longformer. This workshop provides an in-depth look into their architecture and applications. The subdomain of Question Answering in NLP is popular because of the wide range of applications in answering questions with a reference document. The solution to this problem is tackled by using a dataset that consists of an input text, a query, and the segment of text or span from the input text which contains the answer to the query. With the help of deep learning models, there has been a significant improvement in achieving human-level predictions from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer Architecture\n",
    "The transformer architecture was proposed in the paper Attention is All You Need. The encoders encode the input text, and the decoder processes the encodings to understand the contextual information behind the sequence. Each encoder and decoder in the stack uses an attention mechanism to process each input along with every other input for weighing their relevance, and generates the output sequence with the help of the decoder. The attention mechanism enables dynamic highlighting and understanding the features in the input text.\n",
    "\n",
    "<img src=\"https://lh3.googleusercontent.com/d/1kMk1ZgOi5FA2GpTODXCWjdXBfmfSr9A_\" alt=\"drawing\" height=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "The huggingface `transformers` library provides nicely wrapped implementations for us to experiment with. \n",
    "\n",
    "\n",
    "\n",
    "Let's begin our experiments by first diving into a BERT model for understanding its performance. BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained NLP model developed by Google that uses bidirectional attention to understand the context of words in a sentence. It can be fine-tuned for various tasks, including text classification, question answering, and language generation.\n",
    "\n",
    "<img src=\"https://lh3.googleusercontent.com/d/1ye4IvOzGM4b2ix1e2ByMntCUv1rM9G7n\" alt=\"drawing\" width=\"650\"><br><br>\n",
    "<img src=\"https://lh3.googleusercontent.com/d/15D_t6bQ3ziz0b2pUIbGHxGTKlJSsbWe0\" alt=\"drawing\" width=\"450\">\n",
    "<img src=\"https://lh3.googleusercontent.com/d/1e5UhuKT5v1Jn0nsqNmEeIA9SPQMGOgeH\" alt=\"drawing\" width=\"350\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,AlbertTokenizer,AutoTokenizer, AutoModelForQuestionAnswering ,BertForQuestionAnswering, AlbertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "model_name='bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'who', 'discovered', 'pen', '##ici', '##llin', '?', '[SEP]', 'pen', '##ici', '##llin', 'was', 'discovered', 'in', '1928', 'by', 'scottish', 'scientist', 'sir', 'alexander', 'fleming', '.', 'his', 'discovery', 'led', 'to', 'the', 'development', 'of', 'antibiotics', ',', 'which', 'have', 'saved', 'countless', 'lives', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "question = \"Who discovered penicillin?\"\n",
    "answer_text = \"Penicillin was discovered in 1928 by Scottish scientist Sir Alexander Fleming. His discovery led to the development of antibiotics, which have saved countless lives.\"\n",
    "\n",
    "input_ids = tokenizer.encode(question, answer_text)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa(question,answer_text,model,tokenizer):\n",
    "  inputs = tokenizer.encode_plus(question, answer_text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "  input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "  text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "  print(text_tokens)\n",
    "  outputs = model(**inputs)\n",
    "  answer_start_scores=outputs.start_logits\n",
    "  answer_end_scores=outputs.end_logits\n",
    "\n",
    "  answer_start = torch.argmax(\n",
    "      answer_start_scores\n",
    "  )  # Get the most likely beginning of answer with the argmax of the score\n",
    "  answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
    "  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "  \n",
    "  # Combine the tokens in the answer and print it out.\"\"\n",
    "  answer = answer.replace(\"#\",\"\")\n",
    "\n",
    "  print('Answer: \"' + answer + '\"')\n",
    "  return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'who', 'discovered', 'pen', '##ici', '##llin', '?', '[SEP]', 'pen', '##ici', '##llin', 'was', 'discovered', 'in', '1928', 'by', 'scottish', 'scientist', 'sir', 'alexander', 'fleming', '.', 'his', 'discovery', 'led', 'to', 'the', 'development', 'of', 'antibiotics', ',', 'which', 'have', 'saved', 'countless', 'lives', '.', '[SEP]']\n",
      "Answer: \"sir alexander fleming\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sir alexander fleming'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa(question,answer_text,model,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T5\n",
    "\n",
    "T5, developed by Google Research, frames every NLP task as a text-to-text problem, providing a unified framework for tasks like translation, summarization, and question answering. It excels at these tasks by leveraging large-scale pretraining on diverse text data with a denoising objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: <pad> Sir Alexander Fleming</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_name = 't5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def t5_qa(question, context, model, tokenizer):\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    inputs = tokenizer.encode_plus(input_text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    \n",
    "    outputs = model.generate(inputs['input_ids'])\n",
    "    answer = tokenizer.decode(outputs[0])\n",
    "    \n",
    "    print('Answer:', answer)\n",
    "\n",
    "t5_qa(question, answer_text, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALBERT (A Lite BERT)\n",
    "ALBERT, developed by Google Research, is a smaller, faster BERT variant with significantly reduced memory requirements. It achieves comparable performance to BERT by using parameter-sharing techniques and sentence-order prediction, making it ideal for scaling NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ahotrod/albert_xxlargev1_squad2_512 were not used when initializing AlbertForQuestionAnswering: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁who', '▁discovered', '▁pen', 'ici', 'll', 'in', '?', '[SEP]', '▁pen', 'ici', 'll', 'in', '▁was', '▁discovered', '▁in', '▁1928', '▁by', '▁scottish', '▁scientist', '▁sir', '▁alexander', '▁fleming', '.', '▁his', '▁discovery', '▁led', '▁to', '▁the', '▁development', '▁of', '▁antibiotic', 's', ',', '▁which', '▁have', '▁saved', '▁countless', '▁lives', '.', '[SEP]']\n",
      "Answer: \"sir alexander fleming\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sir alexander fleming'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=AlbertTokenizer.from_pretrained('ahotrod/albert_xxlargev1_squad2_512')\n",
    "model=AlbertForQuestionAnswering.from_pretrained('ahotrod/albert_xxlargev1_squad2_512')\n",
    "qa(question,answer_text,model,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ELECTRA (Efficiently Learning an Encoder that Classifies Token Replacements Accurately): \n",
    "ELECTRA, also by Google Research, replaces BERT's masked language modeling with a \"discriminator-generator\" approach. The generator corrupts input tokens, and the discriminator identifies altered tokens, training models efficiently to provide high-quality language representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'who', 'discovered', 'pen', '##ici', '##llin', '?', '[SEP]', 'pen', '##ici', '##llin', 'was', 'discovered', 'in', '1928', 'by', 'scottish', 'scientist', 'sir', 'alexander', 'fleming', '.', 'his', 'discovery', 'led', 'to', 'the', 'development', 'of', 'antibiotics', ',', 'which', 'have', 'saved', 'countless', 'lives', '.', '[SEP]']\n",
      "Answer: \"sir alexander fleming\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sir alexander fleming'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"valhalla/electra-base-discriminator-finetuned_squadv1\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"valhalla/electra-base-discriminator-finetuned_squadv1\")\n",
    "\n",
    "qa(question,answer_text,model,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Document Question-Answering Using Standard Models\n",
    " The transformers model uses the help of the self-attention operation to provide meaningful results. As the length of the sequence increases, the computation scales drastically for this mechanism. If we use the standard BERT model as we used earlier, an error will be observed, indicating that the long sequence of inputs cannot be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (538 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'how', 'many', 'pilots', 'did', 'the', 'ship', 'have', '?', '[SEP]', 'tug', 'boats', 'had', 'spent', 'several', 'hours', 'on', 'monday', 'working', 'to', 'free', 'the', 'bow', 'of', 'the', 'massive', 'vessel', 'after', 'di', '##sl', '##od', '##ging', 'the', 'stern', 'earlier', 'in', 'the', 'day', '.', 'marine', 'traffic', 'websites', 'showed', 'images', 'of', 'the', 'ship', 'away', 'from', 'the', 'banks', 'of', 'the', 'suez', 'canal', 'for', 'the', 'first', 'time', 'in', 'seven', 'days', 'following', 'an', 'around', '-', 'the', '-', 'clock', 'international', 'effort', 'to', 're', '##open', 'the', 'global', 'shipping', 'lane', '.', 'there', 'are', 'still', '422', 'ships', 'waiting', 'to', 'go', 'through', 'the', 'suez', 'canal', ',', 'ra', '##bie', 'said', ',', 'adding', 'that', 'the', 'canal', \"'\", 's', 'authorities', 'decided', 'the', 'ships', 'will', 'be', 'able', 'to', 'cross', 'the', 'canal', 'on', 'a', 'first', 'come', 'first', 'serve', 'basis', ',', 'though', 'the', 'ships', 'carrying', 'livestock', 'were', 'permitted', 'to', 'cross', 'in', 'the', 'first', 'convoy', 'of', 'the', 'day', '.', 'the', 'average', 'number', 'of', 'ships', 'that', 'transit', '##ed', 'through', 'the', 'canal', 'on', 'a', 'daily', 'basis', 'before', 'the', 'accident', 'was', 'between', '80', 'to', '90', 'ships', ',', 'according', 'to', 'lloyd', '##s', 'list', ';', 'however', ',', 'the', 'head', 'of', 'the', 'suez', 'canal', 'authority', 'said', 'that', 'the', 'channel', 'will', 'work', 'over', '24', 'hours', 'a', 'day', 'to', 'facilitate', 'the', 'passage', 'of', 'almost', '400', 'ships', 'carrying', 'billions', 'of', 'dollars', 'in', 'freight', '.', 'the', 'journey', 'to', 'cross', 'the', 'canal', 'takes', '10', 'to', '12', 'hours', 'and', 'in', 'the', 'event', 'the', 'channel', 'operates', 'for', '24', 'hours', ',', 'two', 'convoys', 'per', 'day', 'will', 'be', 'able', 'to', 'successfully', 'pass', 'through', '.', 'still', ',', 'shipping', 'giant', 'mae', '##rsk', 'issued', 'an', 'advisory', 'telling', 'customers', 'it', 'could', 'take', '\"', '6', 'days', 'or', 'more', '\"', 'for', 'the', 'line', 'to', 'clear', '.', 'the', 'company', 'said', 'that', 'was', 'an', 'estimate', 'and', 'subject', 'to', 'change', 'as', 'more', 'vessels', 'reach', 'the', 'block', '##age', 'or', 'are', 'diverted', '.', 'the', 'rescue', 'operation', 'had', 'intensified', 'in', 'both', 'urgency', 'and', 'global', 'attention', 'with', 'each', 'day', 'that', 'passed', ',', 'as', 'ships', 'from', 'around', 'the', 'world', ',', 'carrying', 'vital', 'fuel', 'and', 'cargo', ',', 'were', 'blocked', 'from', 'entering', 'the', 'canal', 'during', 'the', 'crisis', ',', 'raising', 'alarm', 'over', 'the', 'impact', 'on', 'global', 'supply', 'chains', '.', 'what', 'its', 'really', 'like', 'steering', 'the', 'worlds', 'biggest', 'ships', 'what', 'it', \"'\", 's', 'really', 'like', 'steering', 'the', 'world', \"'\", 's', 'biggest', 'ships', 'promising', 'signs', 'first', 'emerged', 'earlier', 'on', 'monday', 'when', 'the', 'rear', 'of', 'the', 'vessel', 'was', 'freed', 'from', 'one', 'of', 'the', 'canal', \"'\", 's', 'banks', '.', 'people', 'at', 'the', 'canal', 'cheered', 'as', 'news', 'of', 'monday', \"'\", 's', 'progress', 'came', 'in', '.', 'the', 'panama', 'maritime', 'authority', 'said', 'that', 'according', 'to', 'preliminary', 'reports', 'ever', 'given', 'suffered', 'mechanical', 'problems', 'that', 'affected', 'its', 'maneuver', '##ability', '.', 'the', 'ship', 'had', 'two', 'pilots', 'on', 'board', 'during', 'the', 'transit', '.', 'however', ',', 'the', 'owner', 'of', 'the', 'vessel', ',', 'japanese', 'shipping', 'company', 'shoe', 'ki', '##sen', 'insists', 'that', 'there', 'had', 'been', 'no', 'blackout', 'resulting', 'in', 'loss', 'of', 'power', 'prior', 'to', 'the', 'ship', '’', 's', 'ground', '##ing', '.', 'instead', ',', 'gust', '##ing', 'winds', 'of', '30', 'knots', 'and', 'poor', 'visibility', 'due', 'to', 'a', 'dust', 'storm', 'have', 'been', 'identified', 'as', 'the', 'likely', 'causes', 'of', 'the', 'ground', '##ing', ',', 'which', 'left', 'the', 'box', '##ship', 'stuck', 'sideways', 'in', 'a', 'narrow', 'point', 'of', 'the', 'waterway', '.', 'the', 'incident', 'has', 'not', 'resulted', 'in', 'any', 'marine', 'pollution', 'ot', 'injuries', 'to', 'the', 'crew', ',', 'only', 'some', 'structural', 'damage', 'to', 'the', 'ship', ',', 'which', 'is', 'yet', 'to', 'be', 'determined', '.', '[SEP]']\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (538) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForQuestionAnswering\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mqa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43manswer_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36mqa\u001b[0;34m(question, answer_text, model, tokenizer)\u001b[0m\n\u001b[1;32m      5\u001b[0m text_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(input_ids)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(text_tokens)\n\u001b[0;32m----> 7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m answer_start_scores\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mstart_logits\n\u001b[1;32m      9\u001b[0m answer_end_scores\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mend_logits\n",
      "File \u001b[0;32m~/Desktop/Files/Tribe/venv_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Files/Tribe/venv_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Files/Tribe/venv_ml/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1821\u001b[0m, in \u001b[0;36mBertForQuestionAnswering.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1809\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1810\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1817\u001b[0m \u001b[38;5;124;03m    are not taken into account for computing the loss.\u001b[39;00m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1819\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1821\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1826\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1827\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1828\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1829\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1831\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1833\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1835\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqa_outputs(sequence_output)\n",
      "File \u001b[0;32m~/Desktop/Files/Tribe/venv_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Files/Tribe/venv_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Files/Tribe/venv_ml/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:981\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    979\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m--> 981\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    989\u001b[0m     embedding_output,\n\u001b[1;32m    990\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    998\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    999\u001b[0m )\n\u001b[1;32m   1000\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Files/Tribe/venv_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Files/Tribe/venv_ml/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Files/Tribe/venv_ml/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:213\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabsolute\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    212\u001b[0m     position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)\n\u001b[0;32m--> 213\u001b[0m     \u001b[43membeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\n\u001b[1;32m    214\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)\n\u001b[1;32m    215\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (538) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "question=\"how many pilots did the ship have?\"\n",
    "\n",
    "answer_text=\"\"\"\n",
    "Tug boats had spent several hours on Monday working to free the bow of the massive vessel after dislodging the stern earlier in the day.\n",
    "Marine traffic websites showed images of the ship away from the banks of the Suez Canal for the first time in seven days following an around-the-clock international effort to reopen the global shipping lane.\n",
    "There are still 422 ships waiting to go through the Suez Canal, Rabie said, adding that the canal's authorities decided the ships will be able to cross the canal on a first come first serve basis, though the ships carrying livestock were permitted to cross in the first convoy of the day.\n",
    "The average number of ships that transited through the canal on a daily basis before the accident was between 80 to 90 ships, according to Lloyds List; however, the head of the Suez Canal Authority said that the channel will work over 24 hours a day to facilitate the passage of almost 400 ships carrying billions of dollars in freight.\n",
    "The journey to cross the canal takes 10 to 12 hours and in the event the channel operates for 24 hours, two convoys per day will be able to successfully pass through.\n",
    "Still, shipping giant Maersk issued an advisory telling customers it could take \"6 days or more\" for the line to clear. The company said that was an estimate and subject to change as more vessels reach the blockage or are diverted.\n",
    "The rescue operation had intensified in both urgency and global attention with each day that passed, as ships from around the world, carrying vital fuel and cargo, were blocked from entering the canal during the crisis, raising alarm over the impact on global supply chains.\n",
    "What its really like steering the worlds biggest ships\n",
    "What it's really like steering the world's biggest ships\n",
    "Promising signs first emerged earlier on Monday when the rear of the vessel was freed from one of the canal's banks.\n",
    "People at the canal cheered as news of Monday's progress came in.\n",
    "The Panama Maritime Authority said that according to preliminary reports Ever Given suffered mechanical problems that affected its maneuverability.\n",
    "\n",
    "The ship had two pilots on board during the transit.\n",
    "\n",
    "However, the owner of the vessel, Japanese shipping company Shoe Kisen insists that there had been no blackout resulting in loss of power prior to the ship’s grounding.\n",
    "Instead, gusting winds of 30 knots and poor visibility due to a dust storm have been identified as the likely causes of the grounding, which left the boxship stuck sideways in a narrow point of the waterway.\n",
    "\n",
    "The incident has not resulted in any marine pollution ot injuries to the crew, only some structural damage to the ship, which is yet to be determined.\n",
    "\"\"\"\n",
    "\n",
    "from transformers import BertTokenizer,BertForQuestionAnswering\n",
    "model_name='bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "qa(question,answer_text,model,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need another model which has been pre-trained to include a longer sequence of input documents, and also an architecture that can process the same.\n",
    "\n",
    "#### Longformer\n",
    "Developed by the Allen Institute for AI, Longformer is a Transformer model designed to handle long documents. It uses a combination of local and global attention mechanisms to reduce memory requirements, allowing it to capture relationships in sequences of up to 4,096 tokens while maintaining efficient computational performance. Longformer is particularly suited for tasks involving lengthy documents like text classification, summarization, and question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at valhalla/longformer-base-4096-finetuned-squadv1 were not used when initializing LongformerForQuestionAnswering: ['longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']\n",
      "- This IS expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Input ids are automatically padded from 562 to 1024 to be a multiple of `config.attention_window`: 512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'how', 'Ġmany', 'Ġpilots', 'Ġdid', 'Ġthe', 'Ġship', 'Ġhave', '?', '</s>', '</s>', 'Ċ', 'T', 'ug', 'Ġboats', 'Ġhad', 'Ġspent', 'Ġseveral', 'Ġhours', 'Ġon', 'ĠMonday', 'Ġworking', 'Ġto', 'Ġfree', 'Ġthe', 'Ġbow', 'Ġof', 'Ġthe', 'Ġmassive', 'Ġvessel', 'Ġafter', 'Ġdisl', 'od', 'ging', 'Ġthe', 'Ġstern', 'Ġearlier', 'Ġin', 'Ġthe', 'Ġday', '.', 'Ċ', 'Mar', 'ine', 'Ġtraffic', 'Ġwebsites', 'Ġshowed', 'Ġimages', 'Ġof', 'Ġthe', 'Ġship', 'Ġaway', 'Ġfrom', 'Ġthe', 'Ġbanks', 'Ġof', 'Ġthe', 'ĠS', 'uez', 'ĠCanal', 'Ġfor', 'Ġthe', 'Ġfirst', 'Ġtime', 'Ġin', 'Ġseven', 'Ġdays', 'Ġfollowing', 'Ġan', 'Ġaround', '-', 'the', '-', 'clock', 'Ġinternational', 'Ġeffort', 'Ġto', 'Ġreopen', 'Ġthe', 'Ġglobal', 'Ġshipping', 'Ġlane', '.', 'Ċ', 'There', 'Ġare', 'Ġstill', 'Ġ422', 'Ġships', 'Ġwaiting', 'Ġto', 'Ġgo', 'Ġthrough', 'Ġthe', 'ĠS', 'uez', 'ĠCanal', ',', 'ĠRab', 'ie', 'Ġsaid', ',', 'Ġadding', 'Ġthat', 'Ġthe', 'Ġcanal', \"'s\", 'Ġauthorities', 'Ġdecided', 'Ġthe', 'Ġships', 'Ġwill', 'Ġbe', 'Ġable', 'Ġto', 'Ġcross', 'Ġthe', 'Ġcanal', 'Ġon', 'Ġa', 'Ġfirst', 'Ġcome', 'Ġfirst', 'Ġserve', 'Ġbasis', ',', 'Ġthough', 'Ġthe', 'Ġships', 'Ġcarrying', 'Ġlivestock', 'Ġwere', 'Ġpermitted', 'Ġto', 'Ġcross', 'Ġin', 'Ġthe', 'Ġfirst', 'Ġconvoy', 'Ġof', 'Ġthe', 'Ġday', '.', 'Ċ', 'The', 'Ġaverage', 'Ġnumber', 'Ġof', 'Ġships', 'Ġthat', 'Ġtrans', 'ited', 'Ġthrough', 'Ġthe', 'Ġcanal', 'Ġon', 'Ġa', 'Ġdaily', 'Ġbasis', 'Ġbefore', 'Ġthe', 'Ġaccident', 'Ġwas', 'Ġbetween', 'Ġ80', 'Ġto', 'Ġ90', 'Ġships', ',', 'Ġaccording', 'Ġto', 'ĠLl', 'oy', 'ds', 'ĠList', ';', 'Ġhowever', ',', 'Ġthe', 'Ġhead', 'Ġof', 'Ġthe', 'ĠS', 'uez', 'ĠCanal', 'ĠAuthority', 'Ġsaid', 'Ġthat', 'Ġthe', 'Ġchannel', 'Ġwill', 'Ġwork', 'Ġover', 'Ġ24', 'Ġhours', 'Ġa', 'Ġday', 'Ġto', 'Ġfacilitate', 'Ġthe', 'Ġpassage', 'Ġof', 'Ġalmost', 'Ġ400', 'Ġships', 'Ġcarrying', 'Ġbillions', 'Ġof', 'Ġdollars', 'Ġin', 'Ġfreight', '.', 'Ċ', 'The', 'Ġjourney', 'Ġto', 'Ġcross', 'Ġthe', 'Ġcanal', 'Ġtakes', 'Ġ10', 'Ġto', 'Ġ12', 'Ġhours', 'Ġand', 'Ġin', 'Ġthe', 'Ġevent', 'Ġthe', 'Ġchannel', 'Ġoperates', 'Ġfor', 'Ġ24', 'Ġhours', ',', 'Ġtwo', 'Ġconv', 'oys', 'Ġper', 'Ġday', 'Ġwill', 'Ġbe', 'Ġable', 'Ġto', 'Ġsuccessfully', 'Ġpass', 'Ġthrough', '.', 'Ċ', 'Still', ',', 'Ġshipping', 'Ġgiant', 'ĠMa', 'ers', 'k', 'Ġissued', 'Ġan', 'Ġadvisory', 'Ġtelling', 'Ġcustomers', 'Ġit', 'Ġcould', 'Ġtake', 'Ġ\"', '6', 'Ġdays', 'Ġor', 'Ġmore', '\"', 'Ġfor', 'Ġthe', 'Ġline', 'Ġto', 'Ġclear', '.', 'ĠThe', 'Ġcompany', 'Ġsaid', 'Ġthat', 'Ġwas', 'Ġan', 'Ġestimate', 'Ġand', 'Ġsubject', 'Ġto', 'Ġchange', 'Ġas', 'Ġmore', 'Ġvessels', 'Ġreach', 'Ġthe', 'Ġblock', 'age', 'Ġor', 'Ġare', 'Ġdiverted', '.', 'Ċ', 'The', 'Ġrescue', 'Ġoperation', 'Ġhad', 'Ġintensified', 'Ġin', 'Ġboth', 'Ġurgency', 'Ġand', 'Ġglobal', 'Ġattention', 'Ġwith', 'Ġeach', 'Ġday', 'Ġthat', 'Ġpassed', ',', 'Ġas', 'Ġships', 'Ġfrom', 'Ġaround', 'Ġthe', 'Ġworld', ',', 'Ġcarrying', 'Ġvital', 'Ġfuel', 'Ġand', 'Ġcargo', ',', 'Ġwere', 'Ġblocked', 'Ġfrom', 'Ġentering', 'Ġthe', 'Ġcanal', 'Ġduring', 'Ġthe', 'Ġcrisis', ',', 'Ġraising', 'Ġalarm', 'Ġover', 'Ġthe', 'Ġimpact', 'Ġon', 'Ġglobal', 'Ġsupply', 'Ġchains', '.', 'Ċ', 'What', 'Ġits', 'Ġreally', 'Ġlike', 'Ġsteering', 'Ġthe', 'Ġworlds', 'Ġbiggest', 'Ġships', 'Ċ', 'What', 'Ġit', \"'s\", 'Ġreally', 'Ġlike', 'Ġsteering', 'Ġthe', 'Ġworld', \"'s\", 'Ġbiggest', 'Ġships', 'Ċ', 'Prom', 'ising', 'Ġsigns', 'Ġfirst', 'Ġemerged', 'Ġearlier', 'Ġon', 'ĠMonday', 'Ġwhen', 'Ġthe', 'Ġrear', 'Ġof', 'Ġthe', 'Ġvessel', 'Ġwas', 'Ġfreed', 'Ġfrom', 'Ġone', 'Ġof', 'Ġthe', 'Ġcanal', \"'s\", 'Ġbanks', '.', 'Ċ', 'People', 'Ġat', 'Ġthe', 'Ġcanal', 'Ġcheered', 'Ġas', 'Ġnews', 'Ġof', 'ĠMonday', \"'s\", 'Ġprogress', 'Ġcame', 'Ġin', '.', 'Ċ', 'The', 'ĠPanama', 'ĠMaritime', 'ĠAuthority', 'Ġsaid', 'Ġthat', 'Ġaccording', 'Ġto', 'Ġpreliminary', 'Ġreports', 'ĠEver', 'ĠGiven', 'Ġsuffered', 'Ġmechanical', 'Ġproblems', 'Ġthat', 'Ġaffected', 'Ġits', 'Ġmaneuver', 'ability', '.', 'Ċ', 'Ċ', 'The', 'Ġship', 'Ġhad', 'Ġtwo', 'Ġpilots', 'Ġon', 'Ġboard', 'Ġduring', 'Ġthe', 'Ġtransit', '.', 'Ċ', 'Ċ', 'However', ',', 'Ġthe', 'Ġowner', 'Ġof', 'Ġthe', 'Ġvessel', ',', 'ĠJapanese', 'Ġshipping', 'Ġcompany', 'ĠSh', 'oe', 'ĠK', 'isen', 'Ġinsists', 'Ġthat', 'Ġthere', 'Ġhad', 'Ġbeen', 'Ġno', 'Ġblackout', 'Ġresulting', 'Ġin', 'Ġloss', 'Ġof', 'Ġpower', 'Ġprior', 'Ġto', 'Ġthe', 'Ġship', 'âĢ', 'Ļ', 's', 'Ġgrounding', '.', 'Ċ', 'Instead', ',', 'Ġg', 'usting', 'Ġwinds', 'Ġof', 'Ġ30', 'Ġknots', 'Ġand', 'Ġpoor', 'Ġvisibility', 'Ġdue', 'Ġto', 'Ġa', 'Ġdust', 'Ġstorm', 'Ġhave', 'Ġbeen', 'Ġidentified', 'Ġas', 'Ġthe', 'Ġlikely', 'Ġcauses', 'Ġof', 'Ġthe', 'Ġgrounding', ',', 'Ġwhich', 'Ġleft', 'Ġthe', 'Ġbox', 'ship', 'Ġstuck', 'Ġsideways', 'Ġin', 'Ġa', 'Ġnarrow', 'Ġpoint', 'Ġof', 'Ġthe', 'Ġwater', 'way', '.', 'Ċ', 'Ċ', 'The', 'Ġincident', 'Ġhas', 'Ġnot', 'Ġresulted', 'Ġin', 'Ġany', 'Ġmarine', 'Ġpollution', 'Ġot', 'Ġinjuries', 'Ġto', 'Ġthe', 'Ġcrew', ',', 'Ġonly', 'Ġsome', 'Ġstructural', 'Ġdamage', 'Ġto', 'Ġthe', 'Ġship', ',', 'Ġwhich', 'Ġis', 'Ġyet', 'Ġto', 'Ġbe', 'Ġdetermined', '.', 'Ċ', '</s>']\n",
      "Answer: \" two\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' two'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n",
    "qa(question,answer_text,model,tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we have learnt to apply various transformer-based models for question answering. T5's text-to-text approach, ALBERT's efficient parameter-sharing, ELECTRA's novel discriminator-generator learning, and Longformer's scalability are all vital contributions to modern NLP. As you continue experimenting and applying what you've learned, you'll find innovative ways to leverage these models in your projects."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
