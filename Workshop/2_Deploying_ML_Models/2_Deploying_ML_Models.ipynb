{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Machine Learning Models with FastAPI\n",
    "\n",
    "This notebook will introduce you to the key aspects of deploying machine learning models using FastAPI. We'll explore setting up a FastAPI server, handling file uploads, and integrating a pre-trained machine learning model for image classification. You'll learn about the essential components such as route creation, asynchronous request handling and forming JSON responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation Guide\n",
    "\n",
    "```bash\n",
    "pip install fastapi uvicorn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to FastAPI\n",
    "\n",
    "What is FastAPI?\n",
    "FastAPI is a modern Python web framework designed explicitly for building high-performance RESTful APIs.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "def hello():\n",
    "    return {\"message\": \"Hello, this is your first API with FastAPI\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the above example from the command line using:\n",
    "\n",
    "```bash\n",
    "uvicorn fastapi_hello_world:app --host 0.0.0.0 --port 80\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Building Your First Machine Learning API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### FastAPI App Structure\n",
    "The basic structure of a FastAPI application is straightforward:\n",
    "- **App creation**: An instance of `FastAPI()`.\n",
    "- **Endpoint definition**: Using decorators to define functions for particular HTTP methods at specific API paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from fastapi.responses import JSONResponse\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(file: UploadFile = File(...)):\n",
    "    image_data = await file.read()\n",
    "    image = Image.open(io.BytesIO(image_data))\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        predicted_index = output.argmax(1).item()\n",
    "    \n",
    "    return JSONResponse(content={\"predicted_class\": predicted_index})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet illustrates how to define a more complex POST endpoint in FastAPI that handles image data for ML predictions.\n",
    "\n",
    "- `@app.post(\"/predict/\")`: Defines a route that listens for POST requests, where users can send images for classification.\n",
    "- `async def predict(file: UploadFile = File(...))`: The endpoint function is asynchronous, which improves performance by handling requests concurrently. It accepts an uploaded file through the request.\n",
    "- `await file.read()`: Asynchronously reads the uploaded file's data, efficient for handling I/O operations.\n",
    "- `JSONResponse`: Returns the predicted class in a JSON format, making the API suitable for integration with other services or applications that consume JSON.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the API Using Python Requests\n",
    "With the app running, let's use the requests library to send an image to the FastAPI application and receive a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predicted_class': 281}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the FastAPI endpoint\n",
    "url = 'http://localhost:8000/predict/'\n",
    "\n",
    "# Path to the image file\n",
    "file_path = 'imgs/cat.jpeg' \n",
    "\n",
    "# Open the image file in binary mode\n",
    "with open(file_path, 'rb') as f:\n",
    "    # Prepare the request payload as a dictionary\n",
    "    files = {'file': (file_path, f, 'image/jpeg')}\n",
    "    \n",
    "    # Send the POST request\n",
    "    response = requests.post(url, files=files)\n",
    "\n",
    "# Print the response\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `files = {'file': (file_path, f, 'image/jpeg')}`: Constructs a dictionary for the file payload. This dictionary is formatted to be compatible with HTTP multipart/form-data encoding, which is typically used for file uploads\n",
    "- `response = requests.post(url, files=files)`: Sends a POST request to the specified url with the file data. The files parameter is used to indicate that a file is included in the request.\n",
    "- `response.json()`: Parses the JSON response from the FastAPI server. This method converts the JSON returned by the server into a Python dictionary, allowing easy access to the prediction results or any other data returned by the server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dockerizing Your API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Containerization and Docker\n",
    "Containerization involves encapsulating an application and its environment into a container that can run on any Docker-enabled system. This ensures consistency across different development and production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/docker_deployment_1.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/docker_deployment.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dockerfile for FastAPI\n",
    "To dockerize your FastAPI application, you need a Dockerfile that specifies the environment, dependencies, and how to run your app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Docker\n",
    "# Use an official PyTorch runtime as a parent image\n",
    "FROM pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "COPY . /app\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install fastapi uvicorn Pillow torchvision python-multipart\n",
    "\n",
    "# Run app.py when the container launches\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Dockerfile begins with `FROM pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime`, specifying a base image with Pytorch \n",
    "- The `WORKDIR` command sets the working directory inside the container.\n",
    "- The `COPY . /app` command copies the application files from the current directory on the host into the `/app` directory in the container. \n",
    "- Following this, `RUN pip install` installs the Python dependencies\n",
    "- `EXPOSE 80` makes port 80 available outside the container, and the `CMD` command specifies the command to run the FastAPI app using Uvicorn, a lightning-fast ASGI server, on port 80. This setup ensures that the FastAPI application can be easily deployed and run in any environment that supports Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building and Running a Docker Image\n",
    "To build and run your FastAPI application inside a Docker container, use the following commands:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker build -t myfastapiapp .\n",
    "docker run -p 8000:80 myfastapiapp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This segment outlines the commands needed to build and run a Docker image containing a FastAPI application. \n",
    "- `docker build -t myfastapiapp .` command builds a Docker image from the Dockerfile in the current directory and tags it as `myfastapiapp`.\n",
    "- `docker run -p 8000:80 myfastapiapp` command runs the built Docker image as a container. The `-p 8000:80` option maps port 80 inside the container (where the FastAPI app is running) to port 8000 on the host machine. This mapping allows the FastAPI application to be accessed from the host machine's port 8000, facilitating easy interaction with the API during development or production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the API again using Python Requests\n",
    "With the app running, let's use the requests library to send an image to the FastAPI application and receive a prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predicted_class': 281}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the FastAPI endpoint\n",
    "url = 'http://localhost:8000/predict/'\n",
    "\n",
    "# Path to the image file\n",
    "file_path = 'imgs/cat.jpeg' \n",
    "\n",
    "# Open the image file in binary mode\n",
    "with open(file_path, 'rb') as f:\n",
    "    # Prepare the request payload as a dictionary\n",
    "    files = {'file': (file_path, f, 'image/jpeg')}\n",
    "    \n",
    "    # Send the POST request\n",
    "    response = requests.post(url, files=files)\n",
    "\n",
    "# Print the response\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dockerizing Your API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Containerization and Docker\n",
    "Containerization involves encapsulating an application and its environment into a container that can run on any Docker-enabled system. This ensures consistency across different development and production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this this workshop, we have explored the process of deploying machine learning models using FastAPI, from setting up a basic API to integrating and containerizing the application using Docker for consistent, scalable deployments.\n",
    "\n",
    "We covered a wide range of topics essential for anyone looking to deploy their machine learning models into production environments:\n",
    "\n",
    "- **FastAPI's Essentials**: We began with the basics of FastAPI, appreciating its speed, ease of use, and features like automatic data validation and documentation which make it an excellent choice for ML deployments.\n",
    "- **Machine Learning Integration**: We demonstrated how to integrate a pre-trained machine learning model into a FastAPI application, handling both GET and POST requests.\n",
    "- **Dockerization**: We containerized our FastAPI application using Docker, which not only simplifies deployment across different environments but also aids in achieving consistency and scalability.\n",
    "- **Production Deployment**: Finally, we discussed best practices for deploying our application to production, including considerations for scaling, security, and monitoring.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
