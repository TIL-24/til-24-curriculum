{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying pretrained VLM to FastAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating the pretrained VLM Model (CLIP + OD) into FastAPI\n",
    "\n",
    "To aid in deployment, we would define the following functions and class\n",
    "\n",
    "**FastAPI Application**\n",
    "- test is a simple GET endpoint that returns a greeting.\n",
    "- predict is a POST endpoint that takes a VLMInput object as input, performs object detection, and returns the bounding box coordinates of the detected objects.\n",
    "\n",
    "1. **.Test Endpoint**\n",
    "    - This endpoint is a simple test endpoint that returns a hello message with the item ID.\n",
    "    - Request Method: GET\n",
    "    - Request Body: None\n",
    "    - Response: {\"Hello\": \"World_{item_id}\"}\n",
    "    \n",
    "2. **.od_predict - Object Detection Endpoint**\n",
    "    - This endpoint takes an image URL or path and runs object detection on it using the SSD300 model.\n",
    "    - Request Method: POST\n",
    "    - Request Body: VLMInput object with path_or_url and optional labels and threshold fields\n",
    "    - Response: Object detection predictions in the format {\"label\": [(x1, y1, x2, y2), ...]}\n",
    "    \n",
    "3. **.clip_predict - CLIP Endpoint**\n",
    "    - This endpoint takes an image URL or path and runs CLIP (Contrastive Language-Image Pre-training) on it with the given labels.\n",
    "    - Request Method: POST\n",
    "    - Request Body: VLMInput object with path_or_url and labels fields\n",
    "    - Response: CLIP predictions in the format {\"label\": probability}\n",
    "    \n",
    "4. **clip_od_predict - CLIP + OD Endpoint**\n",
    "    - This endpoint takes an image URL or path and runs CLIP+ OD on it to extract the labels's object bounding box.\n",
    "    - Request Method: POST\n",
    "    - Request Body: VLMInput object with path_or_url, labels, and threshold fields\n",
    "    - Response: OD predictions in the format {\"label\": [(x1, y1, x2, y2), ...]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "This is just a copy of the `app.py`, which can be found @ `/vlm_app`, along with the `requirement.txt` and `Dockerfile`.\n",
    "\n",
    "```python\n",
    "import base64\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "import io\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import os\n",
    "from transformers import pipeline\n",
    "import urllib.request\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from urllib.request import urlretrieve\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from os import remove\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForObjectDetection,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel,\n",
    ")\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "detr_model = AutoModelForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", device_map=device)\n",
    "detr_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", device_map=device)\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\", device_map=device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", device_map=device)\n",
    "\n",
    "def load_image(path_or_url):\n",
    "    \"\"\"Loads an image from a given URL or path. If the input is a URL,\n",
    "    it downloads the image and saves it as a temporary file. If the input is a path,\n",
    "    it loads the image from the path. The image is then converted to RGB format and returned.\n",
    "    \"\"\"\n",
    "    if path_or_url.startswith(\"http\"):  # assume URL if starts with http\n",
    "        urlretrieve(path_or_url, \"tmp.png\")\n",
    "        img = Image.open(\"tmp.png\").convert(\"RGB\")\n",
    "        remove(\"tmp.png\")  # cleanup temporary file\n",
    "    else:\n",
    "        img = Image.open(path_or_url).convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "\n",
    "def object_detection_predict(image):\n",
    "    \"\"\"Runs object detection on a given image using the SSD300 model.\n",
    "    The image is preprocessed, and the model is run on the device (either CPU or GPU).\n",
    "    The detections are then returned.\"\"\"\n",
    "    weights = torchvision.models.detection.SSD300_VGG16_Weights.DEFAULT\n",
    "    ssd_model = torchvision.models.detection.ssd300_vgg16(\n",
    "        weights=True, box_score_thresh=0.9\n",
    "    )\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    processed_image = transform(image).unsqueeze(0)\n",
    "\n",
    "    # Label Encoding\n",
    "    id_2_label = {idx: x for idx, x in enumerate(weights.meta[\"categories\"])}\n",
    "\n",
    "    # Run inference\n",
    "    ssd_model.to(device).eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        detections = ssd_model(processed_image.to(device))[0]\n",
    "\n",
    "    boxes = detections[\"boxes\"].tolist()\n",
    "    labels = detections[\"labels\"].tolist()\n",
    "    scores = detections[\"scores\"].tolist()\n",
    "\n",
    "    detected_dict = {}\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score > 0.1:\n",
    "            class_id = label  # Get the class ID\n",
    "            detected_dict[id_2_label[label]] = box\n",
    "    return detected_dict\n",
    "\n",
    "\n",
    "def clip_predict(image, labels):\n",
    "    \"\"\"This function performs object detection and identification using the CLIP model + DETR\"\"\"\n",
    "    inputs = clip_processor(\n",
    "        text=labels.split(\",\"), images=image, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    inputs.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "    logits_per_image = (\n",
    "        outputs.logits_per_image\n",
    "    )  # this is the image-text similarity score\n",
    "    probs = logits_per_image.softmax(\n",
    "        dim=1\n",
    "    )  # we can take the softmax to get the label probabilities\n",
    "    return {x: y.item() for x, y in zip(labels.split(\",\"), probs[0])}\n",
    "\n",
    "\n",
    "def clip_od_predict(img, labels, threshold):\n",
    "    def detect_objects(image):\n",
    "        with torch.no_grad():\n",
    "            inputs = detr_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "            outputs = detr_model(**inputs)\n",
    "            target_sizes = torch.tensor([image.size[::-1]])\n",
    "            results = detr_processor.post_process_object_detection(\n",
    "                outputs, threshold=0.5, target_sizes=target_sizes\n",
    "            )[0]\n",
    "        return results[\"boxes\"]\n",
    "\n",
    "    def object_images(image, boxes):\n",
    "        image_arr = np.array(image)\n",
    "        all_images = []\n",
    "        for box in boxes:\n",
    "            # DETR returns top, left, bottom, right format\n",
    "            x1, y1, x2, y2 = [int(val) for val in box]\n",
    "            _image = image_arr[y1:y2, x1:x2]\n",
    "            all_images.append(_image)\n",
    "        return all_images\n",
    "\n",
    "\n",
    "    def identify_target(labels, images):\n",
    "        inputs = clip_processor(\n",
    "            text=labels.split(\",\"), images=images, return_tensors=\"pt\", padding=True\n",
    "        ).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = clip_model(**inputs)\n",
    "        logits_per_image = outputs.logits_per_image\n",
    "        most_similar_idx = torch.argmax(logits_per_image, dim=0).item()\n",
    "        return most_similar_idx\n",
    "\n",
    "    # detect object bounding boxes\n",
    "    detected_objects = detect_objects(img)\n",
    "\n",
    "    # get images of objects\n",
    "    images = object_images(img, detected_objects)\n",
    "\n",
    "    # identify target\n",
    "    idx = identify_target(labels, images)\n",
    "\n",
    "    # return bounding box of best match\n",
    "    return [int(val) for val in detected_objects[idx].tolist()]\n",
    "\n",
    "\n",
    "class VLMInput(BaseModel):\n",
    "    path_or_url: str\n",
    "    labels: str = \"None\"\n",
    "    threshold: float = 0.01\n",
    "\n",
    "\n",
    "@app.get(\"/{item_id}\")\n",
    "def test():\n",
    "    return {\"Hello\": f\"World_{item_id}\"}\n",
    "\n",
    "\n",
    "@app.post(\"/od_predict\")\n",
    "async def predict(data: VLMInput):\n",
    "    img = load_image(data.path_or_url)\n",
    "    predict_dict = object_detection_predict(img)\n",
    "    print(predict_dict)\n",
    "    return predict_dict\n",
    "\n",
    "\n",
    "@app.post(\"/clip_predict\")\n",
    "async def predict(data: VLMInput):\n",
    "    img = load_image(data.path_or_url)\n",
    "    predict_dict = clip_predict(img, data.labels)\n",
    "    return predict_dict\n",
    "\n",
    "\n",
    "@app.post(\"/clip_od_predict\")\n",
    "async def predict(data: VLMInput):\n",
    "    img = load_image(data.path_or_url)\n",
    "    predict_dict = clip_od_predict(img, data.labels, data.threshold)\n",
    "    return predict_dict\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dockerfile\n",
    "Create a `Dockerfile` in the same directory as your FastAPI app (`app.py`). This file will define the Docker image that includes your app and all its dependencies.\n",
    "\n",
    "```docker\n",
    "FROM us-docker.pkg.dev/deeplearning-platform-release/gcr.io/pytorch-gpu.2-2.py310\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /usr/src/app\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "COPY requirements.txt requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Make port 8000 available to the world outside this container\n",
    "EXPOSE 8000\n",
    "\n",
    "# Define environment variable\n",
    "# ENV MODEL_PATH=/usr/src/app/models\n",
    "\n",
    "# COPY \n",
    "COPY . /usr/src/app\n",
    "RUN ls\n",
    "\n",
    "# Run app.py when the container launches\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Requirements File\n",
    "Create a `requirements.txt` file that lists the packages that your app depends on. Make sure to include fastapi, uvicorn, torch, transformers, and any other required libraries. Torch isn't included in this `requirements.txt` because it's included in the starting Docker image (i.e. the image indicated in the first `FROM` line in the `Dockerfile`).\n",
    "\n",
    "```txt\n",
    "fastapi\n",
    "uvicorn[standard]\n",
    "pydantic\n",
    "timm\n",
    "transformers==4.37.0\n",
    "accelerate\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Docker Image\n",
    "From your project directory (where your `Dockerfile` and `app.py` are located), run the following command to build the Docker image\n",
    "```bash\n",
    "docker build -t vlm_app .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Docker Container\n",
    "```bash\n",
    "docker run -p 8000:8000 --gpus all vlm_app\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker runs the container and map port 8000 of the container to port 8000 on your host, allowing us to access the FastAPI application using the browser, `requests` library or Postman. We also give the container access to all the GPUs on our system such that it can run the models on GPU using CUDA, rather than on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `vlm_app` using `requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: {'dog': [42.76615524291992, 2520.119873046875, 3812.784423828125, 6082.93212890625]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The endpoint URL\n",
    "url = \"http://localhost:8000/od_predict\"\n",
    "\n",
    "# Example url and context\n",
    "data = {\n",
    "    \"path_or_url\": \"imgs/dog1.jpg\",\n",
    "}\n",
    "\n",
    "# Sending a POST request\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Print the response from the server\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: {'horserider': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The endpoint URL\n",
    "url = \"http://localhost:8000/clip_predict\"\n",
    "\n",
    "# Example url and context\n",
    "data = {\n",
    "    \"path_or_url\": \"imgs/horserider.jpg\",\n",
    "    \"labels\": \"horserider\",\n",
    "}\n",
    "\n",
    "# Sending a POST request\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Print the response from the server\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: [345, 23, 640, 368]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# The endpoint URL\n",
    "url = \"http://localhost:8000/clip_od_predict\"\n",
    "\n",
    "# Example url and context\n",
    "data = {\n",
    "    \"path_or_url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n",
    "    \"labels\": \"photo of a cat\",\n",
    "}\n",
    "\n",
    "# Sending a POST request\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Print the response from the server\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result shows that the model is successfully able to respond to the request. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise (20 mins)\n",
    "\n",
    "**1. Object Detection: Refining Object Detection Results: Finding Bounding Box Centers**\n",
    "\n",
    "Adjust the functions value to provide the center of the object's bounding box, you will need to modifying the app.py, rebuild the docker file and validate it with request.\n",
    "\n",
    "**2. Optimizing DockerFile: Setup (Bonus)**\n",
    "\n",
    "Loading models from the internet on every Docker run can be slow and may not be optimal for production environments. Saving the model files and copying them into the Docker image can significantly improve performance and reliability.\n",
    "\n",
    "Here's an updated approach to attempt:\n",
    "1. Download the CLIP and DETR models using Hugging Face's transformers library and save them to a local directory, e.g., models/.\n",
    "2. Update the Dockerfile to COPY the model files into the Docker image:\n",
    "\n",
    "**3. Object Detection: Testing with non-common object (Discussion) (Bonus)** \n",
    "\n",
    "Both DETR (object detection) and CLIP pretrained on mainly COCO datasets, how does could this be adapted to track non-common objects.\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python310cv",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "python310cv (Local)",
   "language": "python",
   "name": "python310cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
