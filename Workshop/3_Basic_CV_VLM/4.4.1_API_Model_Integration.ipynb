{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Model Integration\n",
    "Unlike a traditional object detection task, the task for the TIL-AI competition requires your model to be able to have good results on unknown categories or labels outside the training set. As such, conventional object detection models may not suffice. In this notebook we will give one possible way of solving the competition task by combining a traditional Object Detection model to identify candidate target bounding boxes along with CLIP to identify whether a particular target image bounding box contains the target identified in the caption. We integrate these two models together into a real-world object detection application using FastAPI. The setup involves creating an API that receives an image and caption as input and returns the predicted bounding box. And as before, we will create a Docker image for the FastAPI app to deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# needed for DETR\n",
    "# ! pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: einops in /opt/conda/envs/py310cv/lib/python3.10/site-packages (0.8.0)\n",
      "Requirement already satisfied: optimum in /opt/conda/envs/py310cv/lib/python3.10/site-packages (1.19.1)\n",
      "Requirement already satisfied: auto-gptq in /opt/conda/envs/py310cv/lib/python3.10/site-packages (0.7.1)\n",
      "Requirement already satisfied: bitsandbytes in /opt/conda/envs/py310cv/lib/python3.10/site-packages (0.43.1)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: transformers<4.41.0,>=4.26.0 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (4.40.1)\n",
      "Requirement already satisfied: torch>=1.11 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from optimum) (2.2.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from optimum) (24.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from optimum) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from optimum) (0.22.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from optimum) (2.16.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from auto-gptq) (0.29.3)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from auto-gptq) (0.2.0)\n",
      "Requirement already satisfied: rouge in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from auto-gptq) (1.0.1)\n",
      "Requirement already satisfied: gekko in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from auto-gptq) (1.1.1)\n",
      "Requirement already satisfied: safetensors in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from auto-gptq) (0.4.3)\n",
      "Requirement already satisfied: peft>=0.5.0 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from auto-gptq) (0.10.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from auto-gptq) (4.66.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from transformers<4.41.0,>=4.26.0->transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (2024.4.16)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from transformers<4.41.0,>=4.26.0->transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (0.19.1)\n",
      "Requirement already satisfied: protobuf in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (4.25.3)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from datasets->optimum) (16.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from datasets->optimum) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from datasets->optimum) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from datasets->optimum) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from datasets->optimum) (3.9.5)\n",
      "Requirement already satisfied: six in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/py310cv/lib/python3.10/site-packages (from pandas->datasets->optimum) (2024.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install einops optimum auto-gptq bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Saving the Model and Tokenizer\n",
    "After training your models, you can save them to a directory, commonly done using the `save_pretrained()` method provided by the Hugging Face Transformers library. Here we'll use pre-trained versions of DETR (DEtection TRansformer, discussed in more detail in Unit 5) and CLIP. DETR in this case plays the role of our object detection model, and is used to identify candidate bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14e67d68eb1454fa5e5afcb1d368c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513fdb64027646bab244b847534e7a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/70.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d236b0919e714cf8b8c6edc62c9c0cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219dc0121e234b609492222cb4c99637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e36728f0de4a07b3f150c4631f033f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.31G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4a3bbe23cb4d6f897342abe22f43a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image    \n",
    "import requests\n",
    "\n",
    "model_id = \"xtuner/llava-phi-3-mini-hf\"\n",
    "pipe = pipeline(\"image-to-text\", model=model_id, device=0)\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/ai2d-demo.jpg\"\n",
    "\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "prompt = \"<|user|>\\n<image>\\nWhat does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud<|end|>\\n<|assistant|>\\n\"\n",
    "\n",
    "outputs = pipe(image, prompt=prompt, generate_kwargs={\"max_new_tokens\": 200})\n",
    "print(outputs)\n",
    "# >>> [{'generated_text': '\\nWhat does the label 15 represent? (1) lava (2) core (3) tunnel (4) ash cloud (1) lava'}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForObjectDetection,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel,\n",
    ")\n",
    "\n",
    "\n",
    "# DETR\n",
    "detr_checkpoint = \"facebook/detr-resnet-50\"\n",
    "detr_model = AutoModelForObjectDetection.from_pretrained(detr_checkpoint)\n",
    "detr_processor = AutoImageProcessor.from_pretrained(detr_checkpoint)\n",
    "\n",
    "detr_model_path = \"detr_model.pth\"\n",
    "\n",
    "# CLIP\n",
    "clip_checkpoint = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(clip_checkpoint)\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_checkpoint)\n",
    "\n",
    "clip_model_path = \"clip_model.pth\"\n",
    "\n",
    "# Assume the rest of your model training setup is here\n",
    "# ....\n",
    "\n",
    "# After training:\n",
    "detr_model.save_pretrained(detr_model_path)\n",
    "detr_processor.save_pretrained(detr_model_path)\n",
    "\n",
    "clip_model.save_pretrained(clip_model_path)\n",
    "clip_processor.save_pretrained(clip_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating the Saved Model into FastAPI\n",
    "Now that your models and processors are saved, you can load them from the saved directory in your FastAPI application. This will allow your API to use the fine-tuned models to run our multi-stage object detection workflow. The below example code is stored in `app.py` in the `vlm_app` folder.\n",
    "\n",
    "Note how the `detect_objects` function runs the DETR object detection, returning not just one but multiple bounding boxes. The `object_images` function then extracts the sections of the original image associated with the detected bounding box, which are then passed to the `identify_target` function, which runs CLIP and identifies the image with the strongest association with the given caption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import base64\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForObjectDetection,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel,\n",
    ")\n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Fetch the model directory from the environment variable\n",
    "model_directory = os.getenv(\"MODEL_PATH\", \"/usr/src/app/models\")\n",
    "detr_model_filename = \"detr_model.pth\"  # Specify your model filename here\n",
    "clip_model_filename = \"clip_model.pth\"  # Specify your model filename here\n",
    "\n",
    "# Full path to your model files\n",
    "detr_model_path = os.path.join(model_directory, detr_model_filename)\n",
    "clip_model_path = os.path.join(model_directory, clip_model_filename)\n",
    "\n",
    "# Load the models\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "detr_model = AutoModelForObjectDetection.from_pretrained(\n",
    "    detr_model_path, device_map=device\n",
    ")\n",
    "detr_processor = AutoImageProcessor.from_pretrained(detr_model_path, device_map=device)\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(clip_model_path, device_map=device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_path, device_map=device)\n",
    "\n",
    "\n",
    "class VLMInput(BaseModel):\n",
    "    image: str\n",
    "    caption: str\n",
    "\n",
    "\n",
    "def detect_objects(image):\n",
    "    with torch.no_grad():\n",
    "        inputs = detr_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        outputs = detr_model(**inputs)\n",
    "        target_sizes = torch.tensor([image.size[::-1]])\n",
    "        results = detr_processor.post_process_object_detection(\n",
    "            outputs, threshold=0.5, target_sizes=target_sizes\n",
    "        )[0]\n",
    "    return results[\"boxes\"]\n",
    "\n",
    "\n",
    "def object_images(image, boxes):\n",
    "    image_arr = np.array(image)\n",
    "    all_images = []\n",
    "    for box in boxes:\n",
    "        # DETR returns top, left, bottom, right format\n",
    "        x1, y1, x2, y2 = [int(val) for val in box]\n",
    "        _image = image_arr[y1:y2, x1:x2]\n",
    "        all_images.append(_image)\n",
    "    return all_images\n",
    "\n",
    "\n",
    "def identify_target(query, images):\n",
    "    inputs = clip_processor(\n",
    "        text=[query], images=images, return_tensors=\"pt\", padding=True\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    most_similar_idx = torch.argmax(logits_per_image, dim=0).item()\n",
    "    return most_similar_idx\n",
    "\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(data: VLMInput):\n",
    "    image_bytes = base64.b64decode(data.image)\n",
    "    im = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "    # detect object bounding boxes\n",
    "    detected_objects = detect_objects(im)\n",
    "\n",
    "    # get images of objects\n",
    "    images = object_images(im, detected_objects)\n",
    "\n",
    "    # identify target\n",
    "    idx = identify_target(data.caption, images)\n",
    "\n",
    "    # return bounding box of best match\n",
    "    return [int(val) for val in detected_objects[idx].tolist()]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dockerfile\n",
    "Create a `Dockerfile` in the same directory as your FastAPI app (`app.py`). This file will define the Docker image that includes your app and all its dependencies.\n",
    "\n",
    "```docker\n",
    "FROM us-docker.pkg.dev/deeplearning-platform-release/gcr.io/pytorch-gpu.2-2.py310\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /usr/src/app\n",
    "\n",
    "COPY . /usr/src/app\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Make port 8000 available to the world outside this container\n",
    "EXPOSE 8000\n",
    "\n",
    "# Define environment variable\n",
    "ENV MODEL_PATH=/usr/src/app/models\n",
    "\n",
    "# Run app.py when the container launches\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Requirements File\n",
    "Create a `requirements.txt` file that lists the packages that your app depends on. Make sure to include fastapi, uvicorn, torch, transformers, and any other required libraries. Torch isn't included in this `requirements.txt` because it's included in the starting Docker image (i.e. the image indicated in the first `FROM` line in the `Dockerfile`).\n",
    "\n",
    "```txt\n",
    "fastapi\n",
    "uvicorn[standard]\n",
    "pydantic\n",
    "timm\n",
    "transformers==4.37.0\n",
    "accelerate\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Docker Image\n",
    "From your project directory (where your `Dockerfile` and `app.py` are located), run the following command to build the Docker image\n",
    "```bash\n",
    "docker build -t vlm_app .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Docker Container\n",
    "```bash\n",
    "docker run -p 8000:8000 --gpus all vlm_app\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker runs the container and map port 8000 of the container to port 8000 on your host, allowing us to access the FastAPI application using the browser, `requests` library or Postman. We also give the container access to all the GPUs on our system such that it can run the models on GPU using CUDA, rather than on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `vlm_app` using `requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: [345, 23, 640, 368]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from base64 import b64encode\n",
    "\n",
    "# The endpoint URL\n",
    "url = 'http://localhost:8000/predict'\n",
    "\n",
    "# base64 encode image so it can be passed in json\n",
    "image = b64encode(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\").content).decode(\"utf-8\")\n",
    "\n",
    "# Example question and context\n",
    "data = {\n",
    "    \"image\": image,\n",
    "    \"caption\": \"photo of a cat\",\n",
    "}\n",
    "\n",
    "# Sending a POST request\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Print the response from the server\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result shows that the model is successfully able to respond to the request. "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python310cv",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "python310cv",
   "language": "python",
   "name": "python310cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
