{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying pretrained VLM to FastAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating the pretrained VLM Model (Owl-Vit) into FastAPI\n",
    "\n",
    "To aid in deployment, we would define the following functions and class\n",
    "\n",
    "**Functions**\n",
    "- loading_image: Downloads an image from a URL and converts it to RGB format.\n",
    "- detect_objects: Uses the Hugging Face Transformers library to perform zero-shot object detection on an image.\n",
    "- parsing_results: Filters object detection results based on a confidence threshold and extracts bounding box coordinates.\n",
    "\n",
    "**FastAPI Application**\n",
    "- test is a simple GET endpoint that returns a greeting.\n",
    "- predict is a POST endpoint that takes a VLMInput object as input, performs object detection, and returns the bounding box coordinates of the detected objects.\n",
    "\n",
    "**VLMInput (predict POST payload)**\n",
    "\n",
    "VLMInput is a Pydantic model that defines the input data structure for the predict endpoint. It has three attributes:\n",
    "- url: The URL of the input image.\n",
    "- labels: The list of object labels to detect.\n",
    "- threshold: The confidence threshold for object detection (default: 0.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import base64\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "from transformers import pipeline\n",
    "import urllib.request \n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "def loading_image(url):\n",
    "    urllib.request.urlretrieve(\n",
    "        url,\n",
    "        \"tmpt.png\") \n",
    "    img = Image.open(\"tmpt.png\").convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "def detect_objects(img, labels):\n",
    "    checkpoint = \"google/owlv2-base-patch16-ensemble\"\n",
    "    detector = pipeline(model=checkpoint, task=\"zero-shot-object-detection\")\n",
    "    predictions = detector(\n",
    "        img,\n",
    "        candidate_labels=labels.split(\",\"),\n",
    "    )\n",
    "    print(labels.split(\",\"))\n",
    "    return predictions\n",
    "\n",
    "def parsing_results(predictions, label, threshold):\n",
    "    predict_dict = {}\n",
    "    for prediction in predictions:\n",
    "        if prediction[\"score\"]>threshold:\n",
    "            label = prediction[\"label\"]\n",
    "            predict_dict[label] = [(prediction[\"box\"]['xmin'], prediction[\"box\"]['xmax'], prediction[\"box\"]['ymin'], prediction[\"box\"]['ymax'])]\n",
    "    return predict_dict\n",
    "\n",
    "@app.get(\"/{item_id}\")\n",
    "def test():\n",
    "    return {\"Hello\": f\"World_{item_id}\"}\n",
    "\n",
    "class VLMInput(BaseModel):\n",
    "    url: str\n",
    "    labels: str\n",
    "    threshold: float = 0.01\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(data: VLMInput):\n",
    "    img = loading_image(data.url)\n",
    "    \n",
    "    # detect object bounding boxes\n",
    "    predictions = detect_objects(img, data.labels)\n",
    "\n",
    "    # get images of objects\n",
    "    predict_dict = parsing_results(predictions, data.labels, data.threshold)\n",
    "\n",
    "    # return bounding box of best match\n",
    "    return predict_dict\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dockerfile\n",
    "Create a `Dockerfile` in the same directory as your FastAPI app (`app.py`). This file will define the Docker image that includes your app and all its dependencies.\n",
    "\n",
    "```docker\n",
    "FROM us-docker.pkg.dev/deeplearning-platform-release/gcr.io/pytorch-gpu.2-2.py310\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /usr/src/app\n",
    "\n",
    "COPY . /usr/src/app\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Make port 8000 available to the world outside this container\n",
    "EXPOSE 8000\n",
    "\n",
    "# Define environment variable\n",
    "ENV MODEL_PATH=/usr/src/app/models\n",
    "\n",
    "# Run app.py when the container launches\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Requirements File\n",
    "Create a `requirements.txt` file that lists the packages that your app depends on. Make sure to include fastapi, uvicorn, torch, transformers, and any other required libraries. Torch isn't included in this `requirements.txt` because it's included in the starting Docker image (i.e. the image indicated in the first `FROM` line in the `Dockerfile`).\n",
    "\n",
    "```txt\n",
    "fastapi\n",
    "uvicorn[standard]\n",
    "pydantic\n",
    "timm\n",
    "transformers==4.37.0\n",
    "accelerate\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Docker Image\n",
    "From your project directory (where your `Dockerfile` and `app.py` are located), run the following command to build the Docker image\n",
    "```bash\n",
    "docker build -t vlm_app .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Docker Container\n",
    "```bash\n",
    "docker run -p 8000:8000 vlm_app\n",
    "\n",
    "#--gpus all\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker runs the container and map port 8000 of the container to port 8000 on your host, allowing us to access the FastAPI application using the browser, `requests` library or Postman. We also give the container access to all the GPUs on our system such that it can run the models on GPU using CUDA, rather than on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `vlm_app` using `requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: {'helmet': [[57, 176, 336, 493]], ' nasa badge': [[158, 250, 123, 168]]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from base64 import b64encode\n",
    "\n",
    "# The endpoint URL\n",
    "url = 'http://localhost:8000/predict'\n",
    "\n",
    "# Example url and context\n",
    "data = {\n",
    "    \"url\": \"https://th.bing.com/th/id/OIP.WhJW62tRiVMktCDMwRb52gHaJQ?rs=1&pid=ImgDetMain\",\n",
    "    \"labels\": \"helmet, nasa badge\",\n",
    "}\n",
    "\n",
    "# Sending a POST request\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Print the response from the server\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result shows that the model is successfully able to respond to the request. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise (20 mins)\n",
    "\n",
    "**1. Refining Object Detection Results: Applying Confidence Thresholds and Bounding Box Centers**\n",
    "\n",
    "In object detection tasks, it's essential to refine the results to ensure accuracy and relevance. \n",
    "\n",
    "Let's explore how to modify the request to return only confident predictions and adjust the functions value to provide the center of the object's bounding box."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python310cv",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "python310cv",
   "language": "python",
   "name": "python310cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
