{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Model Integration\n",
    "In this notebook we will take a look at integrating the fine-tuned VLM model for object detection into a real-world application using FastAPI. The setup involves creating an API that receives an image and caption as input and returns the predicted bounding box. As before, we will create a Docker image for the FastAPI app to deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /opt/conda/envs/pytorch/lib/python3.10/site-packages (0.9.16)\n",
      "Requirement already satisfied: torch in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from timm) (1.13.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from timm) (0.14.1)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from timm) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from timm) (0.22.2)\n",
      "Requirement already satisfied: safetensors in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from timm) (0.4.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface_hub->timm) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface_hub->timm) (24.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface_hub->timm) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface_hub->timm) (4.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->timm) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->timm) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->timm) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch->timm) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->timm) (69.2.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->timm) (0.42.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torchvision->timm) (10.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "# needed for DETR\n",
    "! pip install timm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Saving the Model and Tokenizer\n",
    "After training your models, you can save them to a directory, commonly done using the `save_pretrained()` method provided by the Hugging Face Transformers library. Here we'll use pre-trained versions of DETR (DEtection TRansformer, discussed in more detail in Unit 6) and CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/detr-resnet-50 were not used when initializing DetrForObjectDetection: ['model.backbone.conv_encoder.model.layer1.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing DetrForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DetrForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForObjectDetection,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel,\n",
    ")\n",
    "\n",
    "\n",
    "# DETR\n",
    "detr_checkpoint = \"facebook/detr-resnet-50\"\n",
    "detr_model = AutoModelForObjectDetection.from_pretrained(detr_checkpoint)\n",
    "detr_processor = AutoImageProcessor.from_pretrained(detr_checkpoint)\n",
    "\n",
    "detr_model_path = \"detr_model.pth\"\n",
    "\n",
    "# CLIP\n",
    "clip_checkpoint = \"openai/clip-vit-base-patch32\"\n",
    "clip_model = CLIPModel.from_pretrained(clip_checkpoint)\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_checkpoint)\n",
    "\n",
    "clip_model_path = \"clip_model.pth\"\n",
    "\n",
    "# Assume the rest of your model training setup is here\n",
    "# ....\n",
    "\n",
    "# After training:\n",
    "detr_model.save_pretrained(detr_model_path)\n",
    "detr_processor.save_pretrained(detr_model_path)\n",
    "\n",
    "clip_model.save_pretrained(clip_model_path)\n",
    "clip_processor.save_pretrained(clip_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating the Saved Model into FastAPI\n",
    "Now that your model and processor are saved, you can load them from the saved directory in your FastAPI application. This will allow your API to use the fine-tuned model to run object detection. The below example code is stored in `app.py` in the `vlm_app` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import base64\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForObjectDetection,\n",
    "    CLIPProcessor,\n",
    "    CLIPModel,\n",
    ")\n",
    "import numpy as np\n",
    "import io\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Fetch the model directory from the environment variable\n",
    "model_directory = os.getenv(\"MODEL_PATH\", \"/usr/src/app/models\")\n",
    "detr_model_filename = \"detr_model.pth\"  # Specify your model filename here\n",
    "clip_model_filename = \"clip_model.pth\"  # Specify your model filename here\n",
    "\n",
    "# Full path to your model files\n",
    "detr_model_path = os.path.join(model_directory, detr_model_filename)\n",
    "clip_model_path = os.path.join(model_directory, clip_model_filename)\n",
    "\n",
    "# Load the models\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "detr_model = AutoModelForObjectDetection.from_pretrained(\n",
    "    detr_model_path, device_map=device\n",
    ")\n",
    "detr_processor = AutoImageProcessor.from_pretrained(detr_model_path, device_map=device)\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(clip_model_path, device_map=device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(clip_model_path, device_map=device)\n",
    "\n",
    "\n",
    "class VLMInput(BaseModel):\n",
    "    image: str\n",
    "    caption: str\n",
    "\n",
    "\n",
    "def detect_objects(image):\n",
    "    with torch.no_grad():\n",
    "        inputs = detr_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "        outputs = detr_model(**inputs)\n",
    "        target_sizes = torch.tensor([image.size[::-1]])\n",
    "        results = detr_processor.post_process_object_detection(\n",
    "            outputs, threshold=0.5, target_sizes=target_sizes\n",
    "        )[0]\n",
    "    return results[\"boxes\"]\n",
    "\n",
    "\n",
    "def object_images(image, boxes):\n",
    "    image_arr = np.array(image)\n",
    "    all_images = []\n",
    "    for box in boxes:\n",
    "        # DETR returns top, left, bottom, right format\n",
    "        x1, y1, x2, y2 = [int(val) for val in box]\n",
    "        _image = image_arr[y1:y2, x1:x2]\n",
    "        all_images.append(_image)\n",
    "    return all_images\n",
    "\n",
    "\n",
    "def identify_target(query, images):\n",
    "    inputs = clip_processor(\n",
    "        text=[query], images=images, return_tensors=\"pt\", padding=True\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = clip_model(**inputs)\n",
    "    logits_per_image = outputs.logits_per_image\n",
    "    most_similar_idx = torch.argmax(logits_per_image, dim=0).item()\n",
    "    return most_similar_idx\n",
    "\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(data: VLMInput):\n",
    "    image_bytes = base64.b64decode(data.image)\n",
    "    im = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "    # detect object bounding boxes\n",
    "    detected_objects = detect_objects(im)\n",
    "\n",
    "    # get images of objects\n",
    "    images = object_images(im, detected_objects)\n",
    "\n",
    "    # identify target\n",
    "    idx = identify_target(data.caption, images)\n",
    "\n",
    "    # return bounding box of best match\n",
    "    return [int(val) for val in detected_objects[idx].tolist()]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dockerfile\n",
    "Create a `Dockerfile` in the same directory as your FastAPI app (`app.py`). This file will define the Docker image that includes your app and all its dependencies.\n",
    "\n",
    "```docker\n",
    "FROM us-docker.pkg.dev/deeplearning-platform-release/gcr.io/pytorch-gpu.2-2.py310\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /usr/src/app\n",
    "\n",
    "COPY . /usr/src/app\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Make port 8000 available to the world outside this container\n",
    "EXPOSE 8000\n",
    "\n",
    "# Define environment variable\n",
    "ENV MODEL_PATH=/usr/src/app/models\n",
    "\n",
    "# Run app.py when the container launches\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Requirements File\n",
    "Create a `requirements.txt` file that lists the packages that your app depends on. Make sure to include fastapi, uvicorn, torch, transformers, and any other required libraries. Torch isn't included in this `requirements.txt` because it's included in the starting Docker image (i.e. the image indicated in the first `FROM` line in the `Dockerfile`).\n",
    "\n",
    "```txt\n",
    "fastapi\n",
    "uvicorn[standard]\n",
    "pydantic\n",
    "timm\n",
    "transformers==4.37.0\n",
    "accelerate\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Docker Image\n",
    "From your project directory (where your `Dockerfile` and `app.py` are located), run the following command to build the Docker image\n",
    "```bash\n",
    "docker build -t vlm_app .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Docker Container\n",
    "```bash\n",
    "docker run -p 8000:8000 vlm_app\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker runs the container and map port 8000 of the container to port 8000 on your host, allowing us to access the FastAPI application using the browser, `requests` library or Postman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `vlm_app` using `requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: [345, 23, 640, 368]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from base64 import b64encode\n",
    "\n",
    "# The endpoint URL\n",
    "url = 'http://localhost:8000/predict'\n",
    "\n",
    "# base64 encode image so it can be passed in json\n",
    "image = b64encode(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\").content).decode(\"utf-8\")\n",
    "\n",
    "# Example question and context\n",
    "data = {\n",
    "    \"image\": image,\n",
    "    \"caption\": \"photo of a cat\",\n",
    "}\n",
    "\n",
    "# Sending a POST request\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Print the response from the server\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result shows that the model is successfully able to respond to the request. "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
