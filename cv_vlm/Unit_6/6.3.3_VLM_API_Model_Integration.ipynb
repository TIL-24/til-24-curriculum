{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Model Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will take a look at integrating the fine-tuned VLM model for object detection into a real-world application using FastAPI. The setup involves creating an API that receives an image and caption as input and returns the predicted bounding box. As before, we will create a Docker image for the FastAPI app to deploy it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Saving the Model and Processor\n",
    "After training the model in 6.3.2, you can save it along with its image processor to a directory. This is commonly done using the `save_pretrained()` method provided by the Hugging Face Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "# replace this with your actual model training setup\n",
    "checkpoint = \"google/owlvit-base-patch32\"\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "model_path = \"vlm_model.pth\"\n",
    "# After training:\n",
    "model.save_pretrained(model_path)\n",
    "processor.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating the Saved Model into FastAPI\n",
    "Now that your model and processor are saved, you can load them from the saved directory in your FastAPI application. This will allow your API to use the fine-tuned model to run object detection. The below example code is stored in `app.py` in the `vlm_app` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import base64\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "import io\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Fetch the model directory from the environment variable\n",
    "model_directory = os.getenv(\"MODEL_PATH\", \"/usr/src/app/models\")\n",
    "model_filename = \"vlm_model.pth\"  # Specify your model filename here\n",
    "\n",
    "# Full path to the model file\n",
    "model_path = os.path.join(model_directory, model_filename)\n",
    "\n",
    "# Load the model and tokenizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(\n",
    "    model_path, device_map=device\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_path, device_map=device)\n",
    "\n",
    "\n",
    "class VLMInput(BaseModel):\n",
    "    image: str\n",
    "    caption: str\n",
    "\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(data: VLMInput):\n",
    "    image_bytes = base64.b64decode(data.image)\n",
    "    im = Image.open(io.BytesIO(image_bytes))\n",
    "\n",
    "    # text prompts\n",
    "    inputs = processor(text=[data.caption], images=im, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        target_sizes = torch.tensor([im.size[::-1]])\n",
    "        results = processor.post_process_object_detection(\n",
    "            outputs, threshold=0.1, target_sizes=target_sizes\n",
    "        )[0]\n",
    "\n",
    "    bbox = results[\"boxes\"].tolist()\n",
    "    return bbox\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dockerfile\n",
    "Create a `Dockerfile` in the same directory as your FastAPI app (`app.py`). This file will define the Docker image that includes your app and all its dependencies.\n",
    "\n",
    "```docker\n",
    "FROM us-docker.pkg.dev/deeplearning-platform-release/gcr.io/pytorch-gpu.2-2.py310\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /usr/src/app\n",
    "\n",
    "COPY . /usr/src/app\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Make port 8000 available to the world outside this container\n",
    "EXPOSE 8000\n",
    "\n",
    "# Define environment variable\n",
    "ENV MODEL_PATH=/usr/src/app/models\n",
    "\n",
    "# Run app.py when the container launches\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Requirements File\n",
    "Create a `requirements.txt` file that lists the packages that your app depends on. Make sure to include fastapi, uvicorn, torch, transformers, and any other required libraries. Torch isn't included in this `requirements.txt` because it's included in the starting Docker image (i.e. the image indicated in the first `FROM` line in the `Dockerfile`).\n",
    "\n",
    "```txt\n",
    "fastapi\n",
    "uvicorn[standard]\n",
    "pydantic\n",
    "transformers==4.37.0\n",
    "accelerate\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Docker Image\n",
    "From your project directory (where your `Dockerfile` and `app.py` are located), run the following command to build the Docker image\n",
    "```bash\n",
    "docker build -t vlm_app .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Docker Container\n",
    "```bash\n",
    "docker run -p 8000:8000 vlm_app\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker runs the container and map port 8000 of the container to port 8000 on your host, allowing us to access the FastAPI application using the browser, `requests` library or Postman."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing `vlm_app` using `requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: [[32.91341018676758, 2088.8251953125, 3719.854736328125, 5634.173828125]]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from base64 import b64encode\n",
    "\n",
    "# The endpoint URL\n",
    "url = 'http://localhost:8000/predict'\n",
    "\n",
    "# base64 encode image so it can be passed in json\n",
    "with open(\"../assets/dog1.jpg\", \"rb\") as f:\n",
    "    image = b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "# Example question and context\n",
    "data = {\n",
    "    \"image\": image,\n",
    "    \"caption\": \"dog\",\n",
    "}\n",
    "\n",
    "# Sending a POST request\n",
    "response = requests.post(url, json=data)\n",
    "\n",
    "# Print the response from the server\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result shows that the model is successfully able to respond to the request. "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
