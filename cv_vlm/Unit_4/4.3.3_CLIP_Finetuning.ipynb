{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd5ec949-8b86-4a7f-b031-01767c573e5d",
   "metadata": {},
   "source": [
    "# Fine-tuning CLIP\n",
    "Run the following cell if running this Jupyter Notebook on Google Colab to install additional necessary libraries before you begin. If you are running this on your Vertex AI Workbench Instance, you will likely already have installed these libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bef171-f4f0-404a-b459-b71df7967663",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for google Colab\n",
    "!pip install accelerate transformers==4.37.0 datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1897940c-1da6-4451-85aa-2b5a32c11580",
   "metadata": {},
   "source": [
    "## Initialize CLIP Model\n",
    "Here we initialize the CLIP model as well as a particular tokenizer; here we've chosen the RoBERTa tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13e72766-d808-4418-8bfb-a6b174783193",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The projection layer and logit scale weights `['visual_projection.weight', 'text_projection.weight', 'logit_scale']` are newly initialized. You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from torchvision.io import ImageReadMode, read_image\n",
    "from torchvision.transforms import CenterCrop, ConvertImageDtype, Normalize, Resize\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    VisionTextDualEncoderModel,\n",
    "    VisionTextDualEncoderProcessor,\n",
    "    AutoTokenizer,\n",
    "    AutoImageProcessor\n",
    ")\n",
    "\n",
    "model = VisionTextDualEncoderModel.from_vision_text_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\", \"FacebookAI/roberta-base\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = VisionTextDualEncoderProcessor(image_processor, tokenizer)\n",
    "config = model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d6e8e-72ac-4ac9-9d91-9227f2fe6622",
   "metadata": {},
   "source": [
    "Now we load our datasets. Here we're loading a small dummy COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06a35c7-6244-4f16-967c-f2250dcbe76a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# load datasets, loading dummy COCO dataset for this\n",
    "dataset = load_dataset(\"ydshieh/coco_dataset_script\", \"2017\", data_dir=\"./dummy_data/\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c4798-5f10-4afb-831f-f96195490eb7",
   "metadata": {},
   "source": [
    "We need to pre-process our dataset such that our model will be able to recognize it. So first we define our image preprocessing logic (e.g. resizing, converting to the correct datatype, normalization, etc.), as well as our text preprocessing logic (i.e. tokenization), then apply it to our datasets, both train and eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04a6deb7-9456-4898-b832-94f85f4ecb77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We use torchvision for faster image pre-processing. The transforms are implemented as nn.Module,\n",
    "# so we jit it to be faster.\n",
    "class Transform(torch.nn.Module):\n",
    "    def __init__(self, image_size, mean, std):\n",
    "        super().__init__()\n",
    "        self.transforms = torch.nn.Sequential(\n",
    "            Resize([image_size], interpolation=InterpolationMode.BICUBIC),\n",
    "            CenterCrop(image_size),\n",
    "            ConvertImageDtype(torch.float),\n",
    "            Normalize(mean, std),\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \"\"\"`x` should be an instance of `PIL.Image.Image`\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = self.transforms(x)\n",
    "        return x\n",
    "\n",
    "# For preprocessing the datasets.\n",
    "# Initialize torchvision transforms and jit it for faster processing.\n",
    "image_transformations = Transform(\n",
    "    config.vision_config.image_size, image_processor.image_mean, image_processor.image_std\n",
    ")\n",
    "image_transformations = torch.jit.script(image_transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158017bd-106e-49b2-9ec6-a020ef6a134b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(dataset, split):\n",
    "    # Preprocessing the datasets.\n",
    "    data = dataset[split]\n",
    "    # We need to tokenize inputs and targets.\n",
    "    column_names = data.column_names\n",
    "\n",
    "    # 6. Get the column names for input/target.\n",
    "    image_column = \"image_path\"\n",
    "    caption_column = \"caption\"\n",
    "    dataset_columns = (image_column, caption_column)\n",
    "\n",
    "    # Preprocessing the datasets.\n",
    "    # We need to tokenize input captions and transform the images.\n",
    "    def tokenize_captions(examples):\n",
    "        captions = list(examples[caption_column])\n",
    "        text_inputs = tokenizer(captions, padding=\"max_length\", truncation=True)\n",
    "        examples[\"input_ids\"] = text_inputs.input_ids\n",
    "        examples[\"attention_mask\"] = text_inputs.attention_mask\n",
    "        return examples\n",
    "\n",
    "    def transform_images(examples):\n",
    "        images = [read_image(image_file, mode=ImageReadMode.RGB) for image_file in examples[image_column]]\n",
    "        examples[\"pixel_values\"] = [image_transformations(image) for image in images]\n",
    "        return examples\n",
    "\n",
    "    data = data.map(\n",
    "        function=tokenize_captions,\n",
    "        batched=True,\n",
    "        remove_columns=[col for col in column_names if col != image_column],\n",
    "        desc=f\"Running tokenizer on {split} dataset\",\n",
    "    )\n",
    "\n",
    "    # Transform images on the fly as doing it on the whole dataset takes too much time.\n",
    "    data.set_transform(transform_images)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c47421-96d7-442d-8ca9-71a87adb088a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4864a639499849b284baa1c7d2cd294a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'transform'=<function preprocess_dataset.<locals>.transform_images at 0x7f14d4700d30> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = preprocess_dataset(dataset, \"train\")\n",
    "eval_dataset = preprocess_dataset(dataset, \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a81369-f7ed-4e4b-8477-7c38bf29f71b",
   "metadata": {},
   "source": [
    "Finally we need to write a small function to handle the batching logic for our training. This collates all passed training items in the batch together such that we can pass it to the model for training, along with the kwarg `return_loss=True` such that the model will return its loss for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696678d7-2869-4a02-b3c5-481bd6419b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in examples], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([example[\"attention_mask\"] for example in examples], dtype=torch.long)\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"return_loss\": True,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d0a242-1823-4515-850f-19d9bb2ae2c8",
   "metadata": {},
   "source": [
    "Now we're ready to actually train our CLIP model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fb72d8d-7d65-4a65-9954-ae4b12d71911",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize Trainer\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=0,\n",
    "    weight_decay=0.1,\n",
    "    per_device_train_batch_size=16,\n",
    "    remove_unused_columns=False,\n",
    "    output_dir=\"clip-finetune\",\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec776e85-d358-4320-ac1a-3cc746747853",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.0719118118286133, 'eval_runtime': 2.9102, 'eval_samples_per_second': 27.49, 'eval_steps_per_second': 3.436, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b588b-1ce1-4007-8682-3fa244a72ec2",
   "metadata": {},
   "source": [
    "Once the model is trained, we can save it to our defined `output_dir` (in this case `clip-finetune`) so we can import it into our applications later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e68105-e4ed-481c-bc7c-57ba2179b7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(\"clip-finetune\")\n",
    "image_processor.save_pretrained(\"clip-finetune\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc07fb-87e0-4898-8c87-df2ac1a1d5fb",
   "metadata": {},
   "source": [
    "## Resources\n",
    "* [HF Transformers on training CLIP](https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
