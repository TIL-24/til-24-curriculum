{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65636d9e-561d-488a-a233-705b5a656e61",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67f8e931-4766-4b9a-8aed-b1e6e1371892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a1cfa-94ab-47aa-85d5-e9ec6c2c363c",
   "metadata": {},
   "source": [
    "logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d19b0e8-8f40-44e2-9764-96b30f0f6471",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging (example for `logging` module)\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,  # Adjust as needed (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a87514-3897-4902-99ac-24119117cd8e",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://github.com/huggingface/transformers/issues/28778\n",
    "- https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb\n",
    "\n",
    "Also helpful, but through google's research scenic repo\n",
    "- https://github.com/google-research/scenic/issues/542\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb3176-f38d-4fdf-bee7-43bf6d1e6997",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "- https://huggingface.co/datasets/cppe-5\n",
    "\n",
    "## Dataset Summary\n",
    "\n",
    "CPPE - 5 (Medical Personal Protective Equipment) is a new challenging dataset with the goal to allow the study of subordinate categorization of medical personal protective equipments, which is not possible with other popular data sets that focus on broad level categories.\n",
    "\n",
    "Some features of this dataset are:\n",
    "\n",
    "- high quality images and annotations (~4.6 bounding boxes per image)\n",
    "- real-life images unlike any current such dataset\n",
    "- majority of non-iconic images (allowing easy deployment to real-world environments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1af8836-9989-4309-b577-b9ff89befc9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Coverall', 'Face_Shield', 'Gloves', 'Goggles', 'Mask']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "cppe5 = load_dataset(\"cppe-5\")\n",
    "\n",
    "categories = cppe5[\"train\"].features[\"objects\"].feature[\"category\"].names\n",
    "id2label = {index: x for index, x in enumerate(categories, start=0)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "id2label\n",
    "text_inputs =list(id2label.values())\n",
    "print(text_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e96c4-f4f2-42b3-bdda-2d9e8dd230f5",
   "metadata": {},
   "source": [
    "As a final step of getting familiar with the data, explore it for potential issues. One common problem with datasets for object detection is bounding boxes that “stretch” beyond the edge of the image. Such “runaway” bounding boxes can raise errors during training and should be addressed at this stage. There are a few examples with this issue in this dataset. To keep things simple in this guide, we remove these images from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e915d8d-6a11-445c-8645-4c6af367b731",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_idx = [590, 821, 822, 875, 876, 878, 879]\n",
    "keep = [i for i in range(len(cppe5[\"train\"])) if i not in remove_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca8a45-0757-451e-a274-8d03089a3a2b",
   "metadata": {},
   "source": [
    "Let's limit to only 100 training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "222a3513-244c-4bf0-b2ed-62d5d8fba9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = keep[0:100]\n",
    "cppe5[\"train\"] = cppe5[\"train\"].select(keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21d7eca6-5129-4964-8ae4-faa3fdc68d26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "# using image processor from detr\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "checkpoint = \"google/owlvit-base-patch32\"\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "from transformers import AutoModelForZeroShotObjectDetection\n",
    "\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "import albumentations\n",
    "import numpy as np\n",
    "\n",
    "transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(480, 480),\n",
    "        albumentations.HorizontalFlip(p=1.0),\n",
    "        albumentations.RandomBrightnessContrast(p=1.0),\n",
    "    ],\n",
    "    bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[\"category\"]),\n",
    ")\n",
    "\n",
    "def formatted_anns(image_id, category, area, bbox):\n",
    "    annotations = []\n",
    "    for i in range(0, len(category)):\n",
    "        new_ann = {\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category[i],\n",
    "            \"isCrowd\": 0,\n",
    "            \"area\": area[i],\n",
    "            \"bbox\": list(bbox[i]),\n",
    "        }\n",
    "        annotations.append(new_ann)\n",
    "\n",
    "    return annotations\n",
    "\n",
    "# transforming a batch\n",
    "def transform_aug_ann(examples):\n",
    "    image_ids = examples[\"image_id\"]\n",
    "    images, bboxes, area, categories = [], [], [], []\n",
    "    transformed_data = []\n",
    "    for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n",
    "        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
    "        out = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n",
    "\n",
    "        area.append(objects[\"area\"])\n",
    "        images.append(out[\"image\"])\n",
    "        bboxes.append(out[\"bboxes\"])\n",
    "        categories.append(out[\"category\"])\n",
    "        transformed_data.append(processor(text=text_inputs, images=image, return_tensors=\"pt\"))\n",
    "\n",
    "    \n",
    "    return {\"transformed_data\":transformed_data}\n",
    "\n",
    "# transforming a batch\n",
    "def transform_aug_ann_labels(examples):\n",
    "    image_ids = examples[\"image_id\"]\n",
    "    images, bboxes, area, categories = [], [], [], []\n",
    "    for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n",
    "        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
    "        out = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n",
    "\n",
    "        area.append(objects[\"area\"])\n",
    "        images.append(out[\"image\"])\n",
    "        bboxes.append(out[\"bboxes\"])\n",
    "        categories.append(out[\"category\"])\n",
    "\n",
    "    targets = [\n",
    "        {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\n",
    "        for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\n",
    "    ]\n",
    "\n",
    "    return image_processor(images=images, annotations=targets, return_tensors=\"pt\")\n",
    "\n",
    "transform_1 = cppe5[\"train\"].with_transform(transform_aug_ann)\n",
    "transform_2 = cppe5[\"train\"].with_transform(transform_aug_ann_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca23e6e6-16b1-42e7-814c-2a05763aa67e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "data = []\n",
    "for i in range(len(transform_1)):\n",
    "    dict_ = {}\n",
    "    dict_[\"input_ids\"] = transform_1[i][\"transformed_data\"][\"input_ids\"]\n",
    "    dict_[\"attention_mask\"] = transform_1[i][\"transformed_data\"][\"attention_mask\"]\n",
    "    dict_[\"pixel_values\"] = transform_1[i][\"transformed_data\"][\"pixel_values\"][0]\n",
    "    dict_[\"labels\"] = transform_2[i][\"labels\"]\n",
    "    data.append(dict_)\n",
    "\n",
    "# Preprocessed Training Data\n",
    "train_dataset = Dataset.from_list(data)\n",
    "train_dataset.features\n",
    "\n",
    "# Using Detr-Loss calculation https://github.com/facebookresearch/detr/blob/main/models/matcher.py\n",
    "# https://www.kaggle.com/code/bibhasmondal96/detr-from-scratch\n",
    "\n",
    "class BoxUtils(object):\n",
    "    @staticmethod\n",
    "    def box_cxcywh_to_xyxy(x):\n",
    "        x_c, y_c, w, h = x.unbind(-1)\n",
    "        b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "             (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "        return torch.stack(b, dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def box_xyxy_to_cxcywh(x):\n",
    "        x0, y0, x1, y1 = x.unbind(-1)\n",
    "        b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
    "             (x1 - x0), (y1 - y0)]\n",
    "        return torch.stack(b, dim=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_bboxes(out_bbox, size):\n",
    "        img_h, img_w = size\n",
    "        b = BoxUtils.box_cxcywh_to_xyxy(out_bbox)\n",
    "        b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "        return b\n",
    "\n",
    "    @staticmethod\n",
    "    def box_area(boxes):\n",
    "        \"\"\"\n",
    "        Computes the area of a set of bounding boxes, which are specified by its\n",
    "        (x1, y1, x2, y2) coordinates.\n",
    "        Arguments:\n",
    "            boxes (Tensor[N, 4]): boxes for which the area will be computed. They\n",
    "                are expected to be in (x1, y1, x2, y2) format\n",
    "        Returns:\n",
    "            area (Tensor[N]): area for each box\n",
    "        \"\"\"\n",
    "        return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "        \n",
    "    @staticmethod\n",
    "    # modified from torchvision to also return the union\n",
    "    def box_iou(boxes1, boxes2):\n",
    "        area1 = BoxUtils.box_area(boxes1)\n",
    "        area2 = BoxUtils.box_area(boxes2)\n",
    "\n",
    "        lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
    "        rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
    "\n",
    "        wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "        inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
    "\n",
    "        union = area1[:, None] + area2 - inter\n",
    "\n",
    "        iou = inter / union\n",
    "        return iou, union\n",
    "\n",
    "    @staticmethod\n",
    "    def generalized_box_iou(boxes1, boxes2):\n",
    "        \"\"\"\n",
    "        Generalized IoU from https://giou.stanford.edu/\n",
    "        The boxes should be in [x0, y0, x1, y1] format\n",
    "        Returns a [N, M] pairwise matrix, where N = len(boxes1)\n",
    "        and M = len(boxes2)\n",
    "        \"\"\"\n",
    "        # degenerate boxes gives inf / nan results\n",
    "        # so do an early check\n",
    "        assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
    "        assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
    "        iou, union = BoxUtils.box_iou(boxes1, boxes2)\n",
    "\n",
    "        lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
    "        rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "\n",
    "        wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
    "        area = wh[:, :, 0] * wh[:, :, 1]\n",
    "\n",
    "        return iou - (area - union) / area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "236070f1-793e-401c-8383-aefd82bfed3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhowt51\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jupyter/til-24-curriculum/cv_vlm/Unit_4/wandb/run-20240428_033428-oqbk52mu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/howt51/huggingface/runs/oqbk52mu' target=\"_blank\">wobbly-plant-8</a></strong> to <a href='https://wandb.ai/howt51/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/howt51/huggingface' target=\"_blank\">https://wandb.ai/howt51/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/howt51/huggingface/runs/oqbk52mu' target=\"_blank\">https://wandb.ai/howt51/huggingface/runs/oqbk52mu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 03:34, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>60.692300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.516100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.303000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.079400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=19.3977197265625, metrics={'train_runtime': 229.149, 'train_samples_per_second': 0.873, 'train_steps_per_second': 0.873, 'total_flos': 12238430880000.0, 'train_loss': 19.3977197265625, 'epoch': 2.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "class HungarianMatcher(nn.Module):\n",
    "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
    "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
    "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
    "    while the others are un-matched (and thus treated as non-objects).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 1):\n",
    "        \"\"\"Creates the matcher\n",
    "        Params:\n",
    "            cost_class: This is the relative weight of the classification error in the matching cost\n",
    "            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n",
    "            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cost_class = cost_class\n",
    "        self.cost_bbox = cost_bbox\n",
    "        self.cost_giou = cost_giou\n",
    "        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" Performs the matching\n",
    "        Params:\n",
    "            outputs: This is a dict that contains at least these entries:\n",
    "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
    "                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
    "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
    "                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n",
    "                           objects in the target) containing the class labels\n",
    "                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
    "        Returns:\n",
    "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
    "                - index_i is the indices of the selected predictions (in order)\n",
    "                - index_j is the indices of the corresponding selected targets (in order)\n",
    "            For each batch element, it holds:\n",
    "                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
    "        \"\"\"\n",
    "        logging.info(f\"{outputs.keys()=}\")\n",
    "        bs, num_queries = outputs[\"logits\"].shape[:2]\n",
    "\n",
    "        # We flatten to compute the cost matrices in a batch\n",
    "        out_prob = outputs[\"logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
    "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
    "\n",
    "        # Also concat the target labels and boxes\n",
    "        tgt_ids = torch.cat([v[\"class_labels\"] for v in targets])\n",
    "        logging.info(f\"forward - {tgt_ids}\")\n",
    "        tgt_ids = tgt_ids.int()\n",
    "        logging.info(f\"forward - {tgt_ids}\")\n",
    "\n",
    "\n",
    "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
    "\n",
    "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
    "        # but approximate it in 1 - proba[target class].\n",
    "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
    "        cost_class = -out_prob[:, tgt_ids]\n",
    "\n",
    "        # Compute the L1 cost between boxes\n",
    "        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
    "\n",
    "        # Compute the giou cost betwen boxes\n",
    "        cost_giou = -BoxUtils.generalized_box_iou(\n",
    "            BoxUtils.box_cxcywh_to_xyxy(out_bbox),\n",
    "            BoxUtils.box_cxcywh_to_xyxy(tgt_bbox)\n",
    "        )\n",
    "\n",
    "        # Final cost matrix\n",
    "        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n",
    "        C = C.view(bs, num_queries, -1).cpu()\n",
    "\n",
    "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
    "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
    "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
    "\n",
    "class SetCriterion(nn.Module):\n",
    "    \"\"\" This class computes the loss for DETR.\n",
    "    The process happens in two steps:\n",
    "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
    "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n",
    "        \"\"\" Create the criterion.\n",
    "        Parameters:\n",
    "            num_classes: number of object categories, omitting the special no-object category\n",
    "            matcher: module able to compute a matching between targets and proposals\n",
    "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
    "            eos_coef: relative classification weight applied to the no-object category\n",
    "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.matcher = matcher\n",
    "        self.weight_dict = weight_dict\n",
    "        self.eos_coef = eos_coef\n",
    "        self.losses = losses\n",
    "        empty_weight = torch.ones(self.num_classes + 1)\n",
    "        empty_weight[-1] = self.eos_coef\n",
    "        self.register_buffer('empty_weight', empty_weight)\n",
    "\n",
    "    def loss_labels(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\"Classification loss (NLL)\n",
    "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
    "        \"\"\"\n",
    "        logging.info(f\"loss_labels - {outputs.keys()}\")\n",
    "        assert 'logits' in outputs\n",
    "        src_logits = outputs['logits']\n",
    "\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        target_classes_o = torch.cat([t[\"class_labels\"][J] for t, (_, J) in zip(targets, indices)]).to(torch.int64)\n",
    "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
    "                                    dtype=torch.int64, device=src_logits.device).to(torch.int64)\n",
    "        target_classes[idx] = target_classes_o\n",
    "\n",
    "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, self.empty_weight)\n",
    "        losses = {'loss_ce': loss_ce}\n",
    "        return losses\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
    "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
    "        \"\"\"\n",
    "        pred_logits = outputs['logits']\n",
    "        device = pred_logits.device\n",
    "        tgt_lengths = torch.as_tensor([len(v[\"class_labels\"]) for v in targets], device=device)\n",
    "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
    "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
    "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
    "        losses = {'cardinality_error': card_err}\n",
    "        return losses\n",
    "\n",
    "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
    "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
    "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
    "           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.\n",
    "        \"\"\"\n",
    "        assert 'pred_boxes' in outputs\n",
    "        idx = self._get_src_permutation_idx(indices)\n",
    "        src_boxes = outputs['pred_boxes'][idx]\n",
    "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
    "\n",
    "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
    "\n",
    "        losses = {}\n",
    "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
    "\n",
    "        loss_giou = 1 - torch.diag(BoxUtils.generalized_box_iou(\n",
    "            BoxUtils.box_cxcywh_to_xyxy(src_boxes),\n",
    "            BoxUtils.box_cxcywh_to_xyxy(target_boxes))\n",
    "        )\n",
    "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
    "        return losses\n",
    "\n",
    "    def _get_src_permutation_idx(self, indices):\n",
    "        # permute predictions following indices\n",
    "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
    "        src_idx = torch.cat([src for (src, _) in indices])\n",
    "        return batch_idx, src_idx\n",
    "\n",
    "    def _get_tgt_permutation_idx(self, indices):\n",
    "        # permute targets following indices\n",
    "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
    "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
    "        return batch_idx, tgt_idx\n",
    "\n",
    "    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n",
    "        loss_map = {\n",
    "            'labels': self.loss_labels,\n",
    "            'cardinality': self.loss_cardinality,\n",
    "            'boxes': self.loss_boxes,\n",
    "        }\n",
    "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
    "        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\" This performs the loss computation.\n",
    "        Parameters:\n",
    "             outputs: dict of tensors, see the output specification of the model for the format\n",
    "             targets: list of dicts, such that len(targets) == batch_size.\n",
    "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
    "        \"\"\"\n",
    "        logging.info(f\"{type(outputs)=}\")\n",
    "        logging.info(f\"{type(targets)=}\")\n",
    "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
    "\n",
    "        # Retrieve the matching between the outputs of the last layer and the targets\n",
    "        indices = self.matcher(outputs_without_aux, targets)\n",
    "\n",
    "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
    "        num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n",
    "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
    "\n",
    "        # Compute all the requested losses\n",
    "        losses = {}\n",
    "        for loss in self.losses:\n",
    "            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n",
    "        return losses # sum(loss.values())#losses\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    #input_ids = torch.Tensor([item[\"input_ids\"].tolist() for item in batch]).int()\n",
    "    input_ids = torch.Tensor([item[\"input_ids\"] for item in batch]).int()\n",
    "    input_ids = input_ids.to(device)\n",
    "    # attention_mask = torch.Tensor([item[\"attention_mask\"].tolist() for item in batch]).int()\n",
    "    attention_mask = torch.Tensor([item[\"attention_mask\"] for item in batch]).int()\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    # pixel_values = torch.Tensor([item[\"pixel_values\"].tolist() for item in batch])\n",
    "    pixel_values = torch.Tensor([item[\"pixel_values\"] for item in batch])\n",
    "    pixel_values = pixel_values.to(device)\n",
    "    labels = []\n",
    "    for item in batch:\n",
    "        for (key, value) in item[\"labels\"].items():\n",
    "            item[\"labels\"][key] = torch.Tensor(value).to(device)\n",
    "        labels.append(item[\"labels\"])\n",
    "     \n",
    "    batch = {}\n",
    "    batch[\"input_ids\"] = input_ids\n",
    "    batch[\"attention_mask\"] = attention_mask\n",
    "    batch[\"pixel_values\"] = pixel_values\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n",
    "\n",
    "\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"owlvit-base-patch32_FT_cppe5\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=2,\n",
    "    fp16=True,\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=1e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    gradient_accumulation_steps=1\n",
    ")\n",
    "\n",
    "# custom loss\n",
    "def custom_loss(logits, labels):\n",
    "    num_classes = 4\n",
    "    matcher = HungarianMatcher(cost_class = 1, cost_bbox = 5, cost_giou = 2)\n",
    "    weight_dict = {'loss_ce': 1, 'loss_bbox': 5, 'loss_giou': 2}\n",
    "    losses = ['labels', 'boxes', 'cardinality']\n",
    "    criterion = SetCriterion(num_classes, matcher=matcher, weight_dict=weight_dict, eos_coef=0.1, losses=losses)\n",
    "    criterion.to(device)\n",
    "    logging.info(f\"{type(logits)=}\")\n",
    "    logging.info(f\"{type(labels)=}\")\n",
    "    loss = criterion(logits, labels)\n",
    "    return loss\n",
    "\n",
    "# subclass trainer\n",
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "\n",
    "        inputs[\"input_ids\"] = inputs[\"input_ids\"][0]\n",
    "        inputs[\"attention_mask\"] = inputs[\"attention_mask\"][0]\n",
    "        outputs = model(**inputs, return_dict=True)\n",
    "        loss = custom_loss(outputs, labels)\n",
    "        loss = sum(loss.values())[0] #add\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# use new trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=processor\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62031f-ac20-45bb-a0f5-ceca84af0f48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python310cv",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "python310cv",
   "language": "python",
   "name": "python310cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
