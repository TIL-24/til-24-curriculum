{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with TensorFlow and Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Deep Learning\n",
    "\n",
    "Deep Learning is a subset of machine learning that employs algorithms known as neural networks to learn from and make decisions based on vast amounts of data. It is renowned for its ability to process large and complex datasets, with applications ranging from image and speech recognition to natural language processing and autonomous vehicles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "Ensure Python is installed on your system. Then, install TensorFlow and Keras by running the following command in a code cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "Neural Networks are computational models inspired by the human brain's structure. They consist of layers of nodes or \"neurons,\" interconnected to form a network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "Activation functions determine the output of a neural network node. Common examples include ReLU (Rectified Linear Unit), Sigmoid, and Softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "Loss functions measure how well the model's predictions match the target data during training. Common loss functions include Mean Squared Error for regression tasks and Cross-Entropy for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "Optimizers are algorithms that adjust the weights of the network to minimize the loss function. Examples include SGD (Stochastic Gradient Descent) and Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/NN_learning.png\" alt=\"drawing\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This video](https://www.youtube.com/watch?v=aircAruvnKk&ab_channel=3Blue1Brown) does an exceptional job of introducing neural networks learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Simple Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data Preparation\n",
    "\n",
    "In this example, we will work with the MNIST Dataset. \n",
    "\n",
    "The MNIST dataset, short for Modified National Institute of Standards and Technology dataset, is one of the most iconic datasets in the field of machine learning and deep learning. Comprising a collection of 70,000 handwritten digits, it is split into a training set of 60,000 examples and a test set of 10,000 examples. Each image is grayscale, 28x28 pixels, and labeled with the digit it represents, ranging from 0 to 9. MNIST serves as a benchmark dataset for evaluating the performance of algorithms in the domain of image recognition. Since its release, it has become a standard dataset for beginners and researchers alike to test and benchmark their machine learning and deep learning models. The simplicity of MNIST allows for quick testing of concepts or algorithms, making it an excellent starting point for anyone new to the field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/mnist.webp\" alt=\"drawing\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values of the images\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Modeling\n",
    "\n",
    "Creating a model in `keras` involves several steps:\n",
    "\n",
    "1. **Create your network architecture** using the `Sequential` class.\n",
    "2. **Compile your model** by specifying the loss function, optimizer, and metrics.\n",
    "3. **Fit your model** to the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Model Initialization Steps\n",
    "\n",
    "1. **Instantiate Model**: We start by creating an empty sequential model with `Sequential()`. Neural networks are sequential models, where we build a sequence of layers. Using the `Sequential()` class is one of the simpler ways to accomplish this.\n",
    "2. **Add Layers**: We add layers to our model one by one using `Dense()`.\n",
    "   - **First hidden layer**: We specify the number of nodes, use the `ReLU` activation function, and set the input shape based on `X` (specifically, the number of feature columns; the number of rows is handled automatically). The ideal number of nodes is often trial and error, but a common starting point is to align it with the number of input features.\n",
    "   - **Second hidden layer**: This layer automatically takes the output of the first hidden layer as its input. With the `Sequential()` class handling layer connections, we simply add another layer with a specified number of nodes and `ReLU` activation.\n",
    "   - **Output layer**: We specify the number of nodes equal to the number of classes and use the softmax activation function for a multi-class classification problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Compilation Steps\n",
    "\n",
    "Compiling our model in Keras requires:\n",
    "\n",
    "1. **Choosing an optimizer**: We typically default to `Adam` due to its effectiveness across a wide range of tasks. Adam stands for Adaptive Moment Estimation and combines the best properties of the AdaGrad and RMSProp algorithms to handle sparse gradients on noisy problems. Adam is efficient in terms of computation and requires little memory. It's particularly favored for its adaptiveness, making it suitable for most problems without needing much customization or tuning of the learning rate. This optimizer adjusts the learning rate during training, which helps converge faster and more effectively.\n",
    "2. **Specifying the loss function**: Crucial for neural networks due to the need for a differentiable loss for gradient descent. In our case, we use `sparse_categorical_crossentropy` for multi-class classification. This choice is motivated by our problem type—classification—where we aim to categorize inputs into multiple classes (image to 10 digits). This function compares the distribution of the predictions (the outputs of the softmax function in our model) with the true distribution. It's a good fit for classification problems with multiple classes.\n",
    "3. **Defining metrics**: Metrics like `accuracy` give us a performance indicator to monitor during training. They do not impact the training directly but provide insights into model performance and potential adjustments to the learning rate or other parameters. In classification tasks, accuracy is a common and intuitive metric, providing a straightforward understanding of how well the model is performing. It's particularly useful in balanced datasets where each class has a roughly equal number of instances. However, it's important to complement accuracy with other metrics when dealing with imbalanced datasets or when other aspects of the model's performance are critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Training Steps\n",
    "\n",
    "**Iterations per Training Epoch**: With a hypothetical dataset of 100,000 rows, assuming a 20% validation split leaves us 80,000 rows for training. With a batch size of 32, we get 2,500 iterations per epoch.\n",
    "\n",
    "- **Epochs**: Training neural networks involves multiple epochs, where the entire dataset is passed through the model in each epoch. A common range for the number of epochs is 5-10, with adjustments to the learning rate if you're considering higher epochs for improved accuracy.\n",
    "\n",
    "**Interpreting Output**:\n",
    "\n",
    "- **Loss and Accuracy**: Post each epoch, we observe the average training loss and accuracy, providing insights into the model's learning progress.\n",
    "- **Validation Loss and Accuracy**: Calculated at the end of each epoch, these metrics help gauge the model's generalization ability and highlight potential overfitting if the training accuracy significantly surpasses validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 [==============================] - 1s 971us/step - loss: 0.3362 - accuracy: 0.9063 - val_loss: 0.1803 - val_accuracy: 0.9488\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 1s 834us/step - loss: 0.1513 - accuracy: 0.9561 - val_loss: 0.1312 - val_accuracy: 0.9643\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 1s 837us/step - loss: 0.1057 - accuracy: 0.9692 - val_loss: 0.1062 - val_accuracy: 0.9688\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 1s 830us/step - loss: 0.0797 - accuracy: 0.9770 - val_loss: 0.0996 - val_accuracy: 0.9696\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 1s 842us/step - loss: 0.0617 - accuracy: 0.9825 - val_loss: 0.0923 - val_accuracy: 0.9728\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 1s 857us/step - loss: 0.0497 - accuracy: 0.9858 - val_loss: 0.0917 - val_accuracy: 0.9716\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 1s 843us/step - loss: 0.0393 - accuracy: 0.9893 - val_loss: 0.0861 - val_accuracy: 0.9746\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 1s 853us/step - loss: 0.0327 - accuracy: 0.9910 - val_loss: 0.0826 - val_accuracy: 0.9743\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 1s 894us/step - loss: 0.0266 - accuracy: 0.9929 - val_loss: 0.0840 - val_accuracy: 0.9749\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 1s 844us/step - loss: 0.0225 - accuracy: 0.9941 - val_loss: 0.0860 - val_accuracy: 0.9756\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_images, train_labels, batch_size=64, epochs=10,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluating the Model\n",
    "\n",
    "Test the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.0753 - accuracy: 0.9774 - 122ms/epoch - 390us/step\n",
      "\n",
      "Test accuracy: 0.977400004863739\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "This notebook introduced the basics of training a deep learning model using TensorFlow and Keras, from setting up the environment and understanding key concepts to building, training, and evaluating a simple model. For further exploration, consider diving into more complex models, experimenting with different datasets, and exploring the extensive features of TensorFlow and Keras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
