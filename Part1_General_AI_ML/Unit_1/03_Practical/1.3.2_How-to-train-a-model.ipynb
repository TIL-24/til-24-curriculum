{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning with PyTorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Deep Learning\n",
    "\n",
    "Deep Learning is a subset of machine learning that employs algorithms known as neural networks to learn from and make decisions based on vast amounts of data. It is renowned for its ability to process large and complex datasets, with applications ranging from image and speech recognition to natural language processing and autonomous vehicles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "Ensure Python is installed on your system. Then, install PyTorch by running the following command in a code cell.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "!pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks\n",
    "\n",
    "Neural Networks are computational models inspired by the human brain's structure. They consist of layers of nodes or \"neurons,\" interconnected to form a network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "Activation functions determine the output of a neural network node. Common examples include ReLU (Rectified Linear Unit), Sigmoid, and Softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "Loss functions measure how well the model's predictions match the target data during training. Common loss functions include Mean Squared Error for regression tasks and Cross-Entropy for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "Optimizers are algorithms that adjust the weights of the network to minimize the loss function. Examples include SGD (Stochastic Gradient Descent) and Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/NN_learning.png\" alt=\"drawing\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This video](https://www.youtube.com/watch?v=aircAruvnKk&ab_channel=3Blue1Brown) does an exceptional job of introducing neural networks learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Simple Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Data Preparation\n",
    "\n",
    "In this example, we will work with the MNIST Dataset. \n",
    "\n",
    "The MNIST dataset, short for Modified National Institute of Standards and Technology dataset, is one of the most iconic datasets in the field of machine learning and deep learning. Comprising a collection of 70,000 handwritten digits, it is split into a training set of 60,000 examples and a test set of 10,000 examples. Each image is grayscale, 28x28 pixels, and labeled with the digit it represents, ranging from 0 to 9. MNIST serves as a benchmark dataset for evaluating the performance of algorithms in the domain of image recognition. Since its release, it has become a standard dataset for beginners and researchers alike to test and benchmark their machine learning and deep learning models. The simplicity of MNIST allows for quick testing of concepts or algorithms, making it an excellent starting point for anyone new to the field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/mnist.webp\" alt=\"drawing\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training data for validation\n",
    "train_size = int(0.8 * len(mnist_train))\n",
    "val_size = len(mnist_train) - train_size\n",
    "train_dataset, val_dataset = random_split(mnist_train, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(mnist_test, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Modeling\n",
    "\n",
    "Building a neural network model in PyTorch usually follows these key steps:\n",
    "\n",
    "- Define the model architecture using the `nn.Module` base class.\n",
    "- Set up loss function and optimizer.\n",
    "- Train the model on your dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 1. Model Definition\n",
    "\n",
    "**Class-based approach:** We create a class called NeuralNet that inherits from nn.Module. This gives us the building blocks to create our network.\n",
    "Key Layers:\n",
    "- `nn.Flatten`: Reshapes input 2D images into 1D vectors for the linear layers.\n",
    "- `nn.Linear`: Fully-connected layers that perform the core computations.\n",
    "- `nn.ReLU`: Non-linear activation function often used in hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loss & Optimization\n",
    "\n",
    "Compiling our model in PyTorch requires:\n",
    "\n",
    "1. **Choosing an optimizer**: We typically default to `torch.optim.Adam` due to its effectiveness across a wide range of tasks. Adam stands for Adaptive Moment Estimation and combines the best properties of the AdaGrad and RMSProp algorithms to handle sparse gradients on noisy problems. Adam is efficient in terms of computation and requires little memory. It's particularly favored for its adaptiveness, making it suitable for most problems without needing much customization or tuning of the learning rate. This optimizer adjusts the learning rate during training, which helps converge faster and more effectively.\n",
    "\n",
    "2. **Specifying the loss function**: Crucial for neural networks due to the need for a differentiable loss for gradient descent. In our case, we use `nn.CrossEntropyLoss` for multi-class classification. This choice is motivated by our problem type—classification—where we aim to categorize inputs into multiple classes (image to 10 digits). This function compares the distribution of the predictions (the outputs of the softmax function in our model) with the true distribution. It's a good fit for classification problems with multiple classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Training Steps\n",
    "\n",
    "**Iterations per Training Epoch**: With a hypothetical dataset of 100,000 rows, assuming a 20% validation split leaves us 80,000 rows for training. With a batch size of 32, we get 2,500 iterations per epoch.\n",
    "\n",
    "- **Epochs**: Training neural networks involves multiple epochs, where the entire dataset is passed through the model in each epoch. A common range for the number of epochs is 5-10, with adjustments to the learning rate if you're considering higher epochs for improved accuracy.\n",
    "\n",
    "- During each epoch:\n",
    "    - The `train` function is called to train the model using the training data loader.\n",
    "    - The `test` function is called to evaluate the model using the validation data loader, printing out the validation accuracy and loss.\n",
    "\n",
    "\n",
    "**Interpreting Output**:\n",
    "\n",
    "- **Loss and Accuracy**: Post each epoch, we can observe the average training loss and accuracy, providing insights into the model's learning progress.\n",
    "- **Validation Loss and Accuracy**: Calculated at the end of each epoch, these metrics help gauge the model's generalization ability and highlight potential overfitting if the training accuracy significantly surpasses validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy: 93.2%, Avg loss: 0.225838 \n",
      "Val Accuracy: 95.8%, Avg loss: 0.140560 \n",
      "Val Accuracy: 96.8%, Avg loss: 0.106705 \n",
      "Val Accuracy: 96.9%, Avg loss: 0.102025 \n",
      "Val Accuracy: 97.2%, Avg loss: 0.095793 \n",
      "Val Accuracy: 97.2%, Avg loss: 0.097478 \n",
      "Val Accuracy: 97.1%, Avg loss: 0.100423 \n",
      "Val Accuracy: 97.2%, Avg loss: 0.100610 \n",
      "Val Accuracy: 97.4%, Avg loss: 0.090294 \n",
      "Val Accuracy: 97.0%, Avg loss: 0.104901 \n"
     ]
    }
   ],
   "source": [
    "# Function to train the model\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Function to test the model\n",
    "def test(dataloader, model, loss_fn, test=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    if test == False:\n",
    "        print(f\"Val Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \")\n",
    "    else:\n",
    "        print(f\"Test Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \")\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "    test(val_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluating the Model\n",
    "\n",
    "Test the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.3%, Avg loss: 0.094405 \n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test(test_loader, model, loss_fn, test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "This notebook introduced the basics of training a deep learning model using PyTorch, from setting up the environment and understanding key concepts to building, training, and evaluating a simple model. For further exploration, consider diving into more complex models, experimenting with different datasets, and exploring the extensive features of PyTorch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
