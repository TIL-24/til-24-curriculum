{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability to adapt pre-trained models to specific tasks has revolutionized how we approach complex language problems. BERT (Bidirectional Encoder Representations from Transformers) has emerged as a particularly powerful model for a wide range of NLP tasks. The process of adapting a pre-trained model to a specific task is known as fine-tuning. Fine-tuning allows us to leverage the model's learned representations, adjusting them slightly with additional training on a smaller, task-specific dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "In this notebook, we illustrate the process of fine-tuning a pre-trained BERT model for a sentiment analysis task. We aim to demonstrate the effectiveness of fine-tuning by comparing the performance of the pre-trained model before and after fine-tuning on a small custom dataset. The task involves classifying text into two categories: positive and negative sentiment. Through this exercise, we seek to achieve two main goals:\n",
    "\n",
    "1. Showcase the Process of Fine-Tuning: We will walk through the steps required to fine-tune a pre-trained BERT model on a custom sentiment analysis dataset. This includes data preparation, model configuration, training, and evaluation.\n",
    "2. Demonstrate the Impact of Fine-Tuning: By comparing the model's performance on our task before and after fine-tuning, we can observe the improvements that can be achieved through this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom Dataset for Sentiment Analysis\n",
    "First, we construct a small dataset for demonstration. This dataset consists of texts labeled for sentiment: 1 for positive and 0 for negative. Normally, you'd use a more extensive dataset for robust model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'text': [\n",
    "        'I love this product!',\n",
    "        'Absolutely wonderful service.',\n",
    "        'Not what I expected, sadly.',\n",
    "        'The experience was bad, very bad.',\n",
    "        'Fantastic! Will come again.',\n",
    "        'Do not recommend.',\n",
    "        'Great value for the money.',\n",
    "        'Worst purchase I ever made.',\n",
    "        'Happy with my purchase!',\n",
    "        'Terrible, I hated it.'\n",
    "    ],\n",
    "    'label': [1, 1, 0, 0, 1, 0, 1, 0, 1, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "train_df = df.sample(frac=0.8, random_state=200) # 80% for training\n",
    "val_df = df.drop(train_df.index) # 20% for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above creates a pandas DataFrame from a dictionary. We then split this DataFrame into training and validation sets, ensuring that the model sees only a portion of the data during training, which helps evaluate its performance on unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data\n",
    "Preprocessing involves tokenizing the text data, converting it into a format BERT understands. This includes adding special tokens, padding, and creating attention masks. This code snippet prepares our dataset for training by tokenizing the text, padding sequences to a uniform length, and creating attention masks to help the model distinguish between content and padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def convert_examples_to_features(text, label):\n",
    "    # Tokenize and encode sentences in the BERT's way\n",
    "    input_ids, attention_masks = [], []\n",
    "    for t in text:\n",
    "        encoded = tokenizer.encode_plus(t, add_special_tokens=True, max_length=128, padding='max_length', truncation=True, return_attention_mask=True)\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return tf.data.Dataset.from_tensor_slices(({\"input_ids\": tf.constant(input_ids), \"attention_mask\": tf.constant(attention_masks)}, tf.constant(label))).batch(2)\n",
    "\n",
    "# Preparing dataset for training and validation\n",
    "train_data = convert_examples_to_features(train_df['text'].values, train_df['label'].values)\n",
    "val_data = convert_examples_to_features(val_df['text'].values, val_df['label'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Pre-trained Model Performance\n",
    "Before fine-tuning, we evaluate the pre-trained BERT model on our validation set to establish a performance baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step - loss: 0.7474 - accuracy: 0.5000\n",
      "Baseline accuracy (pre-trained): 50.00%\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "# Evaluate the pre-trained model\n",
    "baseline_result = model.evaluate(val_data)\n",
    "print(f\"Baseline accuracy (pre-trained): {baseline_result[1]*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune BERT on the Custom Dataset\n",
    "Now, we'll fine-tune the model on our training data and evaluate it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4/4 [==============================] - 18s 1s/step - loss: 0.7076 - accuracy: 0.6250 - val_loss: 0.5528 - val_accuracy: 1.0000\n",
      "Epoch 2/2\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.5410 - accuracy: 1.0000 - val_loss: 0.4668 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 0.4668 - accuracy: 1.0000\n",
      "Accuracy after fine-tuning: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "model.fit(train_data, epochs=2, validation_data=val_data)\n",
    "\n",
    "# Evaluate the fine-tuned model\n",
    "finetuned_result = model.evaluate(val_data)\n",
    "print(f\"Accuracy after fine-tuning: {finetuned_result[1]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Results\n",
    "We compare the baseline accuracy of the pre-trained model with the accuracy after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement in accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "improvement = finetuned_result[1] - baseline_result[1]\n",
    "print(f\"Improvement in accuracy: {improvement*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning process adapts the pre-trained BERT model to our specific sentiment analysis task. By comparing the model's accuracy before and after fine-tuning, we can observe the effect of this adaptation. The fine-tuned model showed an improved accuracy on the validation set, demonstrating the value of fine-tuning for custom NLP tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
