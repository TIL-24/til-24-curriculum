{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability to adapt pre-trained models to specific tasks has revolutionized how we approach complex language problems. BERT (Bidirectional Encoder Representations from Transformers) has emerged as a particularly powerful model for a wide range of NLP tasks. The process of adapting a pre-trained model to a specific task is known as fine-tuning. Fine-tuning allows us to leverage the model's learned representations, adjusting them slightly with additional training on a smaller, task-specific dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "In this notebook, we illustrate the process of using a pre-trained BERT model for a sentiment analysis task. We will demonstrate effectiveness of fine-tuning by comparing the performance of the pre-trained model used for transfer learning vs after fine-tuning on a small custom dataset. The task involves classifying text into two categories: positive and negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom Dataset for Sentiment Analysis\n",
    "First, we construct a small dataset for demonstration. This dataset consists of texts labeled for sentiment: 1 for positive and 0 for negative. Normally, you'd use a more extensive dataset for robust model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        label = self.labels[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          return_token_type_ids=False,\n",
    "          pad_to_max_length=True,\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "          'review_text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "texts = [\n",
    "        'I love this product!',\n",
    "        'Absolutely wonderful service.',\n",
    "        'Not what I expected, sadly.',\n",
    "        'The experience was bad, very bad.',\n",
    "        'Fantastic! Will come again.',\n",
    "        'Do not recommend.',\n",
    "        'Great value for the money.',\n",
    "        'Worst purchase I ever made.',\n",
    "        'Happy with my purchase!',\n",
    "        'Terrible, I hated it.'\n",
    "    ]\n",
    "labels = [1, 1, 0, 0, 1, 0, 1, 0, 1, 0] # 1 for positive, 0 for negative sentiment\n",
    "dataset = ClassificationDataset(texts, labels, tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning by Freezing Layers of BERT\n",
    "In transfer learning, we use a pre-trained model as it is for our task. Here, we'll use BERT to encode our texts and then use those embeddings for classification. In transfer learning, it's common to freeze the pre-trained layers of the model to prevent overfitting. This means you keep the weights of most layers (especially the earlier ones) fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7083183526992798, Accuracy: 50.00%\n",
      "Epoch 2, Loss: 0.5474739074707031, Accuracy: 50.00%\n",
      "Epoch 3, Loss: 0.8631421327590942, Accuracy: 50.00%\n",
      "Epoch 4, Loss: 0.7385900616645813, Accuracy: 50.00%\n",
      "Accuracy after Transfer Learning: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model for transfer learning\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Freezing Layers of BERT from weight updates\n",
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    _, preds = torch.max(predictions, dim=1)\n",
    "    correct = (preds == labels).float()\n",
    "    accuracy = correct.sum() / len(correct)\n",
    "    return accuracy * 100\n",
    "\n",
    "# Transfer Learning Training Loop\n",
    "model.train()  # Set the model to training mode\n",
    "for epoch in range(4):  # Assume 4 epochs for simplicity\n",
    "    model.train()\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Compute accuracy\n",
    "    model.eval()\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            total_accuracy += compute_accuracy(logits, labels).item()\n",
    "\n",
    "    avg_accuracy = total_accuracy / len(loader)\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}, Accuracy: {avg_accuracy:.2f}%')\n",
    "print(f'Accuracy after Transfer Learning: {avg_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a consistent accuracy of 50% across all epochs which is no better than random guessing on the binary classification task. The key factor contributing to this outcome is the approach of freezing the BERT model's layers, which means that the weights of the pre-trained BERT layers are not updated during the training process, limiting the model's ability to adapt to the nuances of the custom dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning BERT with All Layers Trainable\n",
    "In this example, we'll fine-tune the entire BERT model on our dataset. During fine-tuning, the common practice is to allow all (or most) of the layers to update their weights slightly according to the new data. This means that the layers aren not frozen, and the entire model is trainable, as shown in the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6042753458023071, Accuracy: 50.00%\n",
      "Epoch 2, Loss: 0.5723692178726196, Accuracy: 90.00%\n",
      "Epoch 3, Loss: 0.46594011783599854, Accuracy: 100.00%\n",
      "Epoch 4, Loss: 0.3435141444206238, Accuracy: 100.00%\n",
      "Accuracy after Fine-Tuning: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Re-initialize the model for fine-tuning\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Fine-Tuning Training Loop\n",
    "model.train()  # Set the model to training mode\n",
    "for epoch in range(4):  # Assume 4 epochs for simplicity\n",
    "    model.train()\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Compute accuracy\n",
    "    model.eval()\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            total_accuracy += compute_accuracy(logits, labels).item()\n",
    "\n",
    "    avg_accuracy = total_accuracy / len(loader)\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}, Accuracy: {avg_accuracy:.2f}%')\n",
    "\n",
    "print(f'Accuracy after Fine-Tuning: {avg_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With fine-tuning, the accuracy improved significantly from 50% in the first epoch to 100% by the third epoch, maintaining this level through the fourth epoch. This marked improvement demonstrates the effectiveness of fine-tuning, where updating the weights of the entire BERT model allows it to adapt more closely to the specific characteristics of the custom dataset.\n",
    "\n",
    "If we want to selectively freeze layers during fine-tuning, you can apply a similar technique as shown above for freezing layers, but you might choose to unfreeze more layers or only specific layers based on the task and dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning vs Fine-tuning\n",
    "When comparing the outcomes of transfer learning and fine-tuning on a custom dataset using BERT, we observe a stark contrast in performance:\n",
    "\n",
    "- Transfer Learning Accuracy: 50%\n",
    "- Fine-tuning Accuracy: 100%\n",
    "\n",
    "Transfer learning, with most of BERT's layers frozen, limited the model's ability to learn from the custom dataset, resulting in an accuracy equivalent to random guessing. On the other hand, fine-tuning the entire BERT model led to a significant improvement, with accuracy reaching 100%. This demonstrates the model's capacity to adapt to the specific nuances of the dataset through fine-tuning.\n",
    "\n",
    "For tasks involving custom datasets, especially when high accuracy is paramount, fine-tuning is recommended. Fine-tuning allows pre-trained models like BERT to adjust their learned representations to better fit the specific characteristics and requirements of the task at hand. While it may require more computational resources and time compared to transfer learning with layer freezing, the substantial gains in performance, as evidenced by our experiment, highlight its value for achieving optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "- **Transfer Learning**: Leverages the pre-trained knowledge of BERT on vast amounts of text data. Good starting point, especially with limited data, but may not fully adapt to your custom dataset's nuances. Typically involves freezing most of the pre-trained model's layers to prevent overfitting on a small dataset or a task very different from the original training task.\n",
    "- **Fine-tuning**: Further customizes the BERT model to your specific dataset and task. This usually leads to higher performance if you have sufficient data. Involves training most or all of the model's layers on the new task, allowing the model to adjust its weights to better fit the specific task.\n",
    "Adjusting the freezing/unfreezing of layers is a crucial aspect of adapting pre-trained models to new tasks and is often fine-tuned based on experimental results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
