{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning Hyperparameter Tuning\n",
    "\n",
    "Deep Learning models are highly sensitive to their hyperparameters, which significantly influence the model's performance. Hyperparameter tuning is the process of selecting the set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is set before the learning process begins.\n",
    "\n",
    "Hyperparameters can broadly be classified into two categories:\n",
    "\n",
    "- **Model hyperparameters** which influence model selection such as the number and width of hidden layers in a neural network.\n",
    "- **Algorithm hyperparameters** which influence the speed and quality of the learning algorithm such as learning rate or batch size.\n",
    "\n",
    "The goal of hyperparameter tuning is to find the combination of hyperparameters that yields the best model performance, often measured through validation data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning is critical in deep learning for several reasons:\n",
    "\n",
    "1. **Improves Model Performance:** Proper tuning can lead to significant improvements in model accuracy.\n",
    "2. **Controls Overfitting:** By tuning regularization parameters, we can control model complexity and mitigate overfitting.\n",
    "3. **Efficiency:** Optimal hyperparameters can make training faster and more efficient, saving time and computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Hyperparameters\n",
    "\n",
    "- **Learning Rate**: Controls the speed at which the model's weights are adjusted during training.\n",
    "- **Batch Size**: The number of samples processed before the model's weights are updated.\n",
    "- **Number of Epochs**: The number of times the model iterates through the entire training dataset.\n",
    "- **Optimizer**: The algorithm used to update the model's weights (e.g., Stochastic Gradient Descent, Adam).\n",
    "- **Network Architecture**: Number of layers, neurons per layer, activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques for Hyperparameter Tuning\n",
    "\n",
    "Several techniques exist for hyperparameter tuning, each with its own advantages and trade-offs:\n",
    "\n",
    "1. **Grid Search:** Exhaustively tries every combination of hyperparameters specified in a grid.\n",
    "2. **Random Search:** Randomly selects combinations of hyperparameters to try.\n",
    "3. **Bayesian Optimization:** Uses a probabilistic model to guide the search for the best hyperparameters.\n",
    "4. **Gradient-based Optimization:** Adjusts hyperparameters using gradient information when available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Example with PyTorch\n",
    "\n",
    "This example demonstrates how to perform hyperparameter tuning for a simple feedforward neural network (FNN) on the MNIST dataset. We use Pytorch for model construction and the tuning process. The goal is to find the best combination of hyperparameters that yields the highest accuracy on the validation set.\n",
    "\n",
    "## Hyperparameters Tuned\n",
    "\n",
    "- **Number of Layers (`num_layers`):** Determines the depth of the neural network. Deep networks can model complex patterns, but are also more computationally expensive and prone to overfitting.\n",
    "- **Number of Neurons in Each Layer (`hidden_size`):** Controls the width of the layers. More neurons can capture more information but also make the network more complex and prone to overfitting.\n",
    "- **Learning Rate (`learning_rate`):** Affects how quickly or slowly a neural network updates its parameters. The right learning rate can make training faster and more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing the Data\n",
    "The MNIST dataset is a collection of handwritten digits, which we'll use for a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "val_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "input_size = 28 * 28\n",
    "output_size = 10\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model\n",
    "We define a class that constructs a neural network model. This will be used to create different models with various hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(FNN, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_sizes)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_size, hidden_sizes[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(hidden_sizes[-1], output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = self.layers(x)\n",
    "        x = self.output(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Model\n",
    "First we define the grid of hyperparameters to be tuned, including learning rate, number of layers, and number of neurons per layer. We use ParameterGrid to iterate over the grid of hyperparameters combinations. For each combination, it creates an instance of the FNN model with the specified hyperparameters, trains the model on the training dataset, evaluates its performance on the validation dataset, and records the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Validation Loss: 0.0026, Validation Accuracy: 0.9502\n",
      "Epoch 2/5, Validation Loss: 0.0020, Validation Accuracy: 0.9610\n",
      "Epoch 3/5, Validation Loss: 0.0016, Validation Accuracy: 0.9666\n",
      "Epoch 4/5, Validation Loss: 0.0015, Validation Accuracy: 0.9697\n",
      "Epoch 5/5, Validation Loss: 0.0014, Validation Accuracy: 0.9732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:17<02:01, 17.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Validation Loss: 0.0023, Validation Accuracy: 0.9532\n",
      "Epoch 2/5, Validation Loss: 0.0021, Validation Accuracy: 0.9609\n",
      "Epoch 3/5, Validation Loss: 0.0018, Validation Accuracy: 0.9637\n",
      "Epoch 4/5, Validation Loss: 0.0016, Validation Accuracy: 0.9680\n",
      "Epoch 5/5, Validation Loss: 0.0016, Validation Accuracy: 0.9708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 2/8 [00:35<01:45, 17.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Validation Loss: 0.0027, Validation Accuracy: 0.9487\n",
      "Epoch 2/5, Validation Loss: 0.0036, Validation Accuracy: 0.9457\n",
      "Epoch 3/5, Validation Loss: 0.0029, Validation Accuracy: 0.9521\n",
      "Epoch 4/5, Validation Loss: 0.0035, Validation Accuracy: 0.9488\n",
      "Epoch 5/5, Validation Loss: 0.0025, Validation Accuracy: 0.9609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3/8 [00:54<01:31, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Validation Loss: 0.0035, Validation Accuracy: 0.9399\n",
      "Epoch 2/5, Validation Loss: 0.0034, Validation Accuracy: 0.9373\n",
      "Epoch 3/5, Validation Loss: 0.0033, Validation Accuracy: 0.9445\n",
      "Epoch 4/5, Validation Loss: 0.0028, Validation Accuracy: 0.9547\n",
      "Epoch 5/5, Validation Loss: 0.0025, Validation Accuracy: 0.9608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [01:12<01:13, 18.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Validation Loss: 0.0021, Validation Accuracy: 0.9576\n",
      "Epoch 2/5, Validation Loss: 0.0016, Validation Accuracy: 0.9684\n",
      "Epoch 3/5, Validation Loss: 0.0014, Validation Accuracy: 0.9716\n",
      "Epoch 4/5, Validation Loss: 0.0014, Validation Accuracy: 0.9732\n",
      "Epoch 5/5, Validation Loss: 0.0012, Validation Accuracy: 0.9781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 5/8 [01:31<00:55, 18.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Validation Loss: 0.0019, Validation Accuracy: 0.9613\n",
      "Epoch 2/5, Validation Loss: 0.0015, Validation Accuracy: 0.9714\n",
      "Epoch 3/5, Validation Loss: 0.0014, Validation Accuracy: 0.9727\n",
      "Epoch 4/5, Validation Loss: 0.0014, Validation Accuracy: 0.9726\n",
      "Epoch 5/5, Validation Loss: 0.0012, Validation Accuracy: 0.9759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 6/8 [01:50<00:37, 18.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Validation Loss: 0.0037, Validation Accuracy: 0.9346\n",
      "Epoch 2/5, Validation Loss: 0.0028, Validation Accuracy: 0.9522\n",
      "Epoch 3/5, Validation Loss: 0.0031, Validation Accuracy: 0.9506\n",
      "Epoch 4/5, Validation Loss: 0.0030, Validation Accuracy: 0.9486\n",
      "Epoch 5/5, Validation Loss: 0.0031, Validation Accuracy: 0.9556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 7/8 [02:08<00:18, 18.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Validation Loss: 0.0037, Validation Accuracy: 0.9440\n",
      "Epoch 2/5, Validation Loss: 0.0034, Validation Accuracy: 0.9490\n",
      "Epoch 3/5, Validation Loss: 0.0029, Validation Accuracy: 0.9521\n",
      "Epoch 4/5, Validation Loss: 0.0030, Validation Accuracy: 0.9522\n",
      "Epoch 5/5, Validation Loss: 0.0027, Validation Accuracy: 0.9598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [02:28<00:00, 18.58s/it]\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0.0\n",
    "best_hyperparams = None\n",
    "\n",
    "# Define hyperparameters and their search space\n",
    "params_grid = {\n",
    "    'lr': [0.001, 0.01,],\n",
    "    'num_layers': [2, 3],\n",
    "    'hidden_size': [64, 128]\n",
    "}\n",
    "\n",
    "# Define a function to train and evaluate the model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in val_loader:\n",
    "                output = model(data)\n",
    "                val_loss += criterion(output, target).item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        accuracy = correct / len(val_loader.dataset)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Hyperparameter tuning\n",
    "for params in tqdm(list(ParameterGrid(params_grid))):\n",
    "    model = FNN(input_size, [params['hidden_size']] * params['num_layers'], output_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    \n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, epochs=5)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    accuracy = correct / len(val_loader.dataset)\n",
    "    \n",
    "    # Update best hyperparameters if better accuracy is achieved\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_hyperparams = params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing the Results\n",
    " After evaluating all combinations, select the combination of hyperparameters that achieved the highest accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:\n",
      "{'hidden_size': 128, 'lr': 0.001, 'num_layers': 3}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters:\")\n",
    "print(best_hyperparams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results represents the optimal hyperparameters found through the hyperparameter optimization process using PyTorch for a neural network model. Each key-value pair in the dictionary has a specific meaning regarding the model's configuration:\n",
    "\n",
    "- **`'num_layers': 3`**: Indicates that the optimal model configuration includes 1 dense layer (excluding the input and output layers). Despite the presence of parameters for up to 5 layers, only the first layer's configuration is applied, based on this optimal number of layers.\n",
    "\n",
    "- **`'hidden_size': 128`**: Specifies that the first and only dense layer in the optimal model configuration should have 448 neurons. This suggests that having a large number of neurons in this layer is beneficial for the model's performance on the task.\n",
    "\n",
    "- **`'lr': 0.001`**: The learning rate for the Adam optimizer in the optimal model configuration. A learning rate around this value is typical and indicates that moderate step sizes during training are optimal for this specific task.\n",
    "\n",
    "In summary, the optimal model configuration includes 3 dense layer with 128 units and utilizes the Adam optimizer with a learning rate of 0.001. The specified units for additional layers beyond the first are not applicable to the best model configuration. Hyperparameter tuning requires careful experimentation and validation to ensure that the selected configuration generalizes well to unseen data and achieves optimal performance for the task at hand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
