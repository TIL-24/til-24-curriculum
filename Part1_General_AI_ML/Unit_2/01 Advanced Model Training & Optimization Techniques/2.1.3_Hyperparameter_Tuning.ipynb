{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning Hyperparameter Tuning\n",
    "\n",
    "Deep Learning models are highly sensitive to their hyperparameters, which significantly influence the model's performance. Hyperparameter tuning is the process of selecting the set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is set before the learning process begins.\n",
    "\n",
    "Hyperparameters can broadly be classified into two categories:\n",
    "\n",
    "- **Model hyperparameters** which influence model selection such as the number and width of hidden layers in a neural network.\n",
    "- **Algorithm hyperparameters** which influence the speed and quality of the learning algorithm such as learning rate or batch size.\n",
    "\n",
    "The goal of hyperparameter tuning is to find the combination of hyperparameters that yields the best model performance, often measured through validation data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning is critical in deep learning for several reasons:\n",
    "\n",
    "1. **Improves Model Performance:** Proper tuning can lead to significant improvements in model accuracy.\n",
    "2. **Controls Overfitting:** By tuning regularization parameters, we can control model complexity and mitigate overfitting.\n",
    "3. **Efficiency:** Optimal hyperparameters can make training faster and more efficient, saving time and computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Hyperparameters\n",
    "\n",
    "- **Learning Rate**: Controls the speed at which the model's weights are adjusted during training.\n",
    "- **Batch Size**: The number of samples processed before the model's weights are updated.\n",
    "- **Number of Epochs**: The number of times the model iterates through the entire training dataset.\n",
    "- **Optimizer**: The algorithm used to update the model's weights (e.g., Stochastic Gradient Descent, Adam).\n",
    "- **Network Architecture**: Number of layers, neurons per layer, activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques for Hyperparameter Tuning\n",
    "\n",
    "Several techniques exist for hyperparameter tuning, each with its own advantages and trade-offs:\n",
    "\n",
    "1. **Grid Search:** Exhaustively tries every combination of hyperparameters specified in a grid.\n",
    "2. **Random Search:** Randomly selects combinations of hyperparameters to try.\n",
    "3. **Bayesian Optimization:** Uses a probabilistic model to guide the search for the best hyperparameters.\n",
    "4. **Gradient-based Optimization:** Adjusts hyperparameters using gradient information when available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning Example with Keras and Keras Tuner\n",
    "\n",
    "This example demonstrates how to perform hyperparameter tuning for a simple feedforward neural network (FNN) on the MNIST dataset. We use Keras for model construction and Keras Tuner for the tuning process. The goal is to find the best combination of hyperparameters that yields the highest accuracy on the validation set.\n",
    "\n",
    "## Hyperparameters Tuned\n",
    "\n",
    "- **Number of Layers (`num_layers`):** Determines the depth of the neural network. Deep networks can model complex patterns, but are also more computationally expensive and prone to overfitting.\n",
    "- **Number of Neurons in Each Layer (`units`):** Controls the width of the layers. More neurons can capture more information but also make the network more complex and prone to overfitting.\n",
    "- **Learning Rate (`learning_rate`):** Affects how quickly or slowly a neural network updates its parameters. The right learning rate can make training faster and more stable.\n",
    "\n",
    "## Required Libraries\n",
    "\n",
    "First, ensure you have Keras and Keras Tuner installed. You can install Keras Tuner via pip:\n",
    "\n",
    "```bash\n",
    "pip install keras-tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preprocessing the Data\n",
    "The MNIST dataset is a collection of handwritten digits, which we'll use for a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize and flatten the data\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model\n",
    "We define a function that constructs a neural network model. This function will be used by Keras Tuner to create different models with various hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Input(shape=(784,)))\n",
    "    for i in range(hp.Int('num_layers', 1, 5)):\n",
    "        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n",
    "                                            min_value=32,\n",
    "                                            max_value=512,\n",
    "                                            step=32),\n",
    "                               activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    # Tuning the optimizer\n",
    "    optimizer_choice = hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop'])\n",
    "\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='log'))\n",
    "    elif optimizer_choice == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='log'))\n",
    "    else: # 'rmsprop'\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, sampling='log'))\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the Model\n",
    "We use Keras Tuner's RandomSearch class to search the hyperparameter space. This method randomly selects combinations of hyperparameters to construct and evaluate different models.\n",
    "The tuner searches for the best hyperparameter set by training the model on the training data (x_train, y_train), with a specified number of epochs (10) and validation split (0.2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir/mnist_keras_tuner/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.RandomSearch(build_model,\n",
    "                        objective='val_accuracy',\n",
    "                        max_trials=10, # The number of different hyperparameter combinations to try\n",
    "                        executions_per_trial=1, # The number of models that should be built and fit for each trial\n",
    "                        directory='my_dir',\n",
    "                        project_name='mnist_keras_tuner')\n",
    "\n",
    "tuner.search(x_train, y_train, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviewing the Results\n",
    "After the search is complete, we can review the best models and their performances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 529us/step - loss: 0.0826 - accuracy: 0.9790\n",
      "Best Loss: 0.08258586376905441, Best Accuracy: 0.9789999723434448\n"
     ]
    }
   ],
   "source": [
    "# Get the top 3 models.\n",
    "models = tuner.get_best_models(num_models=3)\n",
    "\n",
    "# Get the hyperparameters of the best model.\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Evaluate the best model.\n",
    "best_model = models[0]\n",
    "loss, accuracy = best_model.evaluate(x_test, y_test)\n",
    "print(f\"Best Loss: {loss}, Best Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_layers': 1, 'units_0': 448, 'learning_rate': 0.0011107930392350116, 'units_1': 32, 'units_2': 32, 'units_3': 384, 'units_4': 192, 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hps.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results represents the optimal hyperparameters found through the hyperparameter optimization process using Keras Tuner for a neural network model. Each key-value pair in the dictionary has a specific meaning regarding the model's configuration:\n",
    "\n",
    "- **`'num_layers': 1`**: Indicates that the optimal model configuration includes 1 dense layer (excluding the input and output layers). Despite the presence of parameters for up to 5 layers, only the first layer's configuration is applied, based on this optimal number of layers.\n",
    "\n",
    "- **`'units_0': 448`**: Specifies that the first and only dense layer in the optimal model configuration should have 448 neurons. This suggests that having a large number of neurons in this layer is beneficial for the model's performance on the task.\n",
    "\n",
    "- **`'learning_rate': 0.0011107930392350116`**: The learning rate for the Adam optimizer in the optimal model configuration. A learning rate around this value is typical and indicates that moderate step sizes during training are optimal for this specific task.\n",
    "\n",
    "- **`'units_1': 32, 'units_2': 32, 'units_3': 384, 'units_4': 192`**: These values represent the number of units for additional dense layers that were considered during the optimization search but are not used in the final model configuration due to the optimal number of dense layers being 1. These values are artifacts of the search process.\n",
    "\n",
    "- **`'optimizer': 'adam'`**: The Adam optimizer was determined to be the most effective for this task out of the options tested (adam, sgd, rmsprop), indicating its suitability for the model given the task and data.\n",
    "\n",
    "In summary, the optimal model configuration includes one dense layer with 448 units and utilizes the Adam optimizer with a learning rate of approximately 0.0011. The specified units for additional layers beyond the first are not applicable to the best model configuration.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
