{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Optimizations in Deep Learning\n",
    "\n",
    "Optimization algorithms play a crucial role in deep learning, helping models learn from data by minimizing or maximizing an objective function. Advanced optimizers like Adam and RMSProp incorporate mechanisms that consider not just the first derivatives of the loss function but also the second moments of the gradients or adaptive learning rates, thereby enhancing the performance and convergence speed of deep neural networks.\n",
    "\n",
    "https://www.ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/deep_learning_optimization.jpeg\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is the fundamental optimizer in Deep Learning. It iteratively updates the weights of a neural network based on the negative gradient of the loss function. This essentially pushes the weights in a direction that minimizes the loss. However, SGD has limitations:\n",
    "\n",
    "* **Slow Convergence:** SGD can be slow to converge, especially in landscapes with many shallow valleys (local minima). Imagine a ball rolling down a bumpy hill. It might get stuck in a small valley instead of reaching the global minimum at the bottom.\n",
    "\n",
    "* **Sensitivity to Learning Rate:** The learning rate hyperparameter controls the step size taken during each update. Too high a learning rate can cause the weights to overshoot the minimum, bouncing around like a ball with too much momentum. Conversely, a very small learning rate makes the journey to the minimum painstakingly slow. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp (Root Mean Squared Prop)\n",
    "\n",
    "RMSProp addresses SGD's sensitivity to learning rates by introducing an **exponentially decaying average of squared gradients**. This average is used to scale the learning rate for each parameter, leading to **adaptive learning behavior**.\n",
    "\n",
    "* **Analogy:** Imagine a ball rolling down a hill, but the ground is constantly changing its friction. RMSProp adjusts the ball's speed based on the recent steepness it encountered. \n",
    "\n",
    "* **Benefits:**\n",
    "    * **Faster Convergence:** Compared to SGD's constant learning rate, RMSProp's adaptation often leads to faster convergence.\n",
    "    * **Handles Non-stationary Problems:** RMSProp can navigate the changing landscape more effectively when working with noisy or non-stationary data.\n",
    "\n",
    "\n",
    "<img src=\"./imgs/contour.webp\" alt=\"drawing\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam (Adaptive Moment Estimation)\n",
    "\n",
    "Adam is an optimizer that combines the strengths of SGD, Momentum, and RMSProp. It maintains an exponentially decaying average of gradients (similar to RMSProp) and another average of squared gradients with a bias correction. \n",
    "\n",
    "* **Analogy:** Imagine a ball with built-in momentum rolling on a complex surface. Adam considers both the immediate slope and the historical fluctuations to update the learning rate, leading to efficient navigation.\n",
    "\n",
    "* **Benefits:**\n",
    "    * **Often the Best Performer:** Adam is frequently considered the default choice due to its effectiveness across various problems. \n",
    "    * **Minimal Hyperparameter Tuning:** Adam generally requires less hyperparameter tuning compared to SGD or RMSprop.\n",
    "    * **Works Well for Sparse Gradients:** Adam addresses the issue of weights that have very small or zero gradients in certain neural networks. \n",
    "\n",
    "\n",
    "![Adam Optimization](imgs/gradient_descent.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Optimizers\n",
    "\n",
    "To compare the performance of SGD, RMSProp, and Adam optimizers on the MNIST dataset, we'll perform the following steps:\n",
    "\n",
    "1. Load the MNIST dataset.\n",
    "2. Preprocess the data.\n",
    "3. Define a simple neural network model for digit classification.\n",
    "4. Train the model using each of the three optimizers: SGD, RMSProp, and Adam.\n",
    "5. Evaluate and compare the performance of the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transformation to normalize the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # scales to [0, 1]\n",
    "])\n",
    "# Load the MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.dense_layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Train Models with Different Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and compile the model with different optimizers\n",
    "def create_model(optimizer_name):\n",
    "    model = NeuralNet()\n",
    "    if optimizer_name == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
    "    elif optimizer_name == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    return model, optimizer\n",
    "\n",
    "# Function to train and evaluate the model\n",
    "def train_and_evaluate(model, optimizer, train_loader, test_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() * data.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with SGD optimizer...\n",
      "SGD - Test Loss: 0.2395, Test Accuracy: 93.2400, Training Time: 11.22 seconds\n",
      "\n",
      "Training with RMSPROP optimizer...\n",
      "RMSPROP - Test Loss: 0.2502, Test Accuracy: 94.9900, Training Time: 14.49 seconds\n",
      "\n",
      "Training with ADAM optimizer...\n",
      "ADAM - Test Loss: 0.2148, Test Accuracy: 95.2500, Training Time: 14.74 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Main experiment loop\n",
    "optimizers = ['sgd', 'rmsprop', 'adam']\n",
    "results = {}\n",
    "\n",
    "for opt in optimizers:\n",
    "    print(f\"Training with {opt.upper()} optimizer...\")\n",
    "    start_time = time.time()\n",
    "    model, optimizer = create_model(opt)\n",
    "    test_loss, test_acc = train_and_evaluate(model, optimizer, train_loader, test_loader)\n",
    "    duration = time.time() - start_time\n",
    "    results[opt] = (test_loss, test_acc, duration)\n",
    "    print(f\"{opt.upper()} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}, Training Time: {duration:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing SGD, RMSProp, and Adam on the MNIST dataset illustrates the impact of advanced optimization techniques in deep learning training. While SGD serves as a solid baseline, optimizers that adjust learning rates adaptively, such as RMSProp and Adam, can significantly enhance model performance. Adam, in particular, stands out for achieving the best balance between high accuracy, low loss, and reasonable training time, making it an excellent choice for many deep learning tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Optimizer\n",
    "\n",
    "* **There's No Silver Bullet:** The optimal optimizer choice depends on the specific problem, dataset, and model architecture. Experimentation is key!\n",
    "* **Start with Adam:** Due to its robustness and adaptability, Adam is an excellent starting point for most deep learning tasks.\n",
    "* **Consider RMSProp for Noisy Data:** If your data exhibits significant noise or non-stationarity, RMSProp might be a better choice than Adam. \n",
    "* **Fine-Tuning with SGD:**  In some cases, switching to SGD with a carefully selected learning rate can help refine a model that's already been pre-trained with Adam or RMSprop. \n",
    "\n",
    "**Remember:** The best way to determine the most suitable optimizer for your deep learning project is through experimentation! \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
