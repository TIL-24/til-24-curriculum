{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Regularization Techniques\n",
    "\n",
    "Regularisation techniques are essential in deep learning to prevent overfitting, ensuring that models generalize well to new, unseen data. Overfitting occurs when a model learns the training data too well, capturing noise in the training data as if it were a true pattern. This notebook explores two popular regularisation techniques: Dropout and Batch Normalisation.\n",
    "\n",
    "This notebook explores two common regularization techniques used in Deep Learning: Dropout and Batch Normalization. These techniques help address the problem of overfitting, which can significantly impact the performance of deep neural networks.\n",
    "\n",
    "<img src=\"./imgs/overfit_vs_underfit.webp\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dropout\n",
    "\n",
    "Dropout is a straightforward yet effective regularization technique. By randomly \"dropping out\" a proportion of neurons in the network during training, it prevents the network from becoming too dependent on any single neuron. This randomness encourages the network to develop more robust features that are not reliant on specific paths, enhancing generalization to new data.\n",
    "\n",
    "**Concept:**\n",
    "\n",
    "* During training, a random subset of neurons in a layer is temporarily ignored (dropped out) with a predefined probability (e.g., 0.5).\n",
    "* This forces the remaining neurons to learn independently and become more robust to the absence of their neighbors.\n",
    "* At test time, all neurons are included, but their activations are scaled by the dropout rate (e.g., multiplied by 0.5) to account for the neurons that were dropped during training.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "* Reduces overfitting by preventing co-adaptation of features.\n",
    "* Improves generalization performance on unseen data.\n",
    "* Encourages robustness by making the network less reliant on specific neurons.\n",
    "\n",
    "<img src=\"./imgs/dropout.gif\" alt=\"drawing\" width=\"500\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9228879809379578\n",
      "Epoch 2, Loss: 0.3568008244037628\n",
      "Epoch 3, Loss: 1.2302535772323608\n",
      "Epoch 4, Loss: 1.3585325479507446\n",
      "Epoch 5, Loss: 0.5889886021614075\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Generate some dummy data\n",
    "X = torch.randn(100, 10)\n",
    "y = torch.randn(100, 1)\n",
    "\n",
    "# Create a simple model with Dropout\n",
    "class ModelWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelWithDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = ModelWithDropout()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Data loader\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we train a simple neural network model with Dropout on dummy data. The model consists of two fully connected layers. Dropout is applied after the first hidden layer's activation function.\n",
    "\n",
    "- **Model Architecture**:\n",
    "  - A fully connected layer (`fc1`) that maps input features to 50 hidden nodes.\n",
    "  - A ReLU activation function to introduce non-linearity.\n",
    "  - A Dropout layer with a dropout rate of 0.5, meaning half of the units are randomly dropped during training.\n",
    "  - A second fully connected layer (`fc2`) that produces the final output.\n",
    "\n",
    "- **Training**:\n",
    "  - We use Mean Squared Error (MSE) as the loss function.\n",
    "  - The Adam optimizer is used with a learning rate of 0.01.\n",
    "  - The model is trained for 5 epochs, and the loss is printed after each epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch Normalization\n",
    "\n",
    "Batch Normalisation is another powerful technique that normalizes the inputs of each layer to have a mean of 0 and a standard deviation of 1. This normalization helps to stabilize and accelerate the training process, combating issues related to poor initialization and helping gradients flow more smoothly through the network.\n",
    "\n",
    "**Concept:**\n",
    "\n",
    "* During training, for each mini-batch, Batch Normalization subtracts the mean and divides by the standard deviation of the activations of each layer.\n",
    "* This normalizes the activations to a zero mean and unit variance.\n",
    "* The layer then applies learned scale and shift factors to recover the original activation distribution if desired.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "* Stabilizes the training process by making the activations less sensitive to initialization and weight updates.\n",
    "* Improves gradient flow, allowing for faster training and potentially higher accuracy.\n",
    "* Reduces the need for heavy weight initialization schemes.\n",
    "\n",
    "<html>\n",
    "<body>\n",
    "\n",
    "<p>\n",
    "  <img src=\"./imgs/batch_norm.webp\" alt=\"drawing\" width=\"700\"/>\n",
    "  <img src=\"./imgs/batchnorm.webp\" alt=\"drawing\" width=\"500\"/>\n",
    "</p>\n",
    "\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8452563285827637\n",
      "Epoch 2, Loss: 1.280917763710022\n",
      "Epoch 3, Loss: 0.3340492844581604\n",
      "Epoch 4, Loss: 0.4591518044471741\n",
      "Epoch 5, Loss: 0.14126205444335938\n"
     ]
    }
   ],
   "source": [
    "class ModelWithBatchNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelWithBatchNorm, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = ModelWithBatchNorm()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates a model using Batch Normalization:\n",
    "\n",
    "- **Model Architecture**:\n",
    "  - The first fully connected layer (`fc1`) has 50 output features.\n",
    "  - A Batch Normalization layer (`bn1`) normalizes the output from the first layer.\n",
    "  - A ReLU activation function is used for non-linearity.\n",
    "  - The second fully connected layer (`fc2`) outputs the final result.\n",
    "\n",
    "- **Training**:\n",
    "  - The model uses Mean Squared Error (MSE) for loss calculation.\n",
    "  - It is trained using the Adam optimizer with a learning rate of 0.01.\n",
    "  - Training is carried out for 5 epochs, with loss printed after each epoch.\n",
    "\n",
    "Batch Normalization helps in normalizing the inputs to layers within the network which can speed up training and improve the overall performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Choose Between Dropout and Batch Normalization\n",
    "\n",
    "Choosing the right regularization technique is crucial for the success of your deep learning model. While Dropout and Batch Normalization can both improve model generalization, they do so in different ways and have unique considerations. This section will guide you through choosing the most appropriate regularization technique for your specific scenario.\n",
    "\n",
    "\n",
    "### Considerations for Dropout\n",
    "\n",
    "Dropout randomly deactivates a subset of neurons in the network during training, which helps prevent overfitting by ensuring that no single neuron can overly influence the output. It is particularly effective in large networks where overfitting is a significant concern. However, Dropout might not be as beneficial in models that are already small or in cases where every neuron is crucial for the task.\n",
    "\n",
    "\n",
    "#### When to Use Dropout\n",
    "\n",
    "- In deep neural networks prone to overfitting.\n",
    "- In layers with a large number of neurons.\n",
    "- As a complementary technique to other forms of regularization.\n",
    "\n",
    "### Considerations for Batch Normalization\n",
    "\n",
    "Batch Normalization standardizes the inputs to a layer for each mini-batch, stabilizing the learning process and reducing the number of epochs required to train deep networks. It is especially useful when training deep networks with complex architectures. Unlike Dropout, Batch Normalization can sometimes lead to improved performance even in smaller networks.\n",
    "\n",
    "#### When to Use Batch Normalization\n",
    "\n",
    "- To improve training stability and speed.\n",
    "- In very deep networks where vanishing or exploding gradients are a concern.\n",
    "- Before activation functions, to normalize inputs.\n",
    "\n",
    "### Combining Dropout and Batch Normalization\n",
    "\n",
    "In practice, Dropout and Batch Normalization can be combined to leverage the strengths of both techniques. However, the layer order and configuration play a crucial role in how effective the combination is. A common approach is to apply Batch Normalization before activation functions and Dropout after activation functions or in specific layers where overfitting is more likely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7239774465560913\n",
      "Epoch 2, Loss: 1.054260492324829\n",
      "Epoch 3, Loss: 0.3056546747684479\n",
      "Epoch 4, Loss: 0.9020550847053528\n",
      "Epoch 5, Loss: 0.3008429706096649\n"
     ]
    }
   ],
   "source": [
    "# Define a model with both Batch Normalization and Dropout\n",
    "class ModelWithBoth(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelWithBoth, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.bn1 = nn.BatchNorm1d(50)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(50, 20)\n",
    "        self.bn2 = nn.BatchNorm1d(20)\n",
    "        self.fc3 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = ModelWithBoth()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Data loader\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(5):\n",
    "    for inputs, targets in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Tips for Regularization\n",
    "\n",
    "Implementing regularization techniques effectively requires understanding not just when but also how to use them. Here are some practical tips:\n",
    "\n",
    "- Start with a small amount of Dropout (e.g., 0.2 to 0.5) and adjust based on validation performance.\n",
    "- Use Batch Normalization liberally in deep networks to stabilize training, but be mindful of its impact on inference time.\n",
    "- Experiment with combining both techniques, monitoring model performance and training stability.\n",
    "- Remember, regularization is just one part of model development. Model architecture, data preprocessing, and training procedure also play critical roles in building a robust model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
