{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Regularization Techniques\n",
    "\n",
    "Regularisation techniques are essential in deep learning to prevent overfitting, ensuring that models generalize well to new, unseen data. Overfitting occurs when a model learns the training data too well, capturing noise in the training data as if it were a true pattern. This notebook explores two popular regularisation techniques: Dropout and Batch Normalisation.\n",
    "\n",
    "This notebook explores two common regularization techniques used in Deep Learning: Dropout and Batch Normalization. These techniques help address the problem of overfitting, which can significantly impact the performance of deep neural networks.\n",
    "\n",
    "<img src=\"./imgs/overfit_vs_underfit.webp\" alt=\"drawing\" width=\"725\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dropout\n",
    "\n",
    "Dropout is a straightforward yet effective regularization technique. By randomly \"dropping out\" a proportion of neurons in the network during training, it prevents the network from becoming too dependent on any single neuron. This randomness encourages the network to develop more robust features that are not reliant on specific paths, enhancing generalization to new data.\n",
    "\n",
    "**Concept:**\n",
    "\n",
    "* During training, a random subset of neurons in a layer is temporarily ignored (dropped out) with a predefined probability (e.g., 0.5).\n",
    "* This forces the remaining neurons to learn independently and become more robust to the absence of their neighbors.\n",
    "* At test time, all neurons are included, but their activations are scaled by the dropout rate (e.g., multiplied by 0.5) to account for the neurons that were dropped during training.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "* Reduces overfitting by preventing co-adaptation of features.\n",
    "* Improves generalization performance on unseen data.\n",
    "* Encourages robustness by making the network less reliant on specific neurons.\n",
    "\n",
    "![dropout](imgs/dropout.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.5463 - accuracy: 0.1287 - val_loss: 2.3784 - val_accuracy: 0.0950\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.4082 - accuracy: 0.1412 - val_loss: 2.3579 - val_accuracy: 0.1050\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.4279 - accuracy: 0.1125 - val_loss: 2.3489 - val_accuracy: 0.0900\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.4053 - accuracy: 0.1063 - val_loss: 2.3361 - val_accuracy: 0.0900\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.3515 - accuracy: 0.1213 - val_loss: 2.3318 - val_accuracy: 0.1000\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.3500 - accuracy: 0.1287 - val_loss: 2.3274 - val_accuracy: 0.1000\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.3399 - accuracy: 0.1262 - val_loss: 2.3282 - val_accuracy: 0.0900\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.3177 - accuracy: 0.1425 - val_loss: 2.3271 - val_accuracy: 0.1050\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.3104 - accuracy: 0.1437 - val_loss: 2.3263 - val_accuracy: 0.1000\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 2.3146 - accuracy: 0.1262 - val_loss: 2.3245 - val_accuracy: 0.1050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x174746510>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Sample data (adjust for your actual dataset)\n",
    "x_train = tf.random.normal((1000, 20)) \n",
    "y_train = tf.random.uniform((1000,), maxval=10, dtype=tf.int32) \n",
    "\n",
    "# Model with Dropout\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(20,)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batch Normalization\n",
    "\n",
    "Batch Normalisation is another powerful technique that normalizes the inputs of each layer to have a mean of 0 and a standard deviation of 1. This normalization helps to stabilize and accelerate the training process, combating issues related to poor initialization and helping gradients flow more smoothly through the network.\n",
    "\n",
    "**Concept:**\n",
    "\n",
    "* During training, for each mini-batch, Batch Normalization subtracts the mean and divides by the standard deviation of the activations of each layer.\n",
    "* This normalizes the activations to a zero mean and unit variance.\n",
    "* The layer then applies learned scale and shift factors to recover the original activation distribution if desired.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "* Stabilizes the training process by making the activations less sensitive to initialization and weight updates.\n",
    "* Improves gradient flow, allowing for faster training and potentially higher accuracy.\n",
    "* Reduces the need for heavy weight initialization schemes.\n",
    "\n",
    "![batch_norm](imgs/batch_norm.webp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "25/25 [==============================] - 0s 4ms/step - loss: 2.8193 - accuracy: 0.1125 - val_loss: 2.3756 - val_accuracy: 0.1200\n",
      "Epoch 2/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.5058 - accuracy: 0.1363 - val_loss: 2.3756 - val_accuracy: 0.1050\n",
      "Epoch 3/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.3121 - accuracy: 0.1775 - val_loss: 2.3914 - val_accuracy: 0.1000\n",
      "Epoch 4/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.2082 - accuracy: 0.2050 - val_loss: 2.4111 - val_accuracy: 0.1100\n",
      "Epoch 5/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.1033 - accuracy: 0.2450 - val_loss: 2.4324 - val_accuracy: 0.1150\n",
      "Epoch 6/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.0668 - accuracy: 0.2512 - val_loss: 2.4600 - val_accuracy: 0.1100\n",
      "Epoch 7/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 2.0149 - accuracy: 0.2887 - val_loss: 2.4870 - val_accuracy: 0.1100\n",
      "Epoch 8/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 1.9375 - accuracy: 0.3200 - val_loss: 2.5243 - val_accuracy: 0.1050\n",
      "Epoch 9/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 1.8859 - accuracy: 0.3288 - val_loss: 2.5566 - val_accuracy: 0.0950\n",
      "Epoch 10/10\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 1.8404 - accuracy: 0.3500 - val_loss: 2.5773 - val_accuracy: 0.1050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1781bb610>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Model with Batch Normalization\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(20,)),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Choose Between Dropout and Batch Normalization\n",
    "\n",
    "Choosing the right regularization technique is crucial for the success of your deep learning model. While Dropout and Batch Normalization can both improve model generalization, they do so in different ways and have unique considerations. This section will guide you through choosing the most appropriate regularization technique for your specific scenario.\n",
    "\n",
    "\n",
    "### Considerations for Dropout\n",
    "\n",
    "Dropout randomly deactivates a subset of neurons in the network during training, which helps prevent overfitting by ensuring that no single neuron can overly influence the output. It is particularly effective in large networks where overfitting is a significant concern. However, Dropout might not be as beneficial in models that are already small or in cases where every neuron is crucial for the task.\n",
    "\n",
    "\n",
    "#### When to Use Dropout\n",
    "\n",
    "- In deep neural networks prone to overfitting.\n",
    "- In layers with a large number of neurons.\n",
    "- As a complementary technique to other forms of regularization.\n",
    "\n",
    "### Considerations for Batch Normalization\n",
    "\n",
    "Batch Normalization standardizes the inputs to a layer for each mini-batch, stabilizing the learning process and reducing the number of epochs required to train deep networks. It is especially useful when training deep networks with complex architectures. Unlike Dropout, Batch Normalization can sometimes lead to improved performance even in smaller networks.\n",
    "\n",
    "#### When to Use Batch Normalization\n",
    "\n",
    "- To improve training stability and speed.\n",
    "- In very deep networks where vanishing or exploding gradients are a concern.\n",
    "- Before activation functions, to normalize inputs.\n",
    "\n",
    "### Combining Dropout and Batch Normalization\n",
    "\n",
    "In practice, Dropout and Batch Normalization can be combined to leverage the strengths of both techniques. However, the layer order and configuration play a crucial role in how effective the combination is. A common approach is to apply Batch Normalization before activation functions and Dropout after activation functions or in specific layers where overfitting is more likely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of combining Batch Normalization and Dropout in a model layer\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Tips for Regularization\n",
    "\n",
    "Implementing regularization techniques effectively requires understanding not just when but also how to use them. Here are some practical tips:\n",
    "\n",
    "- Start with a small amount of Dropout (e.g., 0.2 to 0.5) and adjust based on validation performance.\n",
    "- Use Batch Normalization liberally in deep networks to stabilize training, but be mindful of its impact on inference time.\n",
    "- Experiment with combining both techniques, monitoring model performance and training stability.\n",
    "- Remember, regularization is just one part of model development. Model architecture, data preprocessing, and training procedure also play critical roles in building a robust model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
