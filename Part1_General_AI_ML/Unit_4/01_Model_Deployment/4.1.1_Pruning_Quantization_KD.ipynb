{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Model Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Model Efficiency Matters\n",
    "\n",
    "In the context of AI, efficiency is not just about speed; it's about making AI accessible and practical for everyday applications. The size and complexity of deep learning models have grown tremendously. While this leads to improved performance, it creates challenges for deployment, especially on mobile devices, embedded systems, or any setting where computational resources or power are limited. Model efficiency techniques aim to address these challenges without sacrificing accuracy.\n",
    "\n",
    "Growing model complexity poses challenges:\n",
    "- Storage: Large models consume more storage space.\n",
    "- Inference Speed: Complex models take longer to process inputs.\n",
    "- Energy Consumption: Computationally demanding models drain batteries quickly.\n",
    "- Deployment: Resource-constrained devices (smartphones, IoT) struggle with large models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Model Definition and Data Loading\n",
    "Before diving into each model effieciency technique, let's establish a common setup, especially for loading the CIFAR-10 dataset and defining a simple CNN model. This setup will be used across for the 1. Pruning and 2. Quantization examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 5 * 5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# CIFAR-10 data loading\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model size: 488494 bytes\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN()\n",
    "torch.save(model.state_dict(), \"model_before_pruning.pth\")\n",
    "original_size = os.path.getsize(\"model_before_pruning.pth\")\n",
    "\n",
    "print(f\"Original model size: {original_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pruning\n",
    "Pruning identifies and removes the less important weights of a neural network that have minimal impact on its output. This creates a sparser and more streamlined model. \n",
    "Unstructured pruning zero-out the least important weights, while structured pruning can be more aggressive by removing full sections of the network.\n",
    "\n",
    "Benefits:\n",
    "- Reduces model size\n",
    "- Can improve inference speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/pruning.webp\" alt=\"drawing\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch includes functionality for both structured and unstructured pruning. Below, we'll show an example of unstructured pruning, which removes individual weights in the model. This example demonstrates how to randomly prune 30% of the connections in the first linear layer of the network by setting their weights to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning: 488480 bytes\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Apply pruning to the model's first convolutional layer\n",
    "prune.l1_unstructured(model.conv1, name=\"weight\", amount=0.3)\n",
    "prune.remove(model.conv1, 'weight')  # Make pruning permanent\n",
    "\n",
    "torch.save(model.state_dict(), \"model_after_pruning.pth\")\n",
    "pruned_size = os.path.getsize(\"model_after_pruning.pth\")\n",
    "\n",
    "print(f\"Model size after pruning: {pruned_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve a more significant difference in model size through pruning, especially in a small model like `SimpleCNN`, we might need to apply more aggressive pruning or use structured pruning which removes entire channels or filters, not just individual weights. This is illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after pruning: 488382 bytes\n"
     ]
    }
   ],
   "source": [
    "# Apply structured pruning (removing entire channels) to the convolutional layers\n",
    "prune.ln_structured(model.conv1, name=\"weight\", amount=0.5, n=2, dim=0)\n",
    "prune.remove(model.conv1, 'weight')  # Make pruning permanent\n",
    "\n",
    "prune.ln_structured(model.conv2, name=\"weight\", amount=0.5, n=2, dim=0)\n",
    "prune.remove(model.conv2, 'weight')  # Make pruning permanent\n",
    "\n",
    "# Save the pruned model to disk\n",
    "torch.save(model.state_dict(), \"pruned_model.pth\")\n",
    "pruned_size = os.path.getsize(\"pruned_model.pth\")\n",
    "\n",
    "print(f\"Model size after pruning: {pruned_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tradeoffs\n",
    "- **Risk of Losing Important Information:** Aggressive pruning might remove weights that are important for the model’s accuracy, leading to a decrease in performance.\n",
    "- **Need for Retraining:** Often, a pruned model will require fine-tuning or full retraining to restore or improve its accuracy post-pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quantization\n",
    "Quantization simplifies the model's mathematical operations, converting those high-precision calculations into something more manageable and, crucially, faster. It  reduces the storage footprint of models by using less precise data types such as representing model weights and activations using lower-precision numbers (e.g., 8-bit integers instead of 32-bit floating-point)\n",
    "\n",
    "Benefits:\n",
    "- Reduces model size\n",
    "- Accelerates computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/quantization.jpeg\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of quantization on the same simple model as before. This code dynamically quantizes the linear layers of the model to int8 precision, which is particularly useful for reducing model size and speeding up inference for AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size after quantization: 169850 bytes\n"
     ]
    }
   ],
   "source": [
    "torch.backends.quantized.engine = 'qnnpack'  # For ARM architectures\n",
    "\n",
    "model.eval()  # Ensure the model is in evaluation mode for quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, {nn.Linear, nn.Conv2d}, dtype=torch.qint8)\n",
    "\n",
    "torch.save(quantized_model.state_dict(), \"model_after_quantization.pth\")\n",
    "quantized_size = os.path.getsize(\"model_after_quantization.pth\")\n",
    "\n",
    "print(f\"Model size after quantization: {quantized_size} bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tradeoffs:\n",
    "- **Potential Accuracy Loss:** Reducing precision can lead to a loss of model fidelity, particularly if not managed carefully with techniques like Quantization-Aware Training (QAT).\n",
    "- **Hardware Dependencies:** The benefits of quantization may depend on specific hardware capabilities, limiting its effectiveness across diverse deployment scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Knowledge Distillation\n",
    "Knowledge distillation involves training a smaller (student) model to replicate the behavior of a larger (teacher) model. The teacher model produces \"soft labels\" (probabilistic outputs). Student model is trained to match the soft labels, not just the original dataset's hard labels.\n",
    "\n",
    "Benefits:\n",
    "- Compresses knowledge into a smaller, more efficient model\n",
    "- Potential for higher accuracy than training the student directly on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/kd.jpeg\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simplified example of how to set this up in PyTorch. In this example, we use a pretrained ResNet18 model from torchvision as the teacher model, demonstrating knowledge distillation to a simpler student model on the CIFAR-10 dataset. The ResNet18 model is pretrained on ImageNet, so we'll adapt it to work with CIFAR-10.\n",
    "\n",
    "The teacher model is set to evaluation mode to ensure it does not update its weights during training.\n",
    "The loss function (CrossEntropyLoss) and optimizer (SGD with learning rate 0.001 and momentum 0.9) are defined for training the student model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Student model size: 1087436 bytes\n",
      "Teacher model size: 44804402 bytes\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainset = Subset(trainset, np.random.choice(len(trainset), 2000, replace=False))\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testset = Subset(testset, np.random.choice(len(testset), 500, replace=False))\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the Student Model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5, padding=2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5, padding=2)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "student_model = SimpleCNN()\n",
    "\n",
    "# Define the Teacher Model (Pretrained ResNet18)\n",
    "teacher_model = models.resnet18(pretrained=True)\n",
    "teacher_model.fc = nn.Linear(teacher_model.fc.in_features, 10)  # Adapt for CIFAR-10\n",
    "\n",
    "# Function to Train the Model (Without Distillation)\n",
    "def train_model(model, dataloader, epochs=1):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Function for Knowledge Distillation\n",
    "def distillation_loss(outputs, teacher_outputs, labels, T, alpha):\n",
    "    soft_loss = F.kl_div(F.log_softmax(outputs/T, dim=1), F.softmax(teacher_outputs/T, dim=1), reduction='batchmean') * (T * T * alpha)\n",
    "    hard_loss = F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "    return soft_loss + hard_loss\n",
    "\n",
    "def train_with_distillation(student_model, teacher_model, dataloader, T=5.0, alpha=0.5, epochs=1):\n",
    "    optimizer = optim.SGD(student_model.parameters(), lr=0.001, momentum=0.9)\n",
    "    teacher_model.eval()\n",
    "    student_model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in dataloader:\n",
    "            student_outputs = student_model(inputs)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(inputs)\n",
    "            loss = distillation_loss(student_outputs, teacher_outputs, labels, T, alpha)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Train and Evaluate Student Model Without Distillation\n",
    "train_model(student_model, trainloader)\n",
    "\n",
    "# Re-initialize the student model for distillation\n",
    "student_model = SimpleCNN()\n",
    "\n",
    "# Train and Evaluate Student Model With Distillation\n",
    "train_with_distillation(student_model, teacher_model, trainloader)\n",
    "\n",
    "# Save models and compare sizes\n",
    "torch.save(student_model.state_dict(), \"student_model.pth\")\n",
    "student_model_size = os.path.getsize(\"student_model.pth\")\n",
    "\n",
    "torch.save(teacher_model.state_dict(), \"teacher_model.pth\")\n",
    "teacher_model_size = os.path.getsize(\"teacher_model.pth\")\n",
    "\n",
    "print(f'Student model size: {student_model_size} bytes')\n",
    "print(f'Teacher model size: {teacher_model_size} bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tradeoffs:\n",
    "- **Dependence on Teacher Quality:** The success of distillation heavily relies on the quality and performance of the teacher model.\n",
    "- **Complex Training Process:** Distillation can add complexity to the training process, requiring careful tuning of parameters like temperature and loss balancing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Implementing these model efficiency techniques involves a balance between improving computational efficiency and managing potential impacts on model accuracy. Each technique has its place and benefits, but also drawbacks that need careful consideration. Depending on the specific requirements of your application—whether it's speed, size, cost, or accuracy—these techniques can be powerful tools in the machine learning practitioner's toolkit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
