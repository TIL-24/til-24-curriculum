{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running ML Model Containers and APIs for Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Building and running Machine Learning model containers is a powerful approach to deploying ML models, allowing for easy scaling, portability, and version control. This notebook will guide you through:\n",
    "- Building and running a Docker container for an ML model\n",
    "- Use FastAPI to make the ML model accessible through a REST API endpoint to get predictions by sending data to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Step 1: Setting Up Your Project\n",
    "First, create a new directory for the project and navigate into it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "```bash\n",
    "mkdir ml_model_container\n",
    "cd ml_model_container\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "(Note: `ml_model_container` can be found within this notebook's folder for easy reference.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Inside this directory, create three files:\n",
    "- `main.py` (for our FastAPI app) \n",
    "- `Dockerfile` (to define our Docker container)\n",
    "- `requirements.txt` file (to list our Python dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Step 2: Creating the ML Model\n",
    "\n",
    "For this example, we will use a simple linear regression model using scikit-learn. Our model will predict a value based on a single feature. This step is simplified for demonstration purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "In `main.py`, add:\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Simple model training\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "model = LinearRegression().fit(X, y)\n",
    "\n",
    "# Exposing an API for inference\n",
    "@app.get(\"/predict/\")\n",
    "def predict(value: float):\n",
    "    prediction = model.predict(np.array([[value]]))\n",
    "    return {\"prediction\": prediction[0]}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Step 3: Setting Up FastAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "The `FastAPI` setup is already included in the `main.py` code. FastAPI will serve our ML model's predictions through a simple API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Step 4: Creating the Dockerfile\n",
    "\n",
    "The Dockerfile defines the environment and instructions for running your application in a Docker container.\n",
    "\n",
    "In `Dockerfile`, add:\n",
    "```Docker\n",
    "# Use an official Python runtime as a parent image\n",
    "FROM python:3.8-slim\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /usr/src/app\n",
    "\n",
    "# Copy the current directory contents into the container at /usr/src/app\n",
    "COPY . .\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Make port 80 available to the world outside this container\n",
    "EXPOSE 80\n",
    "\n",
    "# Define environment variable\n",
    "ENV MODEL_NAME MyModel\n",
    "\n",
    "# Run main.py when the container launches\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Step 5: Defining Requirements\n",
    "In `requirements.txt`, list all the required packages:\n",
    "\n",
    "```tex\n",
    "fastapi\n",
    "uvicorn\n",
    "scikit-learn\n",
    "numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Step 6: Building and Running Your Container\n",
    "Navigate to your project directory in a terminal. Build your Docker container with:\n",
    "```bash\n",
    "docker build -t ml_model_container .\n",
    "```\n",
    "<img src=\"./imgs/docker_build.png\" alt=\"drawing\" width=\"8000\"/>\n",
    "\n",
    "After the build completes, run your container:\n",
    "```bash\n",
    "docker run -p 8000:80 ml_model_container\n",
    "```\n",
    "<img src=\"./imgs/docker_run.png\" alt=\"drawing\" width=\"650\"/>\n",
    "\n",
    "This command maps port 8000 on your host to port 80 on the container, making your API accessible at `http://localhost:8000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Testing Your API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your container running, you can test the prediction endpoint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing API using the browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Open a web browser or use a tool like curl to access:\n",
    "http://localhost:8000/predict/?value=6\n",
    "\n",
    "<img src=\"./imgs/inference.png\" alt=\"drawing\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing API using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your API endpoint from a Python script, you can use the `requests` library, which is a popular library used for making HTTP requests. \n",
    "```bash\n",
    "pip install requests\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for input 6: [12.000000000000002]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the FastAPI app running in Docker\n",
    "url = 'http://localhost:8000/predict/'\n",
    "\n",
    "# Query parameter - value you want to predict\n",
    "params = {\n",
    "    'value': 6  # You can change this to any number to test different predictions\n",
    "}\n",
    "\n",
    "# Sending a GET request to the API\n",
    "response = requests.get(url, params=params)\n",
    "\n",
    "# Checking if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Extracting prediction from the response\n",
    "    data = response.json()\n",
    "    print(f\"Prediction for input {params['value']}: {data['prediction']}\")\n",
    "else:\n",
    "    print(\"Failed to fetch prediction\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing API using Postman\n",
    "\n",
    "Postman is a popular tool for API testing that allows for sending HTTP requests to your server and viewing the responses in a user-friendly interface. Follow these steps to set up Postman to test the prediction endpoint of your FastAPI application running inside a Docker container.\n",
    "\n",
    "<img src=\"./imgs/postman.png\" alt=\"drawing\" width=\"1200\"/>\n",
    "\n",
    "##### Step 1: Install Postman\n",
    "\n",
    "If you don't have Postman installed, download it from the [official Postman website](https://www.postman.com/downloads/). Follow the installation instructions for your operating system.\n",
    "\n",
    "##### Step 2: Launch Postman\n",
    "\n",
    "Open Postman after installing. You'll see an interface where you can create new requests and organize them into collections.\n",
    "\n",
    "##### Step 3: Create a New Request\n",
    "\n",
    "1. **Start a New Request**: Click on the \"New\" button or the \"+\" tab to create a new request.\n",
    "2. **Set the Request Type**: Use the dropdown menu to select \"GET\" as this is the method our API endpoint uses.\n",
    "3. **Enter the Request URL**: The URL should be `http://localhost:8000/predict/`, assuming your Docker setup maps port 80 inside the container to port 8000 on your host.\n",
    "\n",
    "##### Step 4: Configure Query Parameters\n",
    "\n",
    "1. **Add Query Parameters**: Navigate to the \"Params\" section below the URL bar.\n",
    "2. **Key-Value Input**: Add one key-value pair:\n",
    "   - **Key**: `value`\n",
    "   - **Value**: Enter a number, such as `6`, for testing.\n",
    "\n",
    "##### Step 5: Send the Request\n",
    "\n",
    "- Click the \"Send\" button to make the request. Postman will send the request and display the response.\n",
    "\n",
    "##### Step 6: View the Response\n",
    "\n",
    "- **Response Output**: The response will appear in the lower \"Body\" section of the Postman interface, typically showing JSON data like `{\"prediction\": [12.000000000000002]}`.\n",
    "- **Status and Performance**: Observe the status (e.g., 200 OK) and the response time, which are useful for performance analysis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
